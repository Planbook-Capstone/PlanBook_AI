"""
Qdrant Service - Qu·∫£n l√Ω vector embeddings v·ªõi Qdrant
"""

import logging
import threading
from typing import Dict, Any, List, Optional, Union, cast
import uuid
import datetime
import re

# Heavy imports s·∫Ω ƒë∆∞·ª£c lazy load trong __init__ method

from app.core.config import settings
from app.services.semantic_analysis_service import semantic_analysis_service

logger = logging.getLogger(__name__)


class QdrantService:
    """
    Service qu·∫£n l√Ω vector embeddings v·ªõi Qdrant
    Singleton pattern v·ªõi Lazy Initialization
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        """Singleton pattern implementation v·ªõi thread-safe"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(QdrantService, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Lazy initialization - ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn"""
        if self._initialized:
            return

        self.embedding_model = None
        self.qdrant_client = None
        self.vector_size: Optional[int] = None
        self._service_initialized = False
        self._initialized = True

    def _ensure_service_initialized(self):
        """Ensure Qdrant service is initialized"""
        if not self._service_initialized:
            logger.info("üîÑ QdrantService: First-time initialization triggered")
            self._init_embedding_model()
            self._init_qdrant_client()
            self._service_initialized = True
            logger.info("‚úÖ QdrantService: Initialization completed")



    def _init_embedding_model(self):
        """Kh·ªüi t·∫°o m√¥ h√¨nh embedding"""
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            import warnings

            model_name = settings.EMBEDDING_MODEL
            logger.info(f"üîß Initializing embedding model: {model_name}")

            # Suppress the specific pin_memory warning when no CUDA is available
            if not torch.cuda.is_available():
                warnings.filterwarnings(
                    "ignore",
                    message=".*pin_memory.*no accelerator.*",
                    category=UserWarning,
                    module="torch.utils.data.dataloader",
                )

            # Initialize model with appropriate device
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"üîß Using device: {device}")

            # Special handling for different models
            if "nvidia" in model_name.lower():
                logger.info(f"üîß Loading nvidia model with trust_remote_code=True")
                self.embedding_model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
            else:
                logger.info(f"üîß Loading standard model")
                self.embedding_model = SentenceTransformer(model_name, device=device)

            self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(
                f"‚úÖ Embedding model initialized successfully: {model_name} (dim={self.vector_size}, device={device})"
            )

            # Test encoding ƒë·ªÉ ƒë·∫£m b·∫£o model ho·∫°t ƒë·ªông
            test_text = "Test embedding"
            test_embedding = self.embedding_model.encode(test_text)
            logger.info(f"‚úÖ Model test successful, embedding shape: {test_embedding.shape}")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize embedding model: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            self.embedding_model = None
            self.vector_size = None

    def _init_qdrant_client(self):
        """Kh·ªüi t·∫°o k·∫øt n·ªëi Qdrant"""
        try:
            from qdrant_client import QdrantClient
            self.qdrant_client = QdrantClient(
                url=settings.QDRANT_URL,
                api_key=settings.QDRANT_API_KEY if settings.QDRANT_API_KEY else None,
            )
            logger.info(f"Qdrant client initialized: {settings.QDRANT_URL}")
        except Exception as e:
            logger.error(f"Failed to initialize Qdrant client: {e}")
            self.qdrant_client = None

    async def create_collection(self, collection_name: str) -> bool:
        """T·∫°o collection trong Qdrant"""
        self._ensure_service_initialized()
        if not self.qdrant_client or not self.embedding_model or not self.vector_size:
            logger.error(
                "Qdrant client, embedding model, or vector size not initialized"
            )
            return False

        try:
            from qdrant_client.http import models as qdrant_models
            # Ki·ªÉm tra xem collection ƒë√£ t·ªìn t·∫°i ch∆∞a
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]

            if collection_name in existing_names:
                # X√≥a collection c≈© n·∫øu ƒë√£ t·ªìn t·∫°i
                self.qdrant_client.delete_collection(collection_name)
                logger.info(f"Deleted existing collection: {collection_name}")

            # T·∫°o collection m·ªõi
            self.qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=qdrant_models.VectorParams(
                    size=self.vector_size, distance=qdrant_models.Distance.COSINE
                ),
            )

            logger.info(f"Created collection: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"Error creating collection: {e}")
            return False

    async def process_textbook(
        self,
        book_id: str,
        text_content: Optional[str] = None,
        lesson_id: Optional[str] = None,
        book_title: Optional[str] = None,
        book_structure: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω s√°ch gi√°o khoa v√† t·∫°o embeddings

        Args:
            book_id: ID c·ªßa s√°ch
            text_content: N·ªôi dung text ƒë∆°n gi·∫£n (cho simple processing)
            lesson_id: ID b√†i h·ªçc (cho simple processing)
            book_title: Ti√™u ƒë·ªÅ s√°ch
            book_structure: C·∫•u tr√∫c s√°ch ƒë·∫ßy ƒë·ªß (cho full processing)
        """
        self._ensure_service_initialized()

        if (
            not self.qdrant_client
            or not self.embedding_model
            or self.vector_size is None
        ):
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            # T·∫°o collection cho s√°ch
            collection_name = f"textbook_{book_id}"
            collection_created = await self.create_collection(collection_name)

            if not collection_created:
                return {"success": False, "error": "Failed to create collection"}

            # X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o
            if book_structure:
                # X·ª≠ l√Ω book_structure ƒë·∫ßy ƒë·ªß
                return await self._process_book_structure(book_id, book_structure, collection_name)
            elif text_content:
                # X·ª≠ l√Ω text_content ƒë∆°n gi·∫£n
                return await self._process_simple_text(book_id, text_content, lesson_id, book_title, collection_name)
            else:
                return {"success": False, "error": "Either book_structure or text_content must be provided"}



        except Exception as e:
            logger.error(f"Error processing textbook: {e}")
            return {"success": False, "error": str(e)}

    async def _process_simple_text(
        self, book_id: str, text_content: str, lesson_id: str, book_title: Optional[str], collection_name: str
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω text content ƒë∆°n gi·∫£n"""
        from qdrant_client.http import models as qdrant_models

        # T·∫°o chunks t·ª´ text content
        text_chunks = self._create_text_chunks_from_text(
            text_content,
            settings.MAX_CHUNK_SIZE,
            settings.CHUNK_OVERLAP,
        )

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        points = []
        import uuid

        # T·∫°o embeddings cho t·ª´ng chunk v·ªõi semantic metadata
        for i, chunk_text in enumerate(text_chunks):
            chunk_id = str(uuid.uuid4())
            chunk_vector = self.embedding_model.encode(chunk_text).tolist()

            # Ph√¢n lo·∫°i semantic cho chunk s·ª≠ d·ª•ng LLM
            semantic_info = await semantic_analysis_service.analyze_content_semantic(chunk_text)

            points.append(
                qdrant_models.PointStruct(
                    id=chunk_id,
                    vector=chunk_vector,
                    payload={
                        "book_id": book_id,
                        "lesson_id": lesson_id,
                        "chunk_index": i,
                        "type": "lesson_content",
                        "text": chunk_text,
                        # Semantic metadata - Multi-label v·ªõi confidence
                        "semantic_tags": semantic_info["semantic_tags"],
                        "key_concepts": semantic_info["key_concepts"],
                        "contains_examples": semantic_info["contains_examples"],
                        "contains_definitions": semantic_info["contains_definitions"],
                        "contains_formulas": semantic_info["contains_formulas"],
                        "estimated_difficulty": semantic_info["difficulty"],
                        "analysis_method": semantic_info["analysis_method"],
                        "word_count": len(chunk_text.split()),
                        "char_count": len(chunk_text),
                    },
                )
            )

        total_chunks = len(text_chunks)

        # L∆∞u v√†o Qdrant theo batch
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i : i + batch_size]
            self.qdrant_client.upsert(collection_name=collection_name, points=batch)
            logger.info(
                f"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} to Qdrant"
            )

        # L∆∞u metadata
        zero_vector = [0.0] * self.vector_size
        metadata_point = qdrant_models.PointStruct(
            id=str(uuid.uuid4()),
            vector=zero_vector,
            payload={
                "book_id": book_id,
                "lesson_id": lesson_id,
                "type": "metadata",
                "total_chunks": total_chunks,
                "processed_at": datetime.datetime.now().isoformat(),
                "model": settings.EMBEDDING_MODEL,
                "chunk_size": settings.MAX_CHUNK_SIZE,
                "chunk_overlap": settings.CHUNK_OVERLAP,
                "book_title": book_title or "Unknown",
            },
        )

        self.qdrant_client.upsert(
            collection_name=collection_name, points=[metadata_point]
        )

        return {
            "success": True,
            "book_id": book_id,
            "lesson_id": lesson_id,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "vector_dimension": self.vector_size,
        }

    async def _process_book_structure(
        self, book_id: str, book_structure: Dict[str, Any], collection_name: str
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω book structure ƒë·∫ßy ƒë·ªß"""
        from qdrant_client.http import models as qdrant_models

        points = []
        total_chunks = 0
        import uuid

        # X·ª≠ l√Ω t·ª´ng chapter v√† lesson
        for chapter in book_structure.get("chapters", []):
            chapter_id = chapter.get("id", "unknown")

            for lesson in chapter.get("lessons", []):
                lesson_id = lesson.get("id", "unknown")
                lesson_content = lesson.get("content", "")

                if not lesson_content.strip():
                    continue

                # T·∫°o chunks t·ª´ lesson content
                text_chunks = self._create_text_chunks_from_text(
                    lesson_content,
                    settings.MAX_CHUNK_SIZE,
                    settings.CHUNK_OVERLAP,
                )

                # T·∫°o embeddings cho t·ª´ng chunk
                for i, chunk_text in enumerate(text_chunks):
                    chunk_id = str(uuid.uuid4())
                    chunk_vector = self.embedding_model.encode(chunk_text).tolist()

                    # Ph√¢n lo·∫°i semantic cho chunk s·ª≠ d·ª•ng LLM
                    semantic_info = await semantic_analysis_service.analyze_content_semantic(chunk_text)

                    points.append(
                        qdrant_models.PointStruct(
                            id=chunk_id,
                            vector=chunk_vector,
                            payload={
                                "book_id": book_id,
                                "chapter_id": chapter_id,
                                "lesson_id": lesson_id,
                                "chunk_index": i,
                                "type": "lesson_content",
                                "text": chunk_text,
                                # Semantic metadata - Multi-label v·ªõi confidence
                                "semantic_tags": semantic_info["semantic_tags"],
                                "key_concepts": semantic_info["key_concepts"],
                                "contains_examples": semantic_info["contains_examples"],
                                "contains_definitions": semantic_info["contains_definitions"],
                                "contains_formulas": semantic_info["contains_formulas"],
                                "estimated_difficulty": semantic_info["difficulty"],
                                "analysis_method": semantic_info["analysis_method"],
                                "word_count": len(chunk_text.split()),
                                "char_count": len(chunk_text),
                            },
                        )
                    )
                    total_chunks += 1

        # L∆∞u v√†o Qdrant theo batch
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i : i + batch_size]
            self.qdrant_client.upsert(collection_name=collection_name, points=batch)
            logger.info(
                f"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} to Qdrant"
            )

        # L∆∞u metadata
        zero_vector = [0.0] * self.vector_size
        metadata_point = qdrant_models.PointStruct(
            id=str(uuid.uuid4()),
            vector=zero_vector,
            payload={
                "book_id": book_id,
                "type": "metadata",
                "total_chunks": total_chunks,
                "processed_at": datetime.datetime.now().isoformat(),
                "model": settings.EMBEDDING_MODEL,
                "chunk_size": settings.MAX_CHUNK_SIZE,
                "chunk_overlap": settings.CHUNK_OVERLAP,
                "book_title": book_structure.get("title", "Unknown"),
                "total_chapters": len(book_structure.get("chapters", [])),
                "total_lessons": sum(len(ch.get("lessons", [])) for ch in book_structure.get("chapters", [])),
            },
        )

        self.qdrant_client.upsert(
            collection_name=collection_name, points=[metadata_point]
        )

        return {
            "success": True,
            "book_id": book_id,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "vector_dimension": self.vector_size,
        }



    def _create_text_chunks_from_text(
        self, text: str, max_size: int, overlap: int
    ) -> List[str]:
        """T·∫°o chunks t·ª´ text content v·ªõi semantic awareness"""
        if not text or not text.strip():
            return []

        # S·ª≠ d·ª•ng semantic chunking n·∫øu c√≥ th·ªÉ
        try:
            return self._semantic_chunking(text, max_size, overlap)
        except Exception as e:
            logger.warning(f"Semantic chunking failed, fallback to simple chunking: {e}")
            return self._simple_chunking(text, max_size, overlap)

    def _semantic_chunking(self, text: str, max_size: int, overlap: int) -> List[str]:
        """Chia chunks d·ª±a tr√™n ng·ªØ nghƒ©a"""
        
        # 1. Ph√¢n t√°ch theo c·∫•u tr√∫c vƒÉn b·∫£n
        semantic_chunks = []
        
        # Chia theo ti√™u ƒë·ªÅ/ƒë·ªÅ m·ª•c (H1, H2, ##, etc.)
        header_pattern = r'(^#{1,6}\s+.+$|^[A-Z][^.!?]*:$|^\d+\.\s+[A-Z][^.!?]*$)'
        sections = re.split(header_pattern, text, flags=re.MULTILINE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if not section.strip():
                continue
                
            # Ki·ªÉm tra n·∫øu l√† header
            if re.match(header_pattern, section.strip(), re.MULTILINE):
                # L∆∞u section tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_section.strip():
                    semantic_chunks.extend(self._split_by_paragraphs(current_section, max_size))
                current_section = section + "\n"
            else:
                current_section += section
        
        # X·ª≠ l√Ω section cu·ªëi
        if current_section.strip():
            semantic_chunks.extend(self._split_by_paragraphs(current_section, max_size))
        
        # N·∫øu kh√¥ng c√≥ structure r√µ r√†ng, fallback v·ªÅ paragraph-based
        if not semantic_chunks:
            semantic_chunks = self._split_by_paragraphs(text, max_size)
        
        # Th√™m overlap th√¥ng minh
        return self._add_semantic_overlap(semantic_chunks, overlap)

    def _split_by_paragraphs(self, text: str, max_size: int) -> List[str]:
        """Chia theo ƒëo·∫°n vƒÉn"""
        # Chia theo paragraph (2+ newlines)
        paragraphs = re.split(r'\n\s*\n', text.strip())
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # N·∫øu th√™m paragraph n√†y v√†o chunk hi·ªán t·∫°i m√† kh√¥ng v∆∞·ª£t qu√° max_size
            test_chunk = current_chunk + "\n\n" + para if current_chunk else para
            
            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                # L∆∞u chunk hi·ªán t·∫°i
                if current_chunk:
                    chunks.append(current_chunk)
                
                # N·∫øu paragraph qu√° d√†i, chia nh·ªè h∆°n
                if len(para) > max_size:
                    chunks.extend(self._split_by_sentences(para, max_size))
                    current_chunk = ""
                else:
                    current_chunk = para
        
        # Th√™m chunk cu·ªëi
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _split_by_sentences(self, text: str, max_size: int) -> List[str]:
        """Chia theo c√¢u khi paragraph qu√° d√†i"""
        
        # Chia theo c√¢u (d·∫•u .!?)
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                
                # N·∫øu c√¢u qu√° d√†i, chia theo t·ª´
                if len(sentence) > max_size:
                    chunks.extend(self._simple_chunking(sentence, max_size, 0))
                    current_chunk = ""
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _add_semantic_overlap(self, chunks: List[str], overlap: int) -> List[str]:
        """Th√™m overlap th√¥ng minh d·ª±a tr√™n sentences"""
        if len(chunks) <= 1 or overlap <= 0:
            return chunks
        
        import re
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(chunk)
            else:
                # L·∫•y c√¢u cu·ªëi c·ªßa chunk tr∆∞·ªõc
                prev_chunk = chunks[i-1]
                prev_sentences = re.split(r'(?<=[.!?])\s+', prev_chunk)
                
                # L·∫•y overlap_text t·ª´ chunk tr∆∞·ªõc
                overlap_text = ""
                for j in range(len(prev_sentences)-1, -1, -1):
                    test_overlap = prev_sentences[j] + " " + overlap_text if overlap_text else prev_sentences[j]
                    if len(test_overlap) <= overlap:
                        overlap_text = test_overlap
                    else:
                        break
                
                # K·∫øt h·ª£p v·ªõi chunk hi·ªán t·∫°i
                if overlap_text:
                    overlapped_chunk = overlap_text + "\n\n" + chunk
                else:
                    overlapped_chunk = chunk
                
                overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks

    def _simple_chunking(self, text: str, max_size: int, overlap: int) -> List[str]:
        """Fallback chunking method (original logic)"""
        chunks = []
        text = text.strip()

        # N·∫øu text ng·∫Øn h∆°n max_size, tr·∫£ v·ªÅ lu√¥n
        if len(text) <= max_size:
            return [text]

        # Chia text th√†nh c√°c chunks v·ªõi overlap
        start = 0
        while start < len(text):
            end = start + max_size

            # N·∫øu kh√¥ng ph·∫£i chunk cu·ªëi, t√¨m v·ªã tr√≠ ng·∫Øt t·ª± nhi√™n
            if end < len(text):
                # T√¨m d·∫•u c√¢u ho·∫∑c kho·∫£ng tr·∫Øng g·∫ßn nh·∫•t
                for i in range(end, start + max_size // 2, -1):
                    if text[i] in ".!?\n ":
                        end = i + 1
                        break

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # Di chuy·ªÉn start v·ªõi overlap
            start = max(start + max_size - overlap, end)

            # Tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
            if start >= len(text):
                break

        return chunks

    async def search_textbook(
        self, book_id: str, query: str, limit: int = 5, semantic_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """T√¨m ki·∫øm trong s√°ch gi√°o khoa b·∫±ng vector similarity"""
        from qdrant_client.http import models as qdrant_models
        self._ensure_service_initialized()

        if not self.qdrant_client or not self.embedding_model:
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            collection_name = f"textbook_{book_id}"

            # Ki·ªÉm tra xem collection c√≥ t·ªìn t·∫°i kh√¥ng
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]

            if collection_name not in existing_names:
                return {
                    "success": False,
                    "error": f"Collection not found for book_id {book_id}. Please create embeddings first.",
                }

            # T·∫°o embedding cho query
            query_vector = self.embedding_model.encode(query).tolist()

            # Chu·∫©n b·ªã filters cho Qdrant
            qdrant_filter = None
            if semantic_filters:
                filter_conditions = []

                # Filter by semantic tags
                if "semantic_tags" in semantic_filters:
                    tag_types = semantic_filters["semantic_tags"]
                    if isinstance(tag_types, str):
                        tag_types = [tag_types]

                    # T·∫°o filter cho semantic_tags array
                    for tag_type in tag_types:
                        filter_conditions.append(
                            qdrant_models.FieldCondition(
                                key="semantic_tags",
                                match=qdrant_models.MatchAny(any=[tag_type])
                            )
                        )

                # Filter by difficulty
                if "difficulty" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="estimated_difficulty",
                            match=qdrant_models.MatchValue(value=semantic_filters["difficulty"])
                        )
                    )

                # Filter by content features
                if "has_examples" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_examples",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_examples"])
                        )
                    )

                if "has_formulas" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_formulas",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_formulas"])
                        )
                    )

                # Filter by lesson_id
                if "lesson_id" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="lesson_id",
                            match=qdrant_models.MatchValue(value=semantic_filters["lesson_id"])
                        )
                    )

                if filter_conditions:
                    qdrant_filter = qdrant_models.Filter(
                        must=filter_conditions
                    )

            # T√¨m ki·∫øm trong Qdrant
            search_result = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                query_filter=qdrant_filter,
                limit=limit,
                with_payload=True,
                score_threshold=0.3,  # Gi·∫£m threshold ƒë·ªÉ c√≥ nhi·ªÅu k·∫øt qu·∫£ h∆°n
            )

            # Format k·∫øt qu·∫£
            results = []
            for scored_point in search_result:
                # S·ª≠a l·ªói: Ki·ªÉm tra payload tr∆∞·ªõc khi truy c·∫≠p
                payload = scored_point.payload or {}

                # B·ªè qua metadata point b·∫±ng c√°ch check payload
                if payload.get("type") == "metadata":
                    continue

                # Chu·∫©n b·ªã semantic metadata
                semantic_tags = payload.get("semantic_tags", [])

                # Backward compatibility: convert old semantic_type to new format
                if not semantic_tags and "semantic_type" in payload:
                    semantic_tags = [{"type": payload["semantic_type"], "confidence": 0.8}]

                results.append(
                    {
                        "text": payload.get("text", ""),
                        "score": scored_point.score,
                        "chapter_title": payload.get("chapter_title", ""),
                        "lesson_title": payload.get("lesson_title", ""),
                        "lesson_id": payload.get("lesson_id", ""),
                        "chapter_id": payload.get("chapter_id", ""),
                        "type": payload.get("type", ""),
                        "semantic_tags": semantic_tags,
                        "key_concepts": payload.get("key_concepts", []),
                        "estimated_difficulty": payload.get("estimated_difficulty", "basic"),
                        "contains_examples": payload.get("contains_examples", False),
                        "contains_definitions": payload.get("contains_definitions", False),
                        "contains_formulas": payload.get("contains_formulas", False),
                        "analysis_method": payload.get("analysis_method", "unknown")
                    }
                )

            return {
                "success": True,
                "book_id": book_id,
                "query": query,
                "results": results,
            }

        except Exception as e:
            logger.error(f"Error searching textbook: {e}")
            return {"success": False, "error": str(e)}

    async def global_search(
        self, query: str, limit: int = 5, semantic_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """T√¨m ki·∫øm to√†n b·ªô trong t·∫•t c·∫£ collections"""
        from qdrant_client.http import models as qdrant_models

        if not self.qdrant_client or not self.embedding_model:
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            # L·∫•y t·∫•t c·∫£ collections
            collections = self.qdrant_client.get_collections().collections
            textbook_collections = [c.name for c in collections if c.name.startswith("textbook_")]

            if not textbook_collections:
                return {
                    "success": True,
                    "query": query,
                    "results": [],
                    "message": "No textbook collections found"
                }

            # T·∫°o embedding cho query
            query_vector = self.embedding_model.encode(query).tolist()

            # Chu·∫©n b·ªã filters cho Qdrant (t√°i s·ª≠ d·ª•ng logic t·ª´ search_textbook)
            qdrant_filter = None
            if semantic_filters:
                filter_conditions = []

                # Filter by semantic tags
                if "semantic_tags" in semantic_filters:
                    tag_types = semantic_filters["semantic_tags"]
                    if isinstance(tag_types, str):
                        tag_types = [tag_types]

                    for tag_type in tag_types:
                        filter_conditions.append(
                            qdrant_models.FieldCondition(
                                key="semantic_tags",
                                match=qdrant_models.MatchAny(any=[tag_type])
                            )
                        )

                # Filter by difficulty
                if "difficulty" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="estimated_difficulty",
                            match=qdrant_models.MatchValue(value=semantic_filters["difficulty"])
                        )
                    )

                # Filter by content features
                if "has_examples" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_examples",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_examples"])
                        )
                    )

                if "has_formulas" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_formulas",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_formulas"])
                        )
                    )

                # Filter by lesson_id
                if "lesson_id" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="lesson_id",
                            match=qdrant_models.MatchValue(value=semantic_filters["lesson_id"])
                        )
                    )

                if filter_conditions:
                    qdrant_filter = qdrant_models.Filter(
                        must=filter_conditions
                    )

            # T√¨m ki·∫øm trong t·∫•t c·∫£ collections v√† g·ªôp k·∫øt qu·∫£
            all_results = []

            for collection_name in textbook_collections:
                try:
                    search_result = self.qdrant_client.search(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        query_filter=qdrant_filter,
                        limit=limit,
                        with_payload=True,
                        score_threshold=0.3,
                    )

                    # Extract book_id from collection name
                    book_id = collection_name.replace("textbook_", "")

                    # Format k·∫øt qu·∫£
                    for scored_point in search_result:
                        payload = scored_point.payload or {}

                        # B·ªè qua metadata point
                        if payload.get("type") == "metadata":
                            continue

                        # Chu·∫©n b·ªã semantic metadata
                        semantic_tags = payload.get("semantic_tags", [])

                        # Backward compatibility
                        if not semantic_tags and "semantic_type" in payload:
                            semantic_tags = [{"type": payload["semantic_type"], "confidence": 0.8}]

                        all_results.append({
                            "text": payload.get("text", ""),
                            "score": scored_point.score,
                            "book_id": book_id,
                            "chapter_title": payload.get("chapter_title", ""),
                            "lesson_title": payload.get("lesson_title", ""),
                            "lesson_id": payload.get("lesson_id", ""),
                            "chapter_id": payload.get("chapter_id", ""),
                            "type": payload.get("type", ""),
                            "semantic_tags": semantic_tags,
                            "key_concepts": payload.get("key_concepts", []),
                            "estimated_difficulty": payload.get("estimated_difficulty", "basic"),
                            "contains_examples": payload.get("contains_examples", False),
                            "contains_definitions": payload.get("contains_definitions", False),
                            "contains_formulas": payload.get("contains_formulas", False),
                            "analysis_method": payload.get("analysis_method", "unknown")
                        })

                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue

            # S·∫Øp x·∫øp theo score v√† gi·ªõi h·∫°n k·∫øt qu·∫£
            all_results.sort(key=lambda x: x["score"], reverse=True)
            final_results = all_results[:limit]

            return {
                "success": True,
                "query": query,
                "results": final_results,
                "collections_searched": len(textbook_collections),
                "total_results_found": len(all_results)
            }

        except Exception as e:
            logger.error(f"Error in global search: {e}")
            return {"success": False, "error": str(e)}

    async def delete_textbook_by_id(self, book_id: str) -> Dict[str, Any]:
        """
        X√≥a textbook b·∫±ng book_id (x√≥a collection trong Qdrant)
        
        Args:
            book_id: ID c·ªßa textbook c·∫ßn x√≥a
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ x√≥a
        """
        try:
            if not self.qdrant_client:
                return {
                    "success": False,
                    "error": "Qdrant client not initialized"
                }

            collection_name = f"textbook_{book_id}"
            
            # Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]
            
            if collection_name not in existing_names:
                return {
                    "success": False,
                    "error": f"Textbook with ID '{book_id}' not found",
                    "book_id": book_id,
                    "collection_name": collection_name
                }
            
            # L·∫•y th√¥ng tin v·ªÅ collection tr∆∞·ªõc khi x√≥a
            collection_info = self.qdrant_client.get_collection(collection_name)
            vector_count = getattr(collection_info, 'vectors_count', 0)
            
            # X√≥a collection
            self.qdrant_client.delete_collection(collection_name)
            logger.info(f"Deleted textbook collection: {collection_name}")
            
            return {
                "success": True,
                "message": f"Textbook '{book_id}' deleted successfully",
                "book_id": book_id,
                "collection_name": collection_name,
                "deleted_vectors": vector_count
            }
            
        except Exception as e:
            logger.error(f"Error deleting textbook {book_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "book_id": book_id
            }

    async def delete_textbook_by_lesson_id(self, lesson_id: str) -> Dict[str, Any]:
        """
        X√≥a textbook b·∫±ng lesson_id (t√¨m collection ch·ª©a lesson_id r·ªìi x√≥a)

        Args:
            lesson_id: ID c·ªßa lesson ƒë·ªÉ t√¨m textbook c·∫ßn x√≥a
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ x√≥a
        """
        try:
            from qdrant_client.http import models as qdrant_models
            if not self.qdrant_client:
                return {
                    "success": False,
                    "error": "Qdrant client not initialized"
                }

            # T√¨m collection ch·ª©a lesson_id
            collections = self.qdrant_client.get_collections().collections
            found_collection = None
            found_book_id = None

            for collection in collections:
                if collection.name.startswith("textbook_"):
                    try:
                        # T√¨m ki·∫øm lesson_id trong collection n√†y
                        search_result = self.qdrant_client.scroll(
                            collection_name=collection.name,
                            scroll_filter=qdrant_models.Filter(
                                must=[
                                    qdrant_models.FieldCondition(
                                        key="lesson_id",
                                        match=qdrant_models.MatchValue(value=lesson_id),
                                    )
                                ]
                            ),
                            limit=1,
                            with_payload=True,
                        )

                        if search_result[0]:  # T√¨m th·∫•y lesson
                            found_collection = collection.name
                            found_book_id = collection.name.replace("textbook_", "")
                            break

                    except Exception as e:
                        logger.warning(f"Error searching in collection {collection.name}: {e}")
                        continue

            if not found_collection:
                return {
                    "success": False,
                    "error": f"No textbook found containing lesson_id: {lesson_id}",
                    "lesson_id": lesson_id
                }

            # L·∫•y th√¥ng tin v·ªÅ collection tr∆∞·ªõc khi x√≥a
            collection_info = self.qdrant_client.get_collection(found_collection)
            vector_count = getattr(collection_info, 'vectors_count', 0)
            
            # X√≥a collection
            self.qdrant_client.delete_collection(found_collection)
            logger.info(f"Deleted textbook collection: {found_collection} (found by lesson_id: {lesson_id})")
            
            return {
                "success": True,
                "message": f"Textbook containing lesson '{lesson_id}' deleted successfully",
                "lesson_id": lesson_id,
                "book_id": found_book_id,
                "collection_name": found_collection,
                "deleted_vectors": vector_count
            }
            
        except Exception as e:
            logger.error(f"Error deleting textbook by lesson_id {lesson_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "lesson_id": lesson_id
            }



    def get_semantic_suggestions(self, book_id: str) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ semantic ƒë·ªÉ g·ª£i √Ω filter options"""
        
        if not self.qdrant_client:
            return {"success": False, "error": "Qdrant client not initialized"}
        
        try:
            collection_name = f"textbook_{book_id}"
            
            # Get all points to analyze
            points = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=1000,  # Adjust based on your data size
                with_payload=True
            )[0]  # scroll returns (points, next_page_token)
            
            # Analyze semantic distribution
            semantic_stats = {
                "semantic_types": {},
                "difficulty_levels": {},
                "content_features": {
                    "has_examples": 0,
                    "has_definitions": 0,
                    "has_formulas": 0,
                },
                "concepts": {},
                "total_chunks": 0
            }
            
            for point in points:
                payload = point.payload or {}
                
                # Skip metadata points
                if payload.get("type") == "metadata":
                    continue
                    
                semantic_stats["total_chunks"] += 1
                
                # Count semantic tags (multi-label)
                semantic_tags = payload.get("semantic_tags", [])
                for tag_info in semantic_tags:
                    if isinstance(tag_info, dict) and "type" in tag_info:
                        tag_type = tag_info["type"]
                        semantic_stats["semantic_types"][tag_type] = (
                            semantic_stats["semantic_types"].get(tag_type, 0) + 1
                        )
                
                # Count difficulty levels
                difficulty = payload.get("estimated_difficulty", "basic")
                semantic_stats["difficulty_levels"][difficulty] = (
                    semantic_stats["difficulty_levels"].get(difficulty, 0) + 1
                )
                
                # Count content features
                if payload.get("contains_examples"):
                    semantic_stats["content_features"]["has_examples"] += 1
                if payload.get("contains_definitions"):
                    semantic_stats["content_features"]["has_definitions"] += 1
                if payload.get("contains_formulas"):
                    semantic_stats["content_features"]["has_formulas"] += 1
                
                # Count key concepts
                key_concepts = payload.get("key_concepts", [])
                for concept in key_concepts:
                    semantic_stats["concepts"][concept] = (
                        semantic_stats["concepts"].get(concept, 0) + 1
                    )
            
            # Sort and limit concepts
            sorted_concepts = sorted(
                semantic_stats["concepts"].items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:20]
            
            semantic_stats["top_concepts"] = dict(sorted_concepts)
            del semantic_stats["concepts"]  # Remove full list
            
            return {
                "success": True,
                "book_id": book_id,
                "statistics": semantic_stats
            }
            
        except Exception as e:
            logger.error(f"Error getting semantic suggestions: {e}")
            return {"success": False, "error": str(e)}


# H√†m ƒë·ªÉ l·∫•y singleton instance
def get_qdrant_service() -> QdrantService:
    """
    L·∫•y singleton instance c·ªßa QdrantService
    Thread-safe lazy initialization

    Returns:
        QdrantService: Singleton instance
    """
    return QdrantService()


# Backward compatibility - deprecated, s·ª≠ d·ª•ng get_qdrant_service() thay th·∫ø
# Lazy loading ƒë·ªÉ tr√°nh kh·ªüi t·∫°o ngay khi import
_qdrant_service_instance = None

def _get_qdrant_service_lazy():
    """Lazy loading cho backward compatibility"""
    global _qdrant_service_instance
    if _qdrant_service_instance is None:
        _qdrant_service_instance = get_qdrant_service()
    return _qdrant_service_instance

# T·∫°o proxy object ƒë·ªÉ lazy loading
class _QdrantServiceProxy:
    def __getattr__(self, name):
        return getattr(_get_qdrant_service_lazy(), name)

    def __call__(self, *args, **kwargs):
        return _get_qdrant_service_lazy()(*args, **kwargs)

qdrant_service = _QdrantServiceProxy()
