"""
Qdrant Service - Quáº£n lÃ½ vector embeddings vá»›i Qdrant
"""

import logging
import threading
from typing import Dict, Any, List, Optional, Union, cast
import uuid
import datetime
import re

# Heavy imports sáº½ Ä‘Æ°á»£c lazy load trong __init__ method

from app.core.config import settings
from app.services.semantic_analysis_service import get_semantic_analysis_service

logger = logging.getLogger(__name__)


class QdrantService:
    """
    Service quáº£n lÃ½ vector embeddings vá»›i Qdrant
    """

    def __init__(self):
        """Initialize Qdrant service"""
        self.embedding_model = None
        self.qdrant_client = None
        self.vector_size: Optional[int] = None
        self._service_initialized = False
        self.semantic_analysis_service = get_semantic_analysis_service()

    def _ensure_service_initialized(self):
        """Ensure Qdrant service is initialized"""
        if not self._service_initialized:
            logger.info("ğŸ”„ QdrantService: First-time initialization triggered")
            self._init_embedding_model()
            self._init_qdrant_client()
            self._service_initialized = True
            logger.info("âœ… QdrantService: Initialization completed")



    def _init_embedding_model(self):
        """Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding"""
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            import warnings

            model_name = settings.EMBEDDING_MODEL
            logger.info(f"ğŸ”§ Initializing embedding model: {model_name}")

            # Suppress the specific pin_memory warning when no CUDA is available
            if not torch.cuda.is_available():
                warnings.filterwarnings(
                    "ignore",
                    message=".*pin_memory.*no accelerator.*",
                    category=UserWarning,
                    module="torch.utils.data.dataloader",
                )

            # Initialize model with appropriate device
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ğŸ”§ Using device: {device}")

            # Special handling for different models
            if "nvidia" in model_name.lower():
                logger.info(f"ğŸ”§ Loading nvidia model with trust_remote_code=True")
                self.embedding_model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
            else:
                logger.info(f"ğŸ”§ Loading standard model")
                self.embedding_model = SentenceTransformer(model_name, device=device)

            self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(
                f"âœ… Embedding model initialized successfully: {model_name} (dim={self.vector_size}, device={device})"
            )

            # Test encoding Ä‘á»ƒ Ä‘áº£m báº£o model hoáº¡t Ä‘á»™ng
            test_text = "Test embedding"
            test_embedding = self.embedding_model.encode(test_text)
            logger.info(f"âœ… Model test successful, embedding shape: {test_embedding.shape}")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize embedding model: {e}")
            import traceback
            logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            self.embedding_model = None
            self.vector_size = None

    def _init_qdrant_client(self):
        """Khá»Ÿi táº¡o káº¿t ná»‘i Qdrant"""
        try:
            from qdrant_client import QdrantClient
            self.qdrant_client = QdrantClient(
                url=settings.QDRANT_URL,
                api_key=settings.QDRANT_API_KEY if settings.QDRANT_API_KEY else None,
            )
            logger.info(f"Qdrant client initialized: {settings.QDRANT_URL}")
        except Exception as e:
            logger.error(f"Failed to initialize Qdrant client: {e}")
            self.qdrant_client = None

    async def create_collection(self, collection_name: str) -> bool:
        """Táº¡o collection trong Qdrant"""
        self._ensure_service_initialized()
        if not self.qdrant_client or not self.embedding_model or not self.vector_size:
            logger.error(
                "Qdrant client, embedding model, or vector size not initialized"
            )
            return False

        try:
            from qdrant_client.http import models as qdrant_models
            # Kiá»ƒm tra xem collection Ä‘Ã£ tá»“n táº¡i chÆ°a
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]

            if collection_name in existing_names:
                # XÃ³a collection cÅ© náº¿u Ä‘Ã£ tá»“n táº¡i
                self.qdrant_client.delete_collection(collection_name)
                logger.info(f"Deleted existing collection: {collection_name}")

            # Táº¡o collection má»›i
            self.qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=qdrant_models.VectorParams(
                    size=self.vector_size, distance=qdrant_models.Distance.COSINE
                ),
            )

            logger.info(f"Created collection: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"Error creating collection: {e}")
            return False

    async def process_textbook(
        self,
        book_id: str,
        text_content: Optional[str] = None,
        lesson_id: Optional[str] = None,
        book_title: Optional[str] = None,
        book_structure: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ sÃ¡ch giÃ¡o khoa vÃ  táº¡o embeddings

        Args:
            book_id: ID cá»§a sÃ¡ch
            text_content: Ná»™i dung text Ä‘Æ¡n giáº£n (cho simple processing)
            lesson_id: ID bÃ i há»c (cho simple processing)
            book_title: TiÃªu Ä‘á» sÃ¡ch
            book_structure: Cáº¥u trÃºc sÃ¡ch Ä‘áº§y Ä‘á»§ (cho full processing)
        """
        self._ensure_service_initialized()

        if (
            not self.qdrant_client
            or not self.embedding_model
            or self.vector_size is None
        ):
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            # Táº¡o collection cho sÃ¡ch
            collection_name = f"textbook_{book_id}"
            collection_created = await self.create_collection(collection_name)

            if not collection_created:
                return {"success": False, "error": "Failed to create collection"}

            # Xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº§u vÃ o
            if book_structure:
                # Xá»­ lÃ½ book_structure Ä‘áº§y Ä‘á»§
                return await self._process_book_structure(book_id, book_structure, collection_name)
            elif text_content:
                # Xá»­ lÃ½ text_content Ä‘Æ¡n giáº£n
                return await self._process_simple_text(book_id, text_content, lesson_id, book_title, collection_name)
            else:
                return {"success": False, "error": "Either book_structure or text_content must be provided"}



        except Exception as e:
            logger.error(f"Error processing textbook: {e}")
            return {"success": False, "error": str(e)}

    async def _process_simple_text(
        self, book_id: str, text_content: str, lesson_id: str, book_title: Optional[str], collection_name: str
    ) -> Dict[str, Any]:
        """Xá»­ lÃ½ text content Ä‘Æ¡n giáº£n"""
        from qdrant_client.http import models as qdrant_models

        # Táº¡o chunks tá»« text content
        text_chunks = self._create_text_chunks_from_text(
            text_content,
            settings.MAX_CHUNK_SIZE,
            settings.CHUNK_OVERLAP,
        )

        # Chuáº©n bá»‹ dá»¯ liá»‡u
        points = []
        import uuid

        # Táº¡o embeddings cho tá»«ng chunk vá»›i semantic metadata
        for i, chunk_text in enumerate(text_chunks):
            chunk_id = str(uuid.uuid4())
            chunk_vector = self.embedding_model.encode(chunk_text).tolist()

            # PhÃ¢n loáº¡i semantic cho chunk sá»­ dá»¥ng LLM
            semantic_info = await self.semantic_analysis_service.analyze_content_semantic(chunk_text)

            points.append(
                qdrant_models.PointStruct(
                    id=chunk_id,
                    vector=chunk_vector,
                    payload={
                        "book_id": book_id,
                        "lesson_id": lesson_id,
                        "chunk_index": i,
                        "type": "lesson_content",
                        "text": chunk_text,
                        # Semantic metadata - Multi-label vá»›i confidence
                        "semantic_tags": semantic_info["semantic_tags"],
                        "key_concepts": semantic_info["key_concepts"],
                        "contains_examples": semantic_info["contains_examples"],
                        "contains_definitions": semantic_info["contains_definitions"],
                        "contains_formulas": semantic_info["contains_formulas"],
                        "estimated_difficulty": semantic_info["difficulty"],
                        "analysis_method": semantic_info["analysis_method"],
                        "word_count": len(chunk_text.split()),
                        "char_count": len(chunk_text),
                    },
                )
            )

        total_chunks = len(text_chunks)

        # LÆ°u vÃ o Qdrant theo batch
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i : i + batch_size]
            self.qdrant_client.upsert(collection_name=collection_name, points=batch)
            logger.info(
                f"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} to Qdrant"
            )

        # LÆ°u metadata
        zero_vector = [0.0] * self.vector_size
        metadata_point = qdrant_models.PointStruct(
            id=str(uuid.uuid4()),
            vector=zero_vector,
            payload={
                "book_id": book_id,
                "lesson_id": lesson_id,
                "type": "metadata",
                "total_chunks": total_chunks,
                "processed_at": datetime.datetime.now().isoformat(),
                "model": settings.EMBEDDING_MODEL,
                "chunk_size": settings.MAX_CHUNK_SIZE,
                "chunk_overlap": settings.CHUNK_OVERLAP,
                "book_title": book_title or "Unknown",
            },
        )

        self.qdrant_client.upsert(
            collection_name=collection_name, points=[metadata_point]
        )

        return {
            "success": True,
            "book_id": book_id,
            "lesson_id": lesson_id,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "vector_dimension": self.vector_size,
        }

    async def _process_book_structure(
        self, book_id: str, book_structure: Dict[str, Any], collection_name: str
    ) -> Dict[str, Any]:
        """Xá»­ lÃ½ book structure Ä‘áº§y Ä‘á»§"""
        from qdrant_client.http import models as qdrant_models

        points = []
        total_chunks = 0
        import uuid

        # Xá»­ lÃ½ tá»«ng chapter vÃ  lesson
        for chapter in book_structure.get("chapters", []):
            chapter_id = chapter.get("id", "unknown")

            for lesson in chapter.get("lessons", []):
                lesson_id = lesson.get("id", "unknown")
                lesson_content = lesson.get("content", "")

                if not lesson_content.strip():
                    continue

                # Táº¡o chunks tá»« lesson content
                text_chunks = self._create_text_chunks_from_text(
                    lesson_content,
                    settings.MAX_CHUNK_SIZE,
                    settings.CHUNK_OVERLAP,
                )

                # Táº¡o embeddings cho tá»«ng chunk
                for i, chunk_text in enumerate(text_chunks):
                    chunk_id = str(uuid.uuid4())
                    chunk_vector = self.embedding_model.encode(chunk_text).tolist()

                    # PhÃ¢n loáº¡i semantic cho chunk sá»­ dá»¥ng LLM
                    semantic_info = await self.semantic_analysis_service.analyze_content_semantic(chunk_text)

                    points.append(
                        qdrant_models.PointStruct(
                            id=chunk_id,
                            vector=chunk_vector,
                            payload={
                                "book_id": book_id,
                                "chapter_id": chapter_id,
                                "lesson_id": lesson_id,
                                "chunk_index": i,
                                "type": "lesson_content",
                                "text": chunk_text,
                                # Semantic metadata - Multi-label vá»›i confidence
                                "semantic_tags": semantic_info["semantic_tags"],
                                "key_concepts": semantic_info["key_concepts"],
                                "contains_examples": semantic_info["contains_examples"],
                                "contains_definitions": semantic_info["contains_definitions"],
                                "contains_formulas": semantic_info["contains_formulas"],
                                "estimated_difficulty": semantic_info["difficulty"],
                                "analysis_method": semantic_info["analysis_method"],
                                "word_count": len(chunk_text.split()),
                                "char_count": len(chunk_text),
                            },
                        )
                    )
                    total_chunks += 1

        # LÆ°u vÃ o Qdrant theo batch
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i : i + batch_size]
            self.qdrant_client.upsert(collection_name=collection_name, points=batch)
            logger.info(
                f"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} to Qdrant"
            )

        # LÆ°u metadata
        zero_vector = [0.0] * self.vector_size
        metadata_point = qdrant_models.PointStruct(
            id=str(uuid.uuid4()),
            vector=zero_vector,
            payload={
                "book_id": book_id,
                "type": "metadata",
                "total_chunks": total_chunks,
                "processed_at": datetime.datetime.now().isoformat(),
                "model": settings.EMBEDDING_MODEL,
                "chunk_size": settings.MAX_CHUNK_SIZE,
                "chunk_overlap": settings.CHUNK_OVERLAP,
                "book_title": book_structure.get("title", "Unknown"),
                "total_chapters": len(book_structure.get("chapters", [])),
                "total_lessons": sum(len(ch.get("lessons", [])) for ch in book_structure.get("chapters", [])),
            },
        )

        self.qdrant_client.upsert(
            collection_name=collection_name, points=[metadata_point]
        )

        return {
            "success": True,
            "book_id": book_id,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "vector_dimension": self.vector_size,
        }



    def _create_text_chunks_from_text(
        self, text: str, max_size: int, overlap: int
    ) -> List[str]:
        """Táº¡o chunks tá»« text content vá»›i semantic awareness"""
        if not text or not text.strip():
            return []

        # Sá»­ dá»¥ng semantic chunking náº¿u cÃ³ thá»ƒ
        try:
            return self._semantic_chunking(text, max_size, overlap)
        except Exception as e:
            logger.warning(f"Semantic chunking failed, fallback to simple chunking: {e}")
            return self._simple_chunking(text, max_size, overlap)

    def _semantic_chunking(self, text: str, max_size: int, overlap: int) -> List[str]:
        """Chia chunks dá»±a trÃªn ngá»¯ nghÄ©a"""
        
        # 1. PhÃ¢n tÃ¡ch theo cáº¥u trÃºc vÄƒn báº£n
        semantic_chunks = []
        
        # Chia theo tiÃªu Ä‘á»/Ä‘á» má»¥c (H1, H2, ##, etc.)
        header_pattern = r'(^#{1,6}\s+.+$|^[A-Z][^.!?]*:$|^\d+\.\s+[A-Z][^.!?]*$)'
        sections = re.split(header_pattern, text, flags=re.MULTILINE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if not section.strip():
                continue
                
            # Kiá»ƒm tra náº¿u lÃ  header
            if re.match(header_pattern, section.strip(), re.MULTILINE):
                # LÆ°u section trÆ°á»›c Ä‘Ã³ náº¿u cÃ³
                if current_section.strip():
                    semantic_chunks.extend(self._split_by_paragraphs(current_section, max_size))
                current_section = section + "\n"
            else:
                current_section += section
        
        # Xá»­ lÃ½ section cuá»‘i
        if current_section.strip():
            semantic_chunks.extend(self._split_by_paragraphs(current_section, max_size))
        
        # Náº¿u khÃ´ng cÃ³ structure rÃµ rÃ ng, fallback vá» paragraph-based
        if not semantic_chunks:
            semantic_chunks = self._split_by_paragraphs(text, max_size)
        
        # ThÃªm overlap thÃ´ng minh
        return self._add_semantic_overlap(semantic_chunks, overlap)

    def _split_by_paragraphs(self, text: str, max_size: int) -> List[str]:
        """Chia theo Ä‘oáº¡n vÄƒn"""
        # Chia theo paragraph (2+ newlines)
        paragraphs = re.split(r'\n\s*\n', text.strip())
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # Náº¿u thÃªm paragraph nÃ y vÃ o chunk hiá»‡n táº¡i mÃ  khÃ´ng vÆ°á»£t quÃ¡ max_size
            test_chunk = current_chunk + "\n\n" + para if current_chunk else para
            
            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                # LÆ°u chunk hiá»‡n táº¡i
                if current_chunk:
                    chunks.append(current_chunk)
                
                # Náº¿u paragraph quÃ¡ dÃ i, chia nhá» hÆ¡n
                if len(para) > max_size:
                    chunks.extend(self._split_by_sentences(para, max_size))
                    current_chunk = ""
                else:
                    current_chunk = para
        
        # ThÃªm chunk cuá»‘i
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _split_by_sentences(self, text: str, max_size: int) -> List[str]:
        """Chia theo cÃ¢u khi paragraph quÃ¡ dÃ i"""
        
        # Chia theo cÃ¢u (dáº¥u .!?)
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                
                # Náº¿u cÃ¢u quÃ¡ dÃ i, chia theo tá»«
                if len(sentence) > max_size:
                    chunks.extend(self._simple_chunking(sentence, max_size, 0))
                    current_chunk = ""
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _add_semantic_overlap(self, chunks: List[str], overlap: int) -> List[str]:
        """ThÃªm overlap thÃ´ng minh dá»±a trÃªn sentences"""
        if len(chunks) <= 1 or overlap <= 0:
            return chunks
        
        import re
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(chunk)
            else:
                # Láº¥y cÃ¢u cuá»‘i cá»§a chunk trÆ°á»›c
                prev_chunk = chunks[i-1]
                prev_sentences = re.split(r'(?<=[.!?])\s+', prev_chunk)
                
                # Láº¥y overlap_text tá»« chunk trÆ°á»›c
                overlap_text = ""
                for j in range(len(prev_sentences)-1, -1, -1):
                    test_overlap = prev_sentences[j] + " " + overlap_text if overlap_text else prev_sentences[j]
                    if len(test_overlap) <= overlap:
                        overlap_text = test_overlap
                    else:
                        break
                
                # Káº¿t há»£p vá»›i chunk hiá»‡n táº¡i
                if overlap_text:
                    overlapped_chunk = overlap_text + "\n\n" + chunk
                else:
                    overlapped_chunk = chunk
                
                overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks

    def _simple_chunking(self, text: str, max_size: int, overlap: int) -> List[str]:
        """Fallback chunking method (original logic)"""
        chunks = []
        text = text.strip()

        # Náº¿u text ngáº¯n hÆ¡n max_size, tráº£ vá» luÃ´n
        if len(text) <= max_size:
            return [text]

        # Chia text thÃ nh cÃ¡c chunks vá»›i overlap
        start = 0
        while start < len(text):
            end = start + max_size

            # Náº¿u khÃ´ng pháº£i chunk cuá»‘i, tÃ¬m vá»‹ trÃ­ ngáº¯t tá»± nhiÃªn
            if end < len(text):
                # TÃ¬m dáº¥u cÃ¢u hoáº·c khoáº£ng tráº¯ng gáº§n nháº¥t
                for i in range(end, start + max_size // 2, -1):
                    if text[i] in ".!?\n ":
                        end = i + 1
                        break

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # Di chuyá»ƒn start vá»›i overlap
            start = max(start + max_size - overlap, end)

            # TrÃ¡nh vÃ²ng láº·p vÃ´ háº¡n
            if start >= len(text):
                break

        return chunks

    async def search_textbook(
        self, book_id: str, query: str, limit: int = 5, semantic_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """TÃ¬m kiáº¿m trong sÃ¡ch giÃ¡o khoa báº±ng vector similarity"""
        from qdrant_client.http import models as qdrant_models
        self._ensure_service_initialized()

        if not self.qdrant_client or not self.embedding_model:
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            collection_name = f"textbook_{book_id}"

            # Kiá»ƒm tra xem collection cÃ³ tá»“n táº¡i khÃ´ng
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]

            if collection_name not in existing_names:
                return {
                    "success": False,
                    "error": f"Collection not found for book_id {book_id}. Please create embeddings first.",
                }

            # Táº¡o embedding cho query
            query_vector = self.embedding_model.encode(query).tolist()

            # Chuáº©n bá»‹ filters cho Qdrant
            qdrant_filter = None
            if semantic_filters:
                filter_conditions = []

                # Filter by semantic tags
                if "semantic_tags" in semantic_filters:
                    tag_types = semantic_filters["semantic_tags"]
                    if isinstance(tag_types, str):
                        tag_types = [tag_types]

                    # Táº¡o filter cho semantic_tags array
                    for tag_type in tag_types:
                        filter_conditions.append(
                            qdrant_models.FieldCondition(
                                key="semantic_tags",
                                match=qdrant_models.MatchAny(any=[tag_type])
                            )
                        )

                # Filter by difficulty
                if "difficulty" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="estimated_difficulty",
                            match=qdrant_models.MatchValue(value=semantic_filters["difficulty"])
                        )
                    )

                # Filter by content features
                if "has_examples" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_examples",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_examples"])
                        )
                    )

                if "has_formulas" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_formulas",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_formulas"])
                        )
                    )

                # Filter by lesson_id
                if "lesson_id" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="lesson_id",
                            match=qdrant_models.MatchValue(value=semantic_filters["lesson_id"])
                        )
                    )

                if filter_conditions:
                    qdrant_filter = qdrant_models.Filter(
                        must=filter_conditions
                    )

            # TÃ¬m kiáº¿m trong Qdrant
            search_result = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                query_filter=qdrant_filter,
                limit=limit,
                with_payload=True,
                score_threshold=0.3,  # Giáº£m threshold Ä‘á»ƒ cÃ³ nhiá»u káº¿t quáº£ hÆ¡n
            )

            # Format káº¿t quáº£
            results = []
            for scored_point in search_result:
                # Sá»­a lá»—i: Kiá»ƒm tra payload trÆ°á»›c khi truy cáº­p
                payload = scored_point.payload or {}

                # Bá» qua metadata point báº±ng cÃ¡ch check payload
                if payload.get("type") == "metadata":
                    continue

                # Chuáº©n bá»‹ semantic metadata
                semantic_tags = payload.get("semantic_tags", [])

                # Backward compatibility: convert old semantic_type to new format
                if not semantic_tags and "semantic_type" in payload:
                    semantic_tags = [{"type": payload["semantic_type"], "confidence": 0.8}]

                results.append(
                    {
                        "text": payload.get("text", ""),
                        "score": scored_point.score,
                        "chapter_title": payload.get("chapter_title", ""),
                        "lesson_title": payload.get("lesson_title", ""),
                        "lesson_id": payload.get("lesson_id", ""),
                        "chapter_id": payload.get("chapter_id", ""),
                        "type": payload.get("type", ""),
                        "semantic_tags": semantic_tags,
                        "key_concepts": payload.get("key_concepts", []),
                        "estimated_difficulty": payload.get("estimated_difficulty", "basic"),
                        "contains_examples": payload.get("contains_examples", False),
                        "contains_definitions": payload.get("contains_definitions", False),
                        "contains_formulas": payload.get("contains_formulas", False),
                        "analysis_method": payload.get("analysis_method", "unknown")
                    }
                )

            return {
                "success": True,
                "book_id": book_id,
                "query": query,
                "results": results,
            }

        except Exception as e:
            logger.error(f"Error searching textbook: {e}")
            return {"success": False, "error": str(e)}

    async def global_search(
        self, query: str, limit: int = 5, semantic_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """TÃ¬m kiáº¿m toÃ n bá»™ trong táº¥t cáº£ collections"""
        from qdrant_client.http import models as qdrant_models

        if not self.qdrant_client or not self.embedding_model:
            return {
                "success": False,
                "error": "Qdrant client or embedding model not initialized",
            }

        try:
            # Láº¥y táº¥t cáº£ collections
            collections = self.qdrant_client.get_collections().collections
            textbook_collections = [c.name for c in collections if c.name.startswith("textbook_")]

            if not textbook_collections:
                return {
                    "success": True,
                    "query": query,
                    "results": [],
                    "message": "No textbook collections found"
                }

            # Táº¡o embedding cho query
            query_vector = self.embedding_model.encode(query).tolist()

            # Chuáº©n bá»‹ filters cho Qdrant (tÃ¡i sá»­ dá»¥ng logic tá»« search_textbook)
            qdrant_filter = None
            if semantic_filters:
                filter_conditions = []

                # Filter by semantic tags
                if "semantic_tags" in semantic_filters:
                    tag_types = semantic_filters["semantic_tags"]
                    if isinstance(tag_types, str):
                        tag_types = [tag_types]

                    for tag_type in tag_types:
                        filter_conditions.append(
                            qdrant_models.FieldCondition(
                                key="semantic_tags",
                                match=qdrant_models.MatchAny(any=[tag_type])
                            )
                        )

                # Filter by difficulty
                if "difficulty" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="estimated_difficulty",
                            match=qdrant_models.MatchValue(value=semantic_filters["difficulty"])
                        )
                    )

                # Filter by content features
                if "has_examples" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_examples",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_examples"])
                        )
                    )

                if "has_formulas" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="contains_formulas",
                            match=qdrant_models.MatchValue(value=semantic_filters["has_formulas"])
                        )
                    )

                # Filter by lesson_id
                if "lesson_id" in semantic_filters:
                    filter_conditions.append(
                        qdrant_models.FieldCondition(
                            key="lesson_id",
                            match=qdrant_models.MatchValue(value=semantic_filters["lesson_id"])
                        )
                    )

                if filter_conditions:
                    qdrant_filter = qdrant_models.Filter(
                        must=filter_conditions
                    )

            # TÃ¬m kiáº¿m trong táº¥t cáº£ collections vÃ  gá»™p káº¿t quáº£
            all_results = []

            for collection_name in textbook_collections:
                try:
                    search_result = self.qdrant_client.search(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        query_filter=qdrant_filter,
                        limit=limit,
                        with_payload=True,
                        score_threshold=0.3,
                    )

                    # Extract book_id from collection name
                    book_id = collection_name.replace("textbook_", "")

                    # Format káº¿t quáº£
                    for scored_point in search_result:
                        payload = scored_point.payload or {}

                        # Bá» qua metadata point
                        if payload.get("type") == "metadata":
                            continue

                        # Chuáº©n bá»‹ semantic metadata
                        semantic_tags = payload.get("semantic_tags", [])

                        # Backward compatibility
                        if not semantic_tags and "semantic_type" in payload:
                            semantic_tags = [{"type": payload["semantic_type"], "confidence": 0.8}]

                        all_results.append({
                            "text": payload.get("text", ""),
                            "score": scored_point.score,
                            "book_id": book_id,
                            "chapter_title": payload.get("chapter_title", ""),
                            "lesson_title": payload.get("lesson_title", ""),
                            "lesson_id": payload.get("lesson_id", ""),
                            "chapter_id": payload.get("chapter_id", ""),
                            "type": payload.get("type", ""),
                            "semantic_tags": semantic_tags,
                            "key_concepts": payload.get("key_concepts", []),
                            "estimated_difficulty": payload.get("estimated_difficulty", "basic"),
                            "contains_examples": payload.get("contains_examples", False),
                            "contains_definitions": payload.get("contains_definitions", False),
                            "contains_formulas": payload.get("contains_formulas", False),
                            "analysis_method": payload.get("analysis_method", "unknown")
                        })

                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue

            # Sáº¯p xáº¿p theo score vÃ  giá»›i háº¡n káº¿t quáº£
            all_results.sort(key=lambda x: x["score"], reverse=True)
            final_results = all_results[:limit]

            return {
                "success": True,
                "query": query,
                "results": final_results,
                "collections_searched": len(textbook_collections),
                "total_results_found": len(all_results)
            }

        except Exception as e:
            logger.error(f"Error in global search: {e}")
            return {"success": False, "error": str(e)}

    async def delete_textbook_by_id(self, book_id: str) -> Dict[str, Any]:
        """
        XÃ³a textbook báº±ng book_id (xÃ³a collection trong Qdrant)
        
        Args:
            book_id: ID cá»§a textbook cáº§n xÃ³a
            
        Returns:
            Dict chá»©a káº¿t quáº£ xÃ³a
        """
        try:
            if not self.qdrant_client:
                return {
                    "success": False,
                    "error": "Qdrant client not initialized"
                }

            collection_name = f"textbook_{book_id}"
            
            # Kiá»ƒm tra collection cÃ³ tá»“n táº¡i khÃ´ng
            collections = self.qdrant_client.get_collections().collections
            existing_names = [c.name for c in collections]
            
            if collection_name not in existing_names:
                return {
                    "success": False,
                    "error": f"Textbook with ID '{book_id}' not found",
                    "book_id": book_id,
                    "collection_name": collection_name
                }
            
            # Láº¥y thÃ´ng tin vá» collection trÆ°á»›c khi xÃ³a
            collection_info = self.qdrant_client.get_collection(collection_name)
            vector_count = getattr(collection_info, 'vectors_count', 0)
            
            # XÃ³a collection
            self.qdrant_client.delete_collection(collection_name)
            logger.info(f"Deleted textbook collection: {collection_name}")
            
            return {
                "success": True,
                "message": f"Textbook '{book_id}' deleted successfully",
                "book_id": book_id,
                "collection_name": collection_name,
                "deleted_vectors": vector_count
            }
            
        except Exception as e:
            logger.error(f"Error deleting textbook {book_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "book_id": book_id
            }

    async def delete_textbook_by_lesson_id(self, lesson_id: str) -> Dict[str, Any]:
        """
        XÃ³a textbook báº±ng lesson_id (tÃ¬m collection chá»©a lesson_id rá»“i xÃ³a)

        Args:
            lesson_id: ID cá»§a lesson Ä‘á»ƒ tÃ¬m textbook cáº§n xÃ³a
            
        Returns:
            Dict chá»©a káº¿t quáº£ xÃ³a
        """
        try:
            from qdrant_client.http import models as qdrant_models
            if not self.qdrant_client:
                return {
                    "success": False,
                    "error": "Qdrant client not initialized"
                }

            # TÃ¬m collection chá»©a lesson_id
            collections = self.qdrant_client.get_collections().collections
            found_collection = None
            found_book_id = None

            for collection in collections:
                if collection.name.startswith("textbook_"):
                    try:
                        # TÃ¬m kiáº¿m lesson_id trong collection nÃ y
                        search_result = self.qdrant_client.scroll(
                            collection_name=collection.name,
                            scroll_filter=qdrant_models.Filter(
                                must=[
                                    qdrant_models.FieldCondition(
                                        key="lesson_id",
                                        match=qdrant_models.MatchValue(value=lesson_id),
                                    )
                                ]
                            ),
                            limit=1,
                            with_payload=True,
                        )

                        if search_result[0]:  # TÃ¬m tháº¥y lesson
                            found_collection = collection.name
                            found_book_id = collection.name.replace("textbook_", "")
                            break

                    except Exception as e:
                        logger.warning(f"Error searching in collection {collection.name}: {e}")
                        continue

            if not found_collection:
                return {
                    "success": False,
                    "error": f"No textbook found containing lesson_id: {lesson_id}",
                    "lesson_id": lesson_id
                }

            # Láº¥y thÃ´ng tin vá» collection trÆ°á»›c khi xÃ³a
            collection_info = self.qdrant_client.get_collection(found_collection)
            vector_count = getattr(collection_info, 'vectors_count', 0)
            
            # XÃ³a collection
            self.qdrant_client.delete_collection(found_collection)
            logger.info(f"Deleted textbook collection: {found_collection} (found by lesson_id: {lesson_id})")
            
            return {
                "success": True,
                "message": f"Textbook containing lesson '{lesson_id}' deleted successfully",
                "lesson_id": lesson_id,
                "book_id": found_book_id,
                "collection_name": found_collection,
                "deleted_vectors": vector_count
            }
            
        except Exception as e:
            logger.error(f"Error deleting textbook by lesson_id {lesson_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "lesson_id": lesson_id
            }



    def get_semantic_suggestions(self, book_id: str) -> Dict[str, Any]:
        """Láº¥y thá»‘ng kÃª semantic Ä‘á»ƒ gá»£i Ã½ filter options"""
        
        if not self.qdrant_client:
            return {"success": False, "error": "Qdrant client not initialized"}
        
        try:
            collection_name = f"textbook_{book_id}"
            
            # Get all points to analyze
            points = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=1000,  # Adjust based on your data size
                with_payload=True
            )[0]  # scroll returns (points, next_page_token)
            
            # Analyze semantic distribution
            semantic_stats = {
                "semantic_types": {},
                "difficulty_levels": {},
                "content_features": {
                    "has_examples": 0,
                    "has_definitions": 0,
                    "has_formulas": 0,
                },
                "concepts": {},
                "total_chunks": 0
            }
            
            for point in points:
                payload = point.payload or {}
                
                # Skip metadata points
                if payload.get("type") == "metadata":
                    continue
                    
                semantic_stats["total_chunks"] += 1
                
                # Count semantic tags (multi-label)
                semantic_tags = payload.get("semantic_tags", [])
                for tag_info in semantic_tags:
                    if isinstance(tag_info, dict) and "type" in tag_info:
                        tag_type = tag_info["type"]
                        semantic_stats["semantic_types"][tag_type] = (
                            semantic_stats["semantic_types"].get(tag_type, 0) + 1
                        )
                
                # Count difficulty levels
                difficulty = payload.get("estimated_difficulty", "basic")
                semantic_stats["difficulty_levels"][difficulty] = (
                    semantic_stats["difficulty_levels"].get(difficulty, 0) + 1
                )
                
                # Count content features
                if payload.get("contains_examples"):
                    semantic_stats["content_features"]["has_examples"] += 1
                if payload.get("contains_definitions"):
                    semantic_stats["content_features"]["has_definitions"] += 1
                if payload.get("contains_formulas"):
                    semantic_stats["content_features"]["has_formulas"] += 1
                
                # Count key concepts
                key_concepts = payload.get("key_concepts", [])
                for concept in key_concepts:
                    semantic_stats["concepts"][concept] = (
                        semantic_stats["concepts"].get(concept, 0) + 1
                    )
            
            # Sort and limit concepts
            sorted_concepts = sorted(
                semantic_stats["concepts"].items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:20]
            
            semantic_stats["top_concepts"] = dict(sorted_concepts)
            del semantic_stats["concepts"]  # Remove full list
            
            return {
                "success": True,
                "book_id": book_id,
                "statistics": semantic_stats
            }
            
        except Exception as e:
            logger.error(f"Error getting semantic suggestions: {e}")
            return {"success": False, "error": str(e)}


# Factory function Ä‘á»ƒ táº¡o QdrantService instance
def get_qdrant_service() -> QdrantService:
    """
    Táº¡o QdrantService instance má»›i

    Returns:
        QdrantService: Fresh instance
    """
    return QdrantService()
