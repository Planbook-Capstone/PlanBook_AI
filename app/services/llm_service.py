"""
LLM Service ƒë·ªÉ c·∫•u tr√∫c l·∫°i text b·∫±ng Gemini API ho·∫∑c OpenRouter API
"""
import logging
from typing import Dict, Any, Optional
import google.generativeai as genai
from app.core.config import settings
from app.services.openrouter_service import OpenRouterService

logger = logging.getLogger(__name__)

class LLMService:
    """
    Service s·ª≠ d·ª•ng Gemini API ho·∫∑c OpenRouter API ƒë·ªÉ c·∫•u tr√∫c l·∫°i text
    """

    def __init__(self):
        self.model = None
        self.openrouter_service = None
        self.use_openrouter = False
        self._init_llm_service()
    
    def _init_llm_service(self):
        """Initialize LLM service - prioritize OpenRouter, fallback to Gemini"""
        try:
            # ∆Øu ti√™n s·ª≠ d·ª•ng OpenRouter n·∫øu c√≥ API key
            if settings.OPENROUTER_API_KEY:
                logger.info("Initializing OpenRouter service...")
                self.openrouter_service = OpenRouterService()
                if self.openrouter_service.is_available():
                    self.use_openrouter = True
                    logger.info("OpenRouter service initialized successfully")
                    return
                else:
                    logger.warning("OpenRouter service not available, falling back to Gemini")

            # Fallback to Gemini API
            if settings.GEMINI_API_KEY:
                logger.info("Initializing Gemini API...")
                self._init_gemini()
            else:
                logger.warning("No API keys available. LLM features will be disabled.")

        except Exception as e:
            logger.error(f"Failed to initialize LLM service: {e}")
            self.model = None
            self.openrouter_service = None

    def _init_gemini(self):
        """Initialize Gemini API"""
        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            # Use the latest available model
            self.model = genai.GenerativeModel('models/gemini-1.5-flash-latest')
            logger.info("Gemini API initialized with gemini-1.5-flash-latest")

        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}")
            self.model = None
    
    async def format_cv_text(self, raw_text: str) -> Dict[str, Any]:
        """
        C·∫•u tr√∫c l·∫°i text CV th√†nh format ƒë·∫πp
        
        Args:
            raw_text: Text th√¥ t·ª´ PDF
            
        Returns:
            Dict ch·ª©a text ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c l·∫°i
        """
        try:
            if not self.is_available():
                return {
                    "success": False,
                    "error": "LLM service not available. Please set API keys.",
                    "formatted_text": raw_text
                }

            prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia HR v√† CV formatting chuy√™n nghi·ªáp. H√£y c·∫•u tr√∫c l·∫°i CV sau th√†nh format CHU·∫®N QU·ªêC T·∫æ, ƒë·∫πp m·∫Øt v√† chuy√™n nghi·ªáp v·ªõi LAYOUT ƒê·∫∏P.

Y√äU C·∫¶U FORMAT CHUY√äN NGHI·ªÜP:

1. **HEADER LAYOUT ƒê·∫∏P:**
   ```
   # T√äN ƒê·∫¶Y ƒê·ª¶
   ## Ch·ª©c danh chuy√™n nghi·ªáp

   üìß Email | üì± Phone | üîó LinkedIn | üíª GitHub
   üìç Location
   ```

2. **C·∫§U TR√öC CHU·∫®N:**
   - Professional Summary (2-3 c√¢u ng·∫Øn g·ªçn, highlight value)
   - Core Skills (d·∫°ng grid 3-4 c·ªôt, kh√¥ng qu√° d√†i 1 h√†ng)
   - Professional Experience (format ƒë·∫πp v·ªõi icons)
   - Education (compact, highlight achievements)
   - Key Projects (3-4 d·ª± √°n top, format card-style)
   - Technical Skills (ph√¢n lo·∫°i r√µ r√†ng theo nh√≥m)

3. **FORMATTING RULES ƒê·∫∏P:**
   - S·ª≠ d·ª•ng icons/emojis cho sections: üíº üéì üöÄ ‚ö°
   - Skills d·∫°ng grid: `Skill1` `Skill2` `Skill3` (3-4 per line)
   - Dates format: MM/YYYY - MM/YYYY
   - Bullet points v·ªõi action verbs m·∫°nh
   - Spacing ƒë·ªÅu ƒë·∫∑n, kh√¥ng qu√° d√†i 1 h√†ng
   - Contact info tr√™n nhi·ªÅu d√≤ng cho ƒë·∫πp

4. **LAYOUT OPTIMIZATION:**
   - Header: T√™n to, ch·ª©c danh nh·ªè h∆°n, contact info xu·ªëng d√≤ng
   - Skills: Chia th√†nh 3-4 skills per line
   - Experience: Company | Position | Duration (xu·ªëng d√≤ng)
   - Projects: Title tr√™n, description v√† tech stack xu·ªëng d√≤ng
   - Kh√¥ng ƒë·ªÉ text qu√° d√†i 1 h√†ng

5. **PROFESSIONAL CONTENT:**
   - Quantify achievements v·ªõi s·ªë li·ªáu
   - Action verbs: Developed, Implemented, Led, Achieved
   - Focus v√†o impact v√† results
   - Technical skills ph√¢n nh√≥m: Frontend | Backend | Database | Tools

Text CV c·∫ßn format:
{raw_text}

H√£y tr·∫£ v·ªÅ CV v·ªõi LAYOUT ƒê·∫∏P, kh√¥ng d√†i 1 h√†ng, format chuy√™n nghi·ªáp:
"""

            result = await self._generate_content(prompt)

            if result["success"]:
                return {
                    "success": True,
                    "formatted_text": result["text"],
                    "original_length": len(raw_text),
                    "formatted_length": len(result["text"])
                }
            else:
                return {
                    "success": False,
                    "error": result["error"],
                    "formatted_text": raw_text
                }
            
        except Exception as e:
            logger.error(f"Error formatting CV text: {e}")
            return {
                "success": False,
                "error": str(e),
                "formatted_text": raw_text
            }
    
    async def format_document_text(self, raw_text: str, document_type: str = "general") -> Dict[str, Any]:
        """
        C·∫•u tr√∫c l·∫°i text t√†i li·ªáu th√†nh format ƒë·∫πp
        
        Args:
            raw_text: Text th√¥ t·ª´ PDF
            document_type: Lo·∫°i t√†i li·ªáu (cv, report, letter, etc.)
            
        Returns:
            Dict ch·ª©a text ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c l·∫°i
        """
        try:
            if not self.is_available():
                return {
                    "success": False,
                    "error": "LLM service not available. Please set API keys.",
                    "formatted_text": raw_text
                }
            
            # T√πy ch·ªânh prompt theo lo·∫°i t√†i li·ªáu
            if document_type.lower() == "cv":
                return await self.format_cv_text(raw_text)

            # X·ª≠ l√Ω exam questions
            if document_type.lower() == "exam_questions":
                return await self.generate_exam_questions(raw_text)

            # N·∫øu l√† general nh∆∞ng c√≥ v·∫ª nh∆∞ CV, format nh∆∞ CV
            if document_type.lower() == "general" and self._is_cv_content(raw_text):
                return await self.format_cv_text(raw_text)

            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát khi text extraction th·∫•t b·∫°i
            if raw_text.startswith("[PDF_EXTRACTION_FAILED]") or raw_text.startswith("[PDF_PROCESSING_INFO]"):
                prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia h·ªó tr·ª£ x·ª≠ l√Ω t√†i li·ªáu. Ng∆∞·ªùi d√πng ƒë√£ g·∫∑p v·∫•n ƒë·ªÅ khi x·ª≠ l√Ω file PDF.

Th√¥ng tin l·ªói:
{raw_text}

H√£y t·∫°o m·ªôt th√¥ng b√°o h·ªØu √≠ch v√† chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát ƒë·ªÉ:
1. Gi·∫£i th√≠ch v·∫•n ƒë·ªÅ ƒë√£ x·∫£y ra
2. ƒê∆∞a ra c√°c gi·∫£i ph√°p kh·∫£ thi
3. H∆∞·ªõng d·∫´n c√°ch kh·∫Øc ph·ª•c
4. S·ª≠ d·ª•ng markdown formatting ƒë·ªÉ d·ªÖ ƒë·ªçc

Tr·∫£ v·ªÅ th√¥ng b√°o h·ªó tr·ª£:
"""
            else:
                prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ formatting v√† editing t√†i li·ªáu. H√£y c·∫•u tr√∫c l·∫°i t√†i li·ªáu sau th√†nh format ƒë·∫πp, d·ªÖ ƒë·ªçc v√† chuy√™n nghi·ªáp.

Y√™u c·∫ßu:
1. S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic
2. ƒê·ªãnh d·∫°ng r√µ r√†ng v·ªõi ti√™u ƒë·ªÅ v√† c·∫•u tr√∫c ph√¢n c·∫•p
3. S·ª≠a l·ªói ch√≠nh t·∫£ v√† ng·ªØ ph√°p
4. Gi·ªØ nguy√™n t·∫•t c·∫£ th√¥ng tin quan tr·ªçng
5. S·ª≠ d·ª•ng markdown formatting
6. Lo·∫°i b·ªè c√°c k√Ω t·ª± l·∫° v√† format l·ªói

Lo·∫°i t√†i li·ªáu: {document_type}

Text c·∫ßn format:
{raw_text}

H√£y tr·∫£ v·ªÅ t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c format ƒë·∫πp:
"""

            result = await self._generate_content(prompt)

            if result["success"]:
                return {
                    "success": True,
                    "formatted_text": result["text"],
                    "original_length": len(raw_text),
                    "formatted_length": len(result["text"]),
                    "document_type": document_type
                }
            else:
                return {
                    "success": False,
                    "error": result["error"],
                    "formatted_text": raw_text
                }
            
        except Exception as e:
            logger.error(f"Error formatting document text: {e}")
            return {
                "success": False,
                "error": str(e),
                "formatted_text": raw_text
            }
    
    def _is_cv_content(self, text: str) -> bool:
        """Check if text content looks like a CV/Resume"""
        cv_keywords = [
            'curriculum vitae', 'cv', 'resume', 'experience', 'education', 'skills',
            'objective', 'work experience', 'employment', 'projects', 'achievements',
            'qualifications', 'developer', 'engineer', 'manager', 'analyst',
            'university', 'college', 'degree', 'gpa', 'graduation',
            'github', 'linkedin', 'portfolio', 'email', 'phone',
            'kinh nghiem', 'hoc van', 'ky nang', 'du an', 'thanh tich',
            'bang cap', 'tot nghiep', 'cong ty', 'lam viec'
        ]

        text_lower = text.lower()
        keyword_count = sum(1 for keyword in cv_keywords if keyword in text_lower)

        # If more than 3 CV-related keywords found, likely a CV
        return keyword_count >= 3

    def is_available(self) -> bool:
        """Check if LLM service is available"""
        if self.use_openrouter:
            return self.openrouter_service is not None and self.openrouter_service.is_available()
        return self.model is not None

    async def generate_content(self, prompt: str, temperature: float = 0.1, max_tokens: int = 4096) -> Dict[str, Any]:
        """
        Public method ƒë·ªÉ g·ªçi LLM v·ªõi c√°c tham s·ªë t√πy ch·ªânh

        Args:
            prompt: Text prompt ƒë·ªÉ g·ª≠i t·ªõi model
            temperature: Temperature cho response (0.0 - 1.0)
            max_tokens: S·ªë token t·ªëi ƒëa cho response

        Returns:
            Dict ch·ª©a response t·ª´ LLM
        """
        try:
            if self.use_openrouter and self.openrouter_service:
                # S·ª≠ d·ª•ng OpenRouter v·ªõi tham s·ªë t√πy ch·ªânh
                result = await self.openrouter_service.generate_content(
                    prompt=prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return result

            elif self.model:
                # S·ª≠ d·ª•ng Gemini - Gemini kh√¥ng h·ªó tr·ª£ temperature v√† max_tokens nh∆∞ OpenRouter
                response = self.model.generate_content(prompt)
                if response and response.text:
                    return {
                        "success": True,
                        "text": response.text,
                        "error": None
                    }
                else:
                    return {
                        "success": False,
                        "text": "",
                        "error": "No response from Gemini API"
                    }
            else:
                return {
                    "success": False,
                    "text": "",
                    "error": "No LLM service available"
                }

        except Exception as e:
            logger.error(f"Error in generate_content: {e}")
            return {
                "success": False,
                "text": "",
                "error": str(e)
            }

    async def _generate_content(self, prompt: str) -> Dict[str, Any]:
        """
        Helper method ƒë·ªÉ g·ªçi LLM (OpenRouter ho·∫∑c Gemini)

        Args:
            prompt: Text prompt ƒë·ªÉ g·ª≠i t·ªõi model

        Returns:
            Dict ch·ª©a response t·ª´ LLM
        """
        try:
            if self.use_openrouter and self.openrouter_service:
                # S·ª≠ d·ª•ng OpenRouter
                result = await self.openrouter_service.generate_content(prompt)
                if result["success"]:
                    return {
                        "success": True,
                        "text": result["text"],
                        "error": None
                    }
                else:
                    return {
                        "success": False,
                        "text": "",
                        "error": result["error"]
                    }

            elif self.model:
                # S·ª≠ d·ª•ng Gemini
                response = self.model.generate_content(prompt)
                if response and response.text:
                    return {
                        "success": True,
                        "text": response.text,
                        "error": None
                    }
                else:
                    return {
                        "success": False,
                        "text": "",
                        "error": "No response from Gemini API"
                    }
            else:
                return {
                    "success": False,
                    "text": "",
                    "error": "No LLM service available"
                }

        except Exception as e:
            logger.error(f"Error in _generate_content: {e}")
            return {
                "success": False,
                "text": "",
                "error": str(e)
            }

    async def generate_exam_questions(self, prompt: str) -> Dict[str, Any]:
        """
        T·∫°o c√¢u h·ªèi thi tr·∫Øc nghi·ªám b·∫±ng LLM API

        Args:
            prompt: Prompt ƒë·ªÉ t·∫°o c√¢u h·ªèi

        Returns:
            Dict ch·ª©a response t·ª´ LLM
        """
        try:
            if not self.is_available():
                return {
                    "success": False,
                    "error": "LLM service not available. Please set API keys.",
                    "formatted_text": ""
                }

            # G·ªçi LLM API ƒë·ªÉ t·∫°o c√¢u h·ªèi
            result = await self._generate_content(prompt)

            if result["success"]:
                return {
                    "success": True,
                    "formatted_text": result["text"],
                    "error": None
                }
            else:
                return {
                    "success": False,
                    "error": result["error"],
                    "formatted_text": ""
                }

        except Exception as e:
            logger.error(f"Error generating exam questions: {e}")
            return {
                "success": False,
                "error": str(e),
                "formatted_text": ""
            }

# Global instance
llm_service = LLMService()
