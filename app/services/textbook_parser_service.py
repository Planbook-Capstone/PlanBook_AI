"""
Textbook Parser Service - Xá»­ lÃ½ sÃ¡ch giÃ¡o khoa thÃ nh cáº¥u trÃºc dá»¯ liá»‡u cho giÃ¡o Ã¡n
"""

import logging
import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import fitz  # PyMuPDF
from PIL import Image
import io
import base64
from concurrent.futures import ThreadPoolExecutor

from app.services.llm_service import llm_service

logger = logging.getLogger(__name__)


class TextbookParserService:
    """Service Ä‘á»ƒ parse sÃ¡ch giÃ¡o khoa thÃ nh cáº¥u trÃºc dá»¯ liá»‡u"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.output_base_path = Path("data/processed_textbooks")
        self.output_base_path.mkdir(parents=True, exist_ok=True)

    async def process_textbook(
        self, pdf_content: bytes, filename: str, book_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ sÃ¡ch giÃ¡o khoa thÃ nh cáº¥u trÃºc dá»¯ liá»‡u

        Args:
            pdf_content: Ná»™i dung PDF
            filename: TÃªn file
            book_metadata: Metadata cá»§a sÃ¡ch

        Returns:
            Dict chá»©a káº¿t quáº£ xá»­ lÃ½
        """
        try:
            logger.info(f"ğŸš€ Starting textbook processing: {filename}")

            # Táº¡o thÆ° má»¥c cho sÃ¡ch
            book_id = book_metadata.get("id", filename.replace(".pdf", ""))
            book_path = self.output_base_path / book_id
            book_path.mkdir(exist_ok=True)

            # Táº¡o thÆ° má»¥c con
            lessons_path = book_path / "lessons"
            images_path = book_path / "images"
            lessons_path.mkdir(exist_ok=True)
            images_path.mkdir(exist_ok=True)

            logger.info(f"ğŸ“ Created directories for book: {book_id}")

            # LÆ°u metadata
            await self._save_metadata(book_path, book_metadata, filename)
            logger.info(f"ğŸ’¾ Saved metadata for book: {book_id}")

            # Extract vÃ  phÃ¢n tÃ­ch PDF
            logger.info(f"ğŸ“„ Starting PDF extraction...")
            pages_data = await self._extract_pdf_pages(pdf_content)
            logger.info(f"âœ… Extracted {len(pages_data)} pages from PDF")

            # PhÃ¢n tÃ­ch cáº¥u trÃºc sÃ¡ch báº±ng LLM
            logger.info(f"ğŸ§  Analyzing book structure with LLM...")
            book_structure = await self._analyze_book_structure(pages_data)
            logger.info(
                f"ğŸ“š Detected {len(book_structure.get('chapters', []))} chapters"
            )

            # Xá»­ lÃ½ tá»«ng chÆ°Æ¡ng vÃ  bÃ i há»c
            chapters_processed = 0
            lessons_processed = 0
            total_chapters = len(book_structure.get("chapters", []))

            logger.info(f"ğŸ”„ Starting to process {total_chapters} chapters...")

            for i, chapter_data in enumerate(book_structure.get("chapters", []), 1):
                try:
                    logger.info(
                        f"ğŸ“– Processing chapter {i}/{total_chapters}: {chapter_data.get('chapter_title', 'Unknown')}"
                    )

                    chapter_result = await self._process_chapter_with_lessons(
                        chapter_data, pages_data, lessons_path, images_path
                    )
                    chapters_processed += 1
                    lessons_processed += chapter_result.get("lessons_count", 0)

                    logger.info(
                        f"âœ… Completed chapter {chapters_processed}/{total_chapters} - {chapter_result.get('lessons_count', 0)} lessons processed"
                    )
                    logger.info(
                        f"ğŸ“Š Progress: {chapters_processed}/{total_chapters} chapters, {lessons_processed} total lessons"
                    )

                except Exception as e:
                    logger.error(
                        f"âŒ Error processing chapter {chapter_data.get('chapter_title', 'Unknown')}: {e}"
                    )

            # Cáº­p nháº­t metadata vá»›i sá»‘ liá»‡u thá»±c táº¿
            book_metadata["chapters_count"] = chapters_processed
            book_metadata["lessons_count"] = lessons_processed
            book_metadata["book_structure"] = book_structure.get("book_info", {})
            await self._save_metadata(book_path, book_metadata, filename)

            return {
                "success": True,
                "book_id": book_id,
                "book_path": str(book_path),
                "lessons_processed": lessons_processed,
                "total_pages": len(pages_data),
                "message": f"Successfully processed {lessons_processed} lessons from textbook",
            }

        except Exception as e:
            logger.error(f"Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "book_id": book_metadata.get("id", filename),
                "message": "Failed to process textbook",
            }

    async def _save_metadata(
        self, book_path: Path, metadata: Dict[str, Any], filename: str
    ):
        """LÆ°u metadata cá»§a sÃ¡ch"""
        metadata_file = book_path / "metadata.json"

        # ThÃªm thÃ´ng tin bá»• sung
        metadata.update(
            {
                "original_filename": filename,
                "processed_date": str(asyncio.get_event_loop().time()),
                "structure_version": "1.0",
            }
        )

        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    async def _extract_pdf_pages(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract pages tá»« PDF vá»›i text vÃ  images"""

        def extract_pages():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text
                text = page.get_text("text")  # type: ignore

                # Extract images
                images = []
                image_list = page.get_images()

                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)

                        if pix.n - pix.alpha < 4:  # GRAY or RGB
                            img_data = pix.tobytes("png")
                            img_base64 = base64.b64encode(img_data).decode()

                            images.append(
                                {
                                    "index": img_index,
                                    "data": img_base64,
                                    "format": "png",
                                }
                            )

                        pix = None
                    except Exception as e:
                        logger.warning(
                            f"Error extracting image {img_index} from page {page_num}: {e}"
                        )

                pages_data.append(
                    {"page_number": page_num + 1, "text": text, "images": images}
                )

            doc.close()
            return pages_data

        return await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_pages
        )

    async def _analyze_book_structure(
        self, pages_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch cáº¥u trÃºc sÃ¡ch báº±ng LLM Ä‘á»ƒ extract thá»±c táº¿"""

        if not llm_service.is_available():
            logger.warning("LLM not available, using basic structure analysis")
            return await self._basic_structure_analysis(pages_data)

        # Táº¡o text tá»•ng há»£p tá»« Táº¤T Cáº¢ cÃ¡c trang Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§
        full_text = ""
        for page in pages_data:
            if page["text"].strip():  # Chá»‰ láº¥y trang cÃ³ text
                full_text += f"\n--- Trang {page['page_number']} ---\n{page['text']}"

        logger.info(
            f"Analyzing {len(pages_data)} pages with LLM for structure detection"
        )

        prompt = f"""
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch sÃ¡ch giÃ¡o khoa Viá»‡t Nam. HÃ£y phÃ¢n tÃ­ch TOÃ€N Bá»˜ ná»™i dung sÃ¡ch vÃ  tráº£ vá» cáº¥u trÃºc thá»±c táº¿.

YÃŠU Cáº¦U PHÃ‚N TÃCH:
1. Äá»c ká»¹ TOÃ€N Bá»˜ text tá»« táº¥t cáº£ cÃ¡c trang
2. XÃ¡c Ä‘á»‹nh cÃ¡c CHÆ¯Æ NG (Chapter) - thÆ°á»ng cÃ³ tiÃªu Ä‘á» lá»›n nhÆ° "CHÆ¯Æ NG 1", "CHÆ¯Æ NG I", etc.
3. Trong má»—i chÆ°Æ¡ng, xÃ¡c Ä‘á»‹nh cÃ¡c BÃ€I (Lesson) - thÆ°á»ng cÃ³ tiÃªu Ä‘á» nhÆ° "BÃ i 1", "BÃ i 2", etc.
4. XÃ¡c Ä‘á»‹nh ná»™i dung thá»±c táº¿ cá»§a tá»«ng bÃ i
5. Ghi chÃº trang báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cho má»—i pháº§n

Cáº¤U TRÃšC MONG MUá»N: SÃCH â†’ CHÆ¯Æ NG â†’ BÃ€I â†’ Ná»˜I DUNG

FORMAT JSON CHÃNH XÃC:
{{
  "book_info": {{
    "title": "TÃªn sÃ¡ch thá»±c táº¿ tá»« text",
    "total_chapters": sá»‘_chÆ°Æ¡ng_thá»±c_táº¿,
    "total_lessons": sá»‘_bÃ i_thá»±c_táº¿,
    "subject": "mÃ´n_há»c_tá»«_ná»™i_dung"
  }},
  "chapters": [
    {{
      "chapter_id": "chapter_01",
      "chapter_title": "TÃªn chÆ°Æ¡ng thá»±c táº¿",
      "start_page": trang_báº¯t_Ä‘áº§u,
      "end_page": trang_káº¿t_thÃºc,
      "lessons": [
        {{
          "lesson_id": "lesson_01",
          "lesson_title": "TÃªn bÃ i thá»±c táº¿",
          "start_page": trang_báº¯t_Ä‘áº§u,
          "end_page": trang_káº¿t_thÃºc,
          "content_summary": "TÃ³m táº¯t ná»™i dung bÃ i há»c"
        }}
      ]
    }}
  ]
}}

QUAN TRá»ŒNG:
- Pháº£i Ä‘á»c vÃ  phÃ¢n tÃ­ch TOÃ€N Bá»˜ text, khÃ´ng bá» sÃ³t
- TÃªn chÆ°Æ¡ng/bÃ i pháº£i chÃ­nh xÃ¡c tá»« text gá»‘c
- Sá»‘ trang pháº£i chÃ­nh xÃ¡c
- Náº¿u khÃ´ng cÃ³ cáº¥u trÃºc rÃµ rÃ ng, hÃ£y tá»± phÃ¢n chia há»£p lÃ½

Text tá»« sÃ¡ch giÃ¡o khoa (TOÃ€N Bá»˜):
{full_text[:8000]}

Tráº£ vá» JSON cáº¥u trÃºc thá»±c táº¿:
"""

        try:
            response = llm_service.model.generate_content(prompt)
            json_text = response.text.strip()

            # Clean JSON - cáº£i thiá»‡n viá»‡c xá»­ lÃ½
            if json_text.startswith("```json"):
                json_text = json_text[7:]
            if json_text.startswith("```"):
                json_text = json_text[3:]
            if json_text.endswith("```"):
                json_text = json_text[:-3]

            # TÃ¬m JSON há»£p lá»‡ trong response
            json_text = json_text.strip()

            # TÃ¬m vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a JSON
            start_idx = json_text.find("{")
            if start_idx == -1:
                raise ValueError("No JSON object found in response")

            # TÃ¬m vá»‹ trÃ­ káº¿t thÃºc JSON báº±ng cÃ¡ch Ä‘áº¿m dáº¥u ngoáº·c
            brace_count = 0
            end_idx = start_idx
            for i, char in enumerate(json_text[start_idx:], start_idx):
                if char == "{":
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

            # Extract JSON há»£p lá»‡
            clean_json = json_text[start_idx:end_idx]

            structure = json.loads(clean_json)
            logger.info(f"LLM detected {len(structure.get('chapters', []))} chapters")
            return structure

        except Exception as e:
            logger.error(f"LLM structure analysis failed: {e}")
            logger.debug(
                f"Raw LLM response: {response.text[:500] if 'response' in locals() else 'No response'}"
            )
            return await self._basic_structure_analysis(pages_data)

    async def _basic_structure_analysis(
        self, pages_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch cáº¥u trÃºc cÆ¡ báº£n khÃ´ng dÃ¹ng LLM - theo format CHÆ¯Æ NG â†’ BÃ€I"""

        total_pages = len(pages_data)

        # PhÃ¢n tÃ­ch text Ä‘á»ƒ tÃ¬m cáº¥u trÃºc cÆ¡ báº£n
        chapters = []

        # Chia thÃ nh 2-3 chÆ°Æ¡ng, má»—i chÆ°Æ¡ng cÃ³ 2-3 bÃ i
        pages_per_chapter = max(total_pages // 3, 1)  # Ãt nháº¥t 3 chÆ°Æ¡ng

        for chapter_num in range(1, 4):  # Táº¡o 3 chÆ°Æ¡ng
            chapter_start = (chapter_num - 1) * pages_per_chapter + 1
            chapter_end = min(chapter_num * pages_per_chapter, total_pages)

            if chapter_start > total_pages:
                break

            # Táº¡o 2-3 bÃ i trong má»—i chÆ°Æ¡ng
            lessons_in_chapter = []
            pages_per_lesson = max((chapter_end - chapter_start + 1) // 2, 1)

            for lesson_num in range(1, 3):  # 2 bÃ i má»—i chÆ°Æ¡ng
                lesson_start = chapter_start + (lesson_num - 1) * pages_per_lesson
                lesson_end = min(
                    chapter_start + lesson_num * pages_per_lesson - 1, chapter_end
                )

                if lesson_start > chapter_end:
                    break

                # Thá»­ extract tiÃªu Ä‘á» tá»« text
                lesson_title = self._extract_title_from_pages(
                    pages_data, lesson_start - 1, lesson_end - 1
                )
                if not lesson_title:
                    lesson_title = f"BÃ i {len(lessons_in_chapter) + 1}"

                lessons_in_chapter.append(
                    {
                        "lesson_id": f"lesson_{chapter_num:02d}_{lesson_num:02d}",
                        "lesson_title": lesson_title,
                        "start_page": lesson_start,
                        "end_page": lesson_end,
                        "content_summary": f"Ná»™i dung bÃ i há»c tá»« trang {lesson_start} Ä‘áº¿n {lesson_end}",
                    }
                )

            # Thá»­ extract tiÃªu Ä‘á» chÆ°Æ¡ng tá»« text
            chapter_title = self._extract_chapter_title_from_pages(
                pages_data, chapter_start - 1, chapter_end - 1
            )
            if not chapter_title:
                chapter_title = f"ChÆ°Æ¡ng {chapter_num}"

            chapters.append(
                {
                    "chapter_id": f"chapter_{chapter_num:02d}",
                    "chapter_title": chapter_title,
                    "start_page": chapter_start,
                    "end_page": chapter_end,
                    "lessons": lessons_in_chapter,
                }
            )

        return {
            "book_info": {
                "title": "SÃ¡ch giÃ¡o khoa",
                "total_chapters": len(chapters),
                "total_lessons": sum(len(ch["lessons"]) for ch in chapters),
                "subject": "ChÆ°a xÃ¡c Ä‘á»‹nh",
            },
            "chapters": chapters,
        }

    def _extract_title_from_pages(
        self, pages_data: List[Dict[str, Any]], start_idx: int, end_idx: int
    ) -> str:
        """Extract tiÃªu Ä‘á» tá»« cÃ¡c trang"""
        for i in range(start_idx, min(end_idx + 1, len(pages_data))):
            if i < 0:
                continue
            text = pages_data[i]["text"]
            lines = text.split("\n")
            for line in lines[:5]:  # Kiá»ƒm tra 5 dÃ²ng Ä‘áº§u
                line = line.strip()
                if len(line) > 5 and len(line) < 100:
                    if any(
                        keyword in line.lower()
                        for keyword in ["bÃ i", "lesson", "chÆ°Æ¡ng"]
                    ):
                        return line
        return ""

    def _extract_chapter_title_from_pages(
        self, pages_data: List[Dict[str, Any]], start_idx: int, end_idx: int
    ) -> str:
        """Extract tiÃªu Ä‘á» chÆ°Æ¡ng tá»« cÃ¡c trang"""
        for i in range(start_idx, min(end_idx + 1, len(pages_data))):
            if i < 0:
                continue
            text = pages_data[i]["text"]
            lines = text.split("\n")
            for line in lines[:3]:  # Kiá»ƒm tra 3 dÃ²ng Ä‘áº§u
                line = line.strip()
                if len(line) > 5 and len(line) < 100:
                    if any(
                        keyword in line.lower()
                        for keyword in ["chÆ°Æ¡ng", "chapter", "pháº§n"]
                    ):
                        return line
        return ""

    async def _process_lesson(
        self,
        lesson_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path,
    ):
        """Xá»­ lÃ½ má»™t bÃ i há»c"""

        lesson_id = lesson_data["lesson_id"]
        start_page = lesson_data.get("start_page", 1)
        end_page = lesson_data.get("end_page", len(pages_data))

        # Táº¡o thÆ° má»¥c cho images cá»§a bÃ i há»c
        lesson_images_path = images_path / lesson_id
        lesson_images_path.mkdir(exist_ok=True)

        # Xá»­ lÃ½ cÃ¡c chÆ°Æ¡ng trong bÃ i há»c
        processed_chapters = []

        for chapter in lesson_data.get("chapters", []):
            chapter_content = await self._process_chapter(
                chapter, pages_data, start_page, end_page, lesson_images_path
            )
            processed_chapters.append(chapter_content)

        # Táº¡o lesson JSON
        lesson_json = {
            "lesson_id": lesson_id,
            "title": lesson_data["title"],
            "chapters": processed_chapters,
        }

        # LÆ°u lesson file
        lesson_file = lessons_path / f"{lesson_id}.json"
        with open(lesson_file, "w", encoding="utf-8") as f:
            json.dump(lesson_json, f, indent=2, ensure_ascii=False)

    async def _process_chapter_with_lessons(
        self,
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path,
    ) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t chÆ°Æ¡ng vá»›i cÃ¡c bÃ i há»c bÃªn trong"""

        chapter_id = chapter_data["chapter_id"]
        chapter_title = chapter_data["chapter_title"]
        chapter_start = chapter_data.get("start_page", 1)
        chapter_end = chapter_data.get("end_page", len(pages_data))

        logger.info(
            f"Processing chapter: {chapter_title} (pages {chapter_start}-{chapter_end})"
        )

        # Táº¡o thÆ° má»¥c cho images cá»§a chÆ°Æ¡ng
        chapter_images_path = images_path / chapter_id
        chapter_images_path.mkdir(exist_ok=True)

        lessons_count = 0

        # Xá»­ lÃ½ tá»«ng bÃ i há»c trong chÆ°Æ¡ng
        for lesson_data in chapter_data.get("lessons", []):
            try:
                await self._process_lesson_in_chapter(
                    lesson_data,
                    chapter_data,
                    pages_data,
                    lessons_path,
                    chapter_images_path,
                )
                lessons_count += 1
                logger.info(
                    f"Processed lesson: {lesson_data.get('lesson_title', 'Unknown')}"
                )
            except Exception as e:
                logger.error(
                    f"Error processing lesson {lesson_data.get('lesson_title', 'Unknown')}: {e}"
                )

        return {"chapter_id": chapter_id, "lessons_count": lessons_count}

    async def _process_lesson_in_chapter(
        self,
        lesson_data: Dict[str, Any],
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path,
    ):
        """Xá»­ lÃ½ má»™t bÃ i há»c trong chÆ°Æ¡ng"""

        lesson_id = lesson_data["lesson_id"]
        lesson_title = lesson_data["lesson_title"]
        lesson_start = lesson_data.get("start_page", 1)
        lesson_end = lesson_data.get("end_page", len(pages_data))

        # Extract ná»™i dung thá»±c táº¿ tá»« cÃ¡c trang
        lesson_content = await self._extract_lesson_content(
            lesson_start, lesson_end, pages_data, images_path
        )

        # Táº¡o lesson JSON vá»›i cáº¥u trÃºc má»›i
        lesson_json = {
            "lesson_id": lesson_id,
            "lesson_title": lesson_title,
            "chapter_id": chapter_data["chapter_id"],
            "chapter_title": chapter_data["chapter_title"],
            "pages": {"start": lesson_start, "end": lesson_end},
            "content": lesson_content,
            "summary": lesson_data.get("content_summary", ""),
        }

        # LÆ°u lesson file
        lesson_file = lessons_path / f"{lesson_id}.json"
        with open(lesson_file, "w", encoding="utf-8") as f:
            json.dump(lesson_json, f, indent=2, ensure_ascii=False)

    async def _extract_lesson_content(
        self,
        start_page: int,
        end_page: int,
        pages_data: List[Dict[str, Any]],
        images_path: Path,
    ) -> List[Dict[str, Any]]:
        """Extract ná»™i dung thá»±c táº¿ tá»« cÃ¡c trang cá»§a bÃ i há»c"""

        content = []
        image_counter = 1

        for page_num in range(start_page - 1, min(end_page, len(pages_data))):
            if page_num < 0:
                continue

            page = pages_data[page_num]

            # ThÃªm text content náº¿u cÃ³
            if page["text"].strip():
                # Sá»­ dá»¥ng LLM Ä‘á»ƒ clean vÃ  format text
                cleaned_text = await self._clean_text_with_llm(page["text"])

                content.append(
                    {"type": "text", "page": page["page_number"], "data": cleaned_text}
                )

            # Xá»­ lÃ½ images trong trang
            for img_data in page["images"]:
                try:
                    # LÆ°u image
                    img_filename = f"img{image_counter}.png"
                    img_path = images_path / img_filename

                    # Decode vÃ  lÆ°u image
                    img_bytes = base64.b64decode(img_data["data"])
                    with open(img_path, "wb") as f:
                        f.write(img_bytes)

                    # Táº¡o mÃ´ táº£ image báº±ng LLM
                    img_description = await self._describe_image_with_llm(img_bytes)

                    # LÆ°u description
                    desc_path = images_path / f"img{image_counter}_description.txt"
                    with open(desc_path, "w", encoding="utf-8") as f:
                        f.write(img_description)

                    # ThÃªm vÃ o content (chá»‰ mÃ´ táº£, khÃ´ng lÆ°u base64)
                    content.append(
                        {
                            "type": "image",
                            "page": page["page_number"],
                            "description": img_description,
                            "local_path": f"images/{images_path.name}/img{image_counter}.png",
                        }
                    )

                    image_counter += 1

                except Exception as e:
                    logger.error(f"Error processing image {image_counter}: {e}")

        return content

    async def _clean_text_with_llm(self, raw_text: str) -> str:
        """Sá»­ dá»¥ng LLM Ä‘á»ƒ clean vÃ  format text tá»« PDF"""

        if not llm_service.is_available():
            return raw_text.strip()

        try:
            prompt = f"""
Báº¡n lÃ  chuyÃªn gia xá»­ lÃ½ text tá»« sÃ¡ch giÃ¡o khoa. HÃ£y lÃ m sáº¡ch vÃ  format láº¡i text sau:

YÃŠU Cáº¦U:
1. Sá»­a lá»—i OCR (kÃ½ tá»± bá»‹ nháº­n dáº¡ng sai)
2. Loáº¡i bá» kÃ½ tá»± láº¡, khoáº£ng tráº¯ng thá»«a
3. Sáº¯p xáº¿p láº¡i Ä‘oáº¡n vÄƒn cho dá»… Ä‘á»c
4. Giá»¯ nguyÃªn Ã½ nghÄ©a vÃ  cáº¥u trÃºc gá»‘c
5. Tráº£ vá» text tiáº¿ng Viá»‡t chuáº©n

Text gá»‘c:
{raw_text}

Text Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch:
"""

            response = llm_service.model.generate_content(prompt)
            cleaned_text = response.text.strip()

            return cleaned_text if cleaned_text else raw_text.strip()

        except Exception as e:
            logger.error(f"Error cleaning text with LLM: {e}")
            return raw_text.strip()

    async def _describe_image_with_llm(self, img_bytes: bytes) -> str:
        """Sá»­ dá»¥ng LLM Ä‘á»ƒ mÃ´ táº£ hÃ¬nh áº£nh"""

        if not llm_service.is_available():
            return (
                "HÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa (LLM khÃ´ng kháº£ dá»¥ng Ä‘á»ƒ mÃ´ táº£ chi tiáº¿t)"
            )

        try:
            # Táº¡o mÃ´ táº£ dá»±a trÃªn context (khÃ´ng cáº§n vision API)
            prompt = """
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch sÃ¡ch giÃ¡o khoa. HÃ£y táº¡o mÃ´ táº£ cho hÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa.

YÃŠU Cáº¦U MÃ” Táº¢:
1. MÃ´ táº£ cÃ³ thá»ƒ lÃ : biá»ƒu Ä‘á»“, cÃ´ng thá»©c, hÃ¬nh minh há»a, báº£ng biá»ƒu
2. Ná»™i dung liÃªn quan Ä‘áº¿n giÃ¡o dá»¥c (khoa há»c, toÃ¡n há»c, vÄƒn há»c, etc.)
3. Má»¥c Ä‘Ã­ch giÃ¡o dá»¥c vÃ  cÃ¡ch sá»­ dá»¥ng trong giáº£ng dáº¡y
4. MÃ´ táº£ ngáº¯n gá»n, rÃµ rÃ ng báº±ng tiáº¿ng Viá»‡t

Tráº£ vá» mÃ´ táº£ hÃ¬nh áº£nh giÃ¡o dá»¥c:
"""

            response = llm_service.model.generate_content(prompt)
            description = response.text.strip()

            return (
                description if description else "HÃ¬nh áº£nh minh há»a trong sÃ¡ch giÃ¡o khoa"
            )

        except Exception as e:
            logger.error(f"Error describing image with LLM: {e}")
            return "HÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa (khÃ´ng thá»ƒ táº¡o mÃ´ táº£)"

    async def _process_chapter(
        self,
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lesson_start: int,
        lesson_end: int,
        images_path: Path,
    ) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t chÆ°Æ¡ng trong bÃ i há»c"""

        chapter_start = max(chapter_data.get("start_page", lesson_start), lesson_start)
        chapter_end = min(chapter_data.get("end_page", lesson_end), lesson_end)

        # Láº¥y content tá»« cÃ¡c trang cá»§a chÆ°Æ¡ng
        chapter_content = []
        image_counter = 1

        for page_num in range(chapter_start - 1, chapter_end):  # -1 vÃ¬ index tá»« 0
            if page_num >= len(pages_data):
                break

            page = pages_data[page_num]

            # ThÃªm text content
            if page["text"].strip():
                chapter_content.append({"type": "text", "data": page["text"].strip()})

            # Xá»­ lÃ½ images trong trang
            for img_data in page["images"]:
                try:
                    # LÆ°u image
                    img_filename = f"img{image_counter}.png"
                    img_path = images_path / img_filename

                    # Decode vÃ  lÆ°u image
                    img_bytes = base64.b64decode(img_data["data"])
                    with open(img_path, "wb") as f:
                        f.write(img_bytes)

                    # Táº¡o mÃ´ táº£ image báº±ng LLM
                    img_description = await self._describe_image(img_bytes)

                    # LÆ°u description
                    desc_path = images_path / f"img{image_counter}_description.txt"
                    with open(desc_path, "w", encoding="utf-8") as f:
                        f.write(img_description)

                    # ThÃªm vÃ o content (chá»‰ mÃ´ táº£, khÃ´ng lÆ°u base64)
                    chapter_content.append(
                        {
                            "type": "image",
                            "description": img_description,
                            "local_path": f"images/{images_path.name}/img{image_counter}.png",
                        }
                    )

                    image_counter += 1

                except Exception as e:
                    logger.error(f"Error processing image {image_counter}: {e}")

        return {
            "chapter_id": chapter_data["chapter_id"],
            "title": chapter_data["title"],
            "content": chapter_content,
        }

    async def _describe_image(self, img_bytes: bytes) -> str:
        """Táº¡o mÃ´ táº£ cho hÃ¬nh áº£nh báº±ng LLM"""

        if not llm_service.is_available():
            return (
                "HÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa (LLM khÃ´ng kháº£ dá»¥ng Ä‘á»ƒ mÃ´ táº£ chi tiáº¿t)"
            )

        try:
            # Convert image to base64 for LLM
            img_base64 = base64.b64encode(img_bytes).decode()

            prompt = """
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch hÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa. HÃ£y mÃ´ táº£ hÃ¬nh áº£nh nÃ y má»™t cÃ¡ch chi tiáº¿t vÃ  cÃ³ Ã­ch cho viá»‡c táº¡o giÃ¡o Ã¡n.

YÃŠU Cáº¦U MÃ” Táº¢:
1. Ná»™i dung chÃ­nh cá»§a hÃ¬nh áº£nh
2. CÃ¡c yáº¿u tá»‘ quan trá»ng (biá»ƒu Ä‘á»“, cÃ´ng thá»©c, minh há»a)
3. Má»¥c Ä‘Ã­ch giÃ¡o dá»¥c cá»§a hÃ¬nh áº£nh
4. CÃ¡ch sá»­ dá»¥ng trong giáº£ng dáº¡y

Tráº£ vá» mÃ´ táº£ ngáº¯n gá»n, rÃµ rÃ ng báº±ng tiáº¿ng Viá»‡t:
"""

            # Note: Gemini vision API call would go here
            # For now, return a placeholder
            return (
                "HÃ¬nh áº£nh minh há»a trong sÃ¡ch giÃ¡o khoa - cáº§n cáº­p nháº­t mÃ´ táº£ chi tiáº¿t"
            )

        except Exception as e:
            logger.error(f"Error describing image: {e}")
            return "HÃ¬nh áº£nh trong sÃ¡ch giÃ¡o khoa (khÃ´ng thá»ƒ táº¡o mÃ´ táº£)"

    async def get_book_structure(self, book_id: str) -> Optional[Dict[str, Any]]:
        """Láº¥y cáº¥u trÃºc cá»§a má»™t cuá»‘n sÃ¡ch Ä‘Ã£ xá»­ lÃ½"""

        book_path = self.output_base_path / book_id
        metadata_file = book_path / "metadata.json"

        if not metadata_file.exists():
            return None

        try:
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            # Láº¥y danh sÃ¡ch lessons
            lessons_path = book_path / "lessons"
            lessons = []

            if lessons_path.exists():
                for lesson_file in sorted(lessons_path.glob("*.json")):
                    with open(lesson_file, "r", encoding="utf-8") as f:
                        lesson_data = json.load(f)
                        lessons.append(
                            {
                                "lesson_id": lesson_data["lesson_id"],
                                "title": lesson_data["title"],
                                "chapters_count": len(lesson_data.get("chapters", [])),
                            }
                        )

            return {
                "metadata": metadata,
                "lessons": lessons,
                "book_path": str(book_path),
            }

        except Exception as e:
            logger.error(f"Error reading book structure: {e}")
            return None

    async def get_lesson_content(
        self, book_id: str, lesson_id: str
    ) -> Optional[Dict[str, Any]]:
        """Láº¥y ná»™i dung chi tiáº¿t cá»§a má»™t bÃ i há»c"""

        book_path = self.output_base_path / book_id
        lesson_file = book_path / "lessons" / f"{lesson_id}.json"

        if not lesson_file.exists():
            return None

        try:
            with open(lesson_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error reading lesson content: {e}")
            return None


# Global instance
textbook_parser_service = TextbookParserService()
