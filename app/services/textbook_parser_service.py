"""
Textbook Parser Service - X·ª≠ l√Ω s√°ch gi√°o khoa th√†nh c·∫•u tr√∫c d·ªØ li·ªáu cho gi√°o √°n
"""
import logging
import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import fitz  # PyMuPDF
from PIL import Image
import io
import base64
from concurrent.futures import ThreadPoolExecutor

from app.services.llm_service import llm_service

logger = logging.getLogger(__name__)

class TextbookParserService:
    """Service ƒë·ªÉ parse s√°ch gi√°o khoa th√†nh c·∫•u tr√∫c d·ªØ li·ªáu"""
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.output_base_path = Path("data/processed_textbooks")
        self.output_base_path.mkdir(parents=True, exist_ok=True)
    
    async def process_textbook(
        self, 
        pdf_content: bytes, 
        filename: str,
        book_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω s√°ch gi√°o khoa th√†nh c·∫•u tr√∫c d·ªØ li·ªáu
        
        Args:
            pdf_content: N·ªôi dung PDF
            filename: T√™n file
            book_metadata: Metadata c·ªßa s√°ch
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ x·ª≠ l√Ω
        """
        try:
            logger.info(f"üöÄ Starting textbook processing: {filename}")

            # T·∫°o th∆∞ m·ª•c cho s√°ch
            book_id = book_metadata.get('id', filename.replace('.pdf', ''))
            book_path = self.output_base_path / book_id
            book_path.mkdir(exist_ok=True)

            # T·∫°o th∆∞ m·ª•c con
            lessons_path = book_path / "lessons"
            images_path = book_path / "images"
            lessons_path.mkdir(exist_ok=True)
            images_path.mkdir(exist_ok=True)

            logger.info(f"üìÅ Created directories for book: {book_id}")

            # L∆∞u metadata
            await self._save_metadata(book_path, book_metadata, filename)
            logger.info(f"üíæ Saved metadata for book: {book_id}")

            # Extract v√† ph√¢n t√≠ch PDF
            logger.info(f"üìÑ Starting PDF extraction...")
            pages_data = await self._extract_pdf_pages(pdf_content)
            logger.info(f"‚úÖ Extracted {len(pages_data)} pages from PDF")

            # Ph√¢n t√≠ch c·∫•u tr√∫c s√°ch b·∫±ng LLM
            logger.info(f"üß† Analyzing book structure with LLM...")
            book_structure = await self._analyze_book_structure(pages_data)
            logger.info(f"üìö Detected {len(book_structure.get('chapters', []))} chapters")

            # X·ª≠ l√Ω t·ª´ng ch∆∞∆°ng v√† b√†i h·ªçc
            chapters_processed = 0
            lessons_processed = 0
            total_chapters = len(book_structure.get('chapters', []))

            logger.info(f"üîÑ Starting to process {total_chapters} chapters...")

            for i, chapter_data in enumerate(book_structure.get('chapters', []), 1):
                try:
                    logger.info(f"üìñ Processing chapter {i}/{total_chapters}: {chapter_data.get('chapter_title', 'Unknown')}")

                    chapter_result = await self._process_chapter_with_lessons(
                        chapter_data,
                        pages_data,
                        lessons_path,
                        images_path
                    )
                    chapters_processed += 1
                    lessons_processed += chapter_result.get('lessons_count', 0)

                    logger.info(f"‚úÖ Completed chapter {chapters_processed}/{total_chapters} - {chapter_result.get('lessons_count', 0)} lessons processed")
                    logger.info(f"üìä Progress: {chapters_processed}/{total_chapters} chapters, {lessons_processed} total lessons")

                except Exception as e:
                    logger.error(f"‚ùå Error processing chapter {chapter_data.get('chapter_title', 'Unknown')}: {e}")

            # C·∫≠p nh·∫≠t metadata v·ªõi s·ªë li·ªáu th·ª±c t·∫ø
            book_metadata['chapters_count'] = chapters_processed
            book_metadata['lessons_count'] = lessons_processed
            book_metadata['book_structure'] = book_structure.get('book_info', {})
            await self._save_metadata(book_path, book_metadata, filename)
            
            return {
                "success": True,
                "book_id": book_id,
                "book_path": str(book_path),
                "lessons_processed": lessons_processed,
                "total_pages": len(pages_data),
                "message": f"Successfully processed {lessons_processed} lessons from textbook"
            }
            
        except Exception as e:
            logger.error(f"Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "book_id": book_metadata.get('id', filename),
                "message": "Failed to process textbook"
            }
    
    async def _save_metadata(self, book_path: Path, metadata: Dict[str, Any], filename: str):
        """L∆∞u metadata c·ªßa s√°ch"""
        metadata_file = book_path / "metadata.json"
        
        # Th√™m th√¥ng tin b·ªï sung
        metadata.update({
            "original_filename": filename,
            "processed_date": str(asyncio.get_event_loop().time()),
            "structure_version": "1.0"
        })
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    async def _extract_pdf_pages(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract pages t·ª´ PDF v·ªõi text v√† images"""
        
        def extract_pages():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                
                # Extract text
                text = page.get_text("text")  # type: ignore
                
                # Extract images
                images = []
                image_list = page.get_images()
                
                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)
                        
                        if pix.n - pix.alpha < 4:  # GRAY or RGB
                            img_data = pix.tobytes("png")
                            img_base64 = base64.b64encode(img_data).decode()
                            
                            images.append({
                                "index": img_index,
                                "data": img_base64,
                                "format": "png"
                            })
                        
                        pix = None
                    except Exception as e:
                        logger.warning(f"Error extracting image {img_index} from page {page_num}: {e}")
                
                pages_data.append({
                    "page_number": page_num + 1,
                    "text": text,
                    "images": images
                })
            
            doc.close()
            return pages_data
        
        return await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_pages
        )
    
    async def _analyze_book_structure(self, pages_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫•u tr√∫c s√°ch b·∫±ng LLM ƒë·ªÉ extract th·ª±c t·∫ø"""

        if not llm_service.is_available():
            logger.warning("LLM not available, using basic structure analysis")
            return await self._basic_structure_analysis(pages_data)

        # T·∫°o text t·ªïng h·ª£p t·ª´ T·∫§T C·∫¢ c√°c trang ƒë·ªÉ ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß
        full_text = ""
        for page in pages_data:
            if page['text'].strip():  # Ch·ªâ l·∫•y trang c√≥ text
                full_text += f"\n--- Trang {page['page_number']} ---\n{page['text']}"

        logger.info(f"Analyzing {len(pages_data)} pages with LLM for structure detection")

        prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch s√°ch gi√°o khoa Vi·ªát Nam. H√£y ph√¢n t√≠ch TO√ÄN B·ªò n·ªôi dung s√°ch v√† tr·∫£ v·ªÅ c·∫•u tr√∫c th·ª±c t·∫ø.

Y√äU C·∫¶U PH√ÇN T√çCH:
1. ƒê·ªçc k·ªπ TO√ÄN B·ªò text t·ª´ t·∫•t c·∫£ c√°c trang
2. X√°c ƒë·ªãnh c√°c CH∆Ø∆†NG (Chapter) - th∆∞·ªùng c√≥ ti√™u ƒë·ªÅ l·ªõn nh∆∞ "CH∆Ø∆†NG 1", "CH∆Ø∆†NG I", etc.
3. Trong m·ªói ch∆∞∆°ng, x√°c ƒë·ªãnh c√°c B√ÄI (Lesson) - th∆∞·ªùng c√≥ ti√™u ƒë·ªÅ nh∆∞ "B√†i 1", "B√†i 2", etc.
4. X√°c ƒë·ªãnh n·ªôi dung th·ª±c t·∫ø c·ªßa t·ª´ng b√†i
5. Ghi ch√∫ trang b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c cho m·ªói ph·∫ßn

C·∫§U TR√öC MONG MU·ªêN: S√ÅCH ‚Üí CH∆Ø∆†NG ‚Üí B√ÄI ‚Üí N·ªòI DUNG

FORMAT JSON CH√çNH X√ÅC:
{{
  "book_info": {{
    "title": "T√™n s√°ch th·ª±c t·∫ø t·ª´ text",
    "total_chapters": s·ªë_ch∆∞∆°ng_th·ª±c_t·∫ø,
    "total_lessons": s·ªë_b√†i_th·ª±c_t·∫ø,
    "subject": "m√¥n_h·ªçc_t·ª´_n·ªôi_dung"
  }},
  "chapters": [
    {{
      "chapter_id": "chapter_01",
      "chapter_title": "T√™n ch∆∞∆°ng th·ª±c t·∫ø",
      "start_page": trang_b·∫Øt_ƒë·∫ßu,
      "end_page": trang_k·∫øt_th√∫c,
      "lessons": [
        {{
          "lesson_id": "lesson_01",
          "lesson_title": "T√™n b√†i th·ª±c t·∫ø",
          "start_page": trang_b·∫Øt_ƒë·∫ßu,
          "end_page": trang_k·∫øt_th√∫c,
          "content_summary": "T√≥m t·∫Øt n·ªôi dung b√†i h·ªçc"
        }}
      ]
    }}
  ]
}}

QUAN TR·ªåNG:
- Ph·∫£i ƒë·ªçc v√† ph√¢n t√≠ch TO√ÄN B·ªò text, kh√¥ng b·ªè s√≥t
- T√™n ch∆∞∆°ng/b√†i ph·∫£i ch√≠nh x√°c t·ª´ text g·ªëc
- S·ªë trang ph·∫£i ch√≠nh x√°c
- N·∫øu kh√¥ng c√≥ c·∫•u tr√∫c r√µ r√†ng, h√£y t·ª± ph√¢n chia h·ª£p l√Ω

Text t·ª´ s√°ch gi√°o khoa (TO√ÄN B·ªò):
{full_text[:8000]}

Tr·∫£ v·ªÅ JSON c·∫•u tr√∫c th·ª±c t·∫ø:
"""

        try:
            response = llm_service.model.generate_content(prompt)
            json_text = response.text.strip()

            # Clean JSON
            if json_text.startswith('```json'):
                json_text = json_text[7:]
            if json_text.startswith('```'):
                json_text = json_text[3:]
            if json_text.endswith('```'):
                json_text = json_text[:-3]

            structure = json.loads(json_text)
            logger.info(f"LLM detected {len(structure.get('chapters', []))} chapters")
            return structure

        except Exception as e:
            logger.error(f"LLM structure analysis failed: {e}")
            return await self._basic_structure_analysis(pages_data)
    
    async def _basic_structure_analysis(self, pages_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫•u tr√∫c c∆° b·∫£n kh√¥ng d√πng LLM - theo format CH∆Ø∆†NG ‚Üí B√ÄI"""

        total_pages = len(pages_data)

        # Ph√¢n t√≠ch text ƒë·ªÉ t√¨m c·∫•u tr√∫c c∆° b·∫£n
        chapters = []

        # Chia th√†nh 2-3 ch∆∞∆°ng, m·ªói ch∆∞∆°ng c√≥ 2-3 b√†i
        pages_per_chapter = max(total_pages // 3, 1)  # √çt nh·∫•t 3 ch∆∞∆°ng

        for chapter_num in range(1, 4):  # T·∫°o 3 ch∆∞∆°ng
            chapter_start = (chapter_num - 1) * pages_per_chapter + 1
            chapter_end = min(chapter_num * pages_per_chapter, total_pages)

            if chapter_start > total_pages:
                break

            # T·∫°o 2-3 b√†i trong m·ªói ch∆∞∆°ng
            lessons_in_chapter = []
            pages_per_lesson = max((chapter_end - chapter_start + 1) // 2, 1)

            for lesson_num in range(1, 3):  # 2 b√†i m·ªói ch∆∞∆°ng
                lesson_start = chapter_start + (lesson_num - 1) * pages_per_lesson
                lesson_end = min(chapter_start + lesson_num * pages_per_lesson - 1, chapter_end)

                if lesson_start > chapter_end:
                    break

                # Th·ª≠ extract ti√™u ƒë·ªÅ t·ª´ text
                lesson_title = self._extract_title_from_pages(pages_data, lesson_start - 1, lesson_end - 1)
                if not lesson_title:
                    lesson_title = f"B√†i {len(lessons_in_chapter) + 1}"

                lessons_in_chapter.append({
                    "lesson_id": f"lesson_{chapter_num:02d}_{lesson_num:02d}",
                    "lesson_title": lesson_title,
                    "start_page": lesson_start,
                    "end_page": lesson_end,
                    "content_summary": f"N·ªôi dung b√†i h·ªçc t·ª´ trang {lesson_start} ƒë·∫øn {lesson_end}"
                })

            # Th·ª≠ extract ti√™u ƒë·ªÅ ch∆∞∆°ng t·ª´ text
            chapter_title = self._extract_chapter_title_from_pages(pages_data, chapter_start - 1, chapter_end - 1)
            if not chapter_title:
                chapter_title = f"Ch∆∞∆°ng {chapter_num}"

            chapters.append({
                "chapter_id": f"chapter_{chapter_num:02d}",
                "chapter_title": chapter_title,
                "start_page": chapter_start,
                "end_page": chapter_end,
                "lessons": lessons_in_chapter
            })

        return {
            "book_info": {
                "title": "S√°ch gi√°o khoa",
                "total_chapters": len(chapters),
                "total_lessons": sum(len(ch['lessons']) for ch in chapters),
                "subject": "Ch∆∞a x√°c ƒë·ªãnh"
            },
            "chapters": chapters
        }

    def _extract_title_from_pages(self, pages_data: List[Dict[str, Any]], start_idx: int, end_idx: int) -> str:
        """Extract ti√™u ƒë·ªÅ t·ª´ c√°c trang"""
        for i in range(start_idx, min(end_idx + 1, len(pages_data))):
            if i < 0:
                continue
            text = pages_data[i]['text']
            lines = text.split('\n')
            for line in lines[:5]:  # Ki·ªÉm tra 5 d√≤ng ƒë·∫ßu
                line = line.strip()
                if len(line) > 5 and len(line) < 100:
                    if any(keyword in line.lower() for keyword in ['b√†i', 'lesson', 'ch∆∞∆°ng']):
                        return line
        return ""

    def _extract_chapter_title_from_pages(self, pages_data: List[Dict[str, Any]], start_idx: int, end_idx: int) -> str:
        """Extract ti√™u ƒë·ªÅ ch∆∞∆°ng t·ª´ c√°c trang"""
        for i in range(start_idx, min(end_idx + 1, len(pages_data))):
            if i < 0:
                continue
            text = pages_data[i]['text']
            lines = text.split('\n')
            for line in lines[:3]:  # Ki·ªÉm tra 3 d√≤ng ƒë·∫ßu
                line = line.strip()
                if len(line) > 5 and len(line) < 100:
                    if any(keyword in line.lower() for keyword in ['ch∆∞∆°ng', 'chapter', 'ph·∫ßn']):
                        return line
        return ""
    
    async def _process_lesson(
        self, 
        lesson_data: Dict[str, Any], 
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path
    ):
        """X·ª≠ l√Ω m·ªôt b√†i h·ªçc"""
        
        lesson_id = lesson_data['lesson_id']
        start_page = lesson_data.get('start_page', 1)
        end_page = lesson_data.get('end_page', len(pages_data))
        
        # T·∫°o th∆∞ m·ª•c cho images c·ªßa b√†i h·ªçc
        lesson_images_path = images_path / lesson_id
        lesson_images_path.mkdir(exist_ok=True)
        
        # X·ª≠ l√Ω c√°c ch∆∞∆°ng trong b√†i h·ªçc
        processed_chapters = []
        
        for chapter in lesson_data.get('chapters', []):
            chapter_content = await self._process_chapter(
                chapter, 
                pages_data, 
                start_page, 
                end_page,
                lesson_images_path
            )
            processed_chapters.append(chapter_content)
        
        # T·∫°o lesson JSON
        lesson_json = {
            "lesson_id": lesson_id,
            "title": lesson_data['title'],
            "chapters": processed_chapters
        }
        
        # L∆∞u lesson file
        lesson_file = lessons_path / f"{lesson_id}.json"
        with open(lesson_file, 'w', encoding='utf-8') as f:
            json.dump(lesson_json, f, indent=2, ensure_ascii=False)

    async def _process_chapter_with_lessons(
        self,
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω m·ªôt ch∆∞∆°ng v·ªõi c√°c b√†i h·ªçc b√™n trong"""

        chapter_id = chapter_data['chapter_id']
        chapter_title = chapter_data['chapter_title']
        chapter_start = chapter_data.get('start_page', 1)
        chapter_end = chapter_data.get('end_page', len(pages_data))

        logger.info(f"Processing chapter: {chapter_title} (pages {chapter_start}-{chapter_end})")

        # T·∫°o th∆∞ m·ª•c cho images c·ªßa ch∆∞∆°ng
        chapter_images_path = images_path / chapter_id
        chapter_images_path.mkdir(exist_ok=True)

        lessons_count = 0

        # X·ª≠ l√Ω t·ª´ng b√†i h·ªçc trong ch∆∞∆°ng
        for lesson_data in chapter_data.get('lessons', []):
            try:
                await self._process_lesson_in_chapter(
                    lesson_data,
                    chapter_data,
                    pages_data,
                    lessons_path,
                    chapter_images_path
                )
                lessons_count += 1
                logger.info(f"Processed lesson: {lesson_data.get('lesson_title', 'Unknown')}")
            except Exception as e:
                logger.error(f"Error processing lesson {lesson_data.get('lesson_title', 'Unknown')}: {e}")

        return {
            "chapter_id": chapter_id,
            "lessons_count": lessons_count
        }

    async def _process_lesson_in_chapter(
        self,
        lesson_data: Dict[str, Any],
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lessons_path: Path,
        images_path: Path
    ):
        """X·ª≠ l√Ω m·ªôt b√†i h·ªçc trong ch∆∞∆°ng"""

        lesson_id = lesson_data['lesson_id']
        lesson_title = lesson_data['lesson_title']
        lesson_start = lesson_data.get('start_page', 1)
        lesson_end = lesson_data.get('end_page', len(pages_data))

        # Extract n·ªôi dung th·ª±c t·∫ø t·ª´ c√°c trang
        lesson_content = await self._extract_lesson_content(
            lesson_start,
            lesson_end,
            pages_data,
            images_path
        )

        # T·∫°o lesson JSON v·ªõi c·∫•u tr√∫c m·ªõi
        lesson_json = {
            "lesson_id": lesson_id,
            "lesson_title": lesson_title,
            "chapter_id": chapter_data['chapter_id'],
            "chapter_title": chapter_data['chapter_title'],
            "pages": {
                "start": lesson_start,
                "end": lesson_end
            },
            "content": lesson_content,
            "summary": lesson_data.get('content_summary', '')
        }

        # L∆∞u lesson file
        lesson_file = lessons_path / f"{lesson_id}.json"
        with open(lesson_file, 'w', encoding='utf-8') as f:
            json.dump(lesson_json, f, indent=2, ensure_ascii=False)

    async def _extract_lesson_content(
        self,
        start_page: int,
        end_page: int,
        pages_data: List[Dict[str, Any]],
        images_path: Path
    ) -> List[Dict[str, Any]]:
        """Extract n·ªôi dung th·ª±c t·∫ø t·ª´ c√°c trang c·ªßa b√†i h·ªçc"""

        content = []
        image_counter = 1

        for page_num in range(start_page - 1, min(end_page, len(pages_data))):
            if page_num < 0:
                continue

            page = pages_data[page_num]

            # Th√™m text content n·∫øu c√≥
            if page['text'].strip():
                # S·ª≠ d·ª•ng LLM ƒë·ªÉ clean v√† format text
                cleaned_text = await self._clean_text_with_llm(page['text'])

                content.append({
                    "type": "text",
                    "page": page['page_number'],
                    "data": cleaned_text
                })

            # X·ª≠ l√Ω images trong trang
            for img_data in page['images']:
                try:
                    # L∆∞u image
                    img_filename = f"img{image_counter}.png"
                    img_path = images_path / img_filename

                    # Decode v√† l∆∞u image
                    img_bytes = base64.b64decode(img_data['data'])
                    with open(img_path, 'wb') as f:
                        f.write(img_bytes)

                    # T·∫°o m√¥ t·∫£ image b·∫±ng LLM
                    img_description = await self._describe_image_with_llm(img_bytes)

                    # L∆∞u description
                    desc_path = images_path / f"img{image_counter}_description.txt"
                    with open(desc_path, 'w', encoding='utf-8') as f:
                        f.write(img_description)

                    # Th√™m v√†o content (ch·ªâ m√¥ t·∫£, kh√¥ng l∆∞u base64)
                    content.append({
                        "type": "image",
                        "page": page['page_number'],
                        "description": img_description,
                        "local_path": f"images/{images_path.name}/img{image_counter}.png"
                    })

                    image_counter += 1

                except Exception as e:
                    logger.error(f"Error processing image {image_counter}: {e}")

        return content

    async def _clean_text_with_llm(self, raw_text: str) -> str:
        """S·ª≠ d·ª•ng LLM ƒë·ªÉ clean v√† format text t·ª´ PDF"""

        if not llm_service.is_available():
            return raw_text.strip()

        try:
            prompt = f"""
B·∫°n l√† chuy√™n gia x·ª≠ l√Ω text t·ª´ s√°ch gi√°o khoa. H√£y l√†m s·∫°ch v√† format l·∫°i text sau:

Y√äU C·∫¶U:
1. S·ª≠a l·ªói OCR (k√Ω t·ª± b·ªã nh·∫≠n d·∫°ng sai)
2. Lo·∫°i b·ªè k√Ω t·ª± l·∫°, kho·∫£ng tr·∫Øng th·ª´a
3. S·∫Øp x·∫øp l·∫°i ƒëo·∫°n vƒÉn cho d·ªÖ ƒë·ªçc
4. Gi·ªØ nguy√™n √Ω nghƒ©a v√† c·∫•u tr√∫c g·ªëc
5. Tr·∫£ v·ªÅ text ti·∫øng Vi·ªát chu·∫©n

Text g·ªëc:
{raw_text}

Text ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch:
"""

            response = llm_service.model.generate_content(prompt)
            cleaned_text = response.text.strip()

            return cleaned_text if cleaned_text else raw_text.strip()

        except Exception as e:
            logger.error(f"Error cleaning text with LLM: {e}")
            return raw_text.strip()

    async def _describe_image_with_llm(self, img_bytes: bytes) -> str:
        """S·ª≠ d·ª•ng LLM ƒë·ªÉ m√¥ t·∫£ h√¨nh ·∫£nh"""

        if not llm_service.is_available():
            return "H√¨nh ·∫£nh trong s√°ch gi√°o khoa (LLM kh√¥ng kh·∫£ d·ª•ng ƒë·ªÉ m√¥ t·∫£ chi ti·∫øt)"

        try:
            # T·∫°o m√¥ t·∫£ d·ª±a tr√™n context (kh√¥ng c·∫ßn vision API)
            prompt = """
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch s√°ch gi√°o khoa. H√£y t·∫°o m√¥ t·∫£ cho h√¨nh ·∫£nh trong s√°ch gi√°o khoa.

Y√äU C·∫¶U M√î T·∫¢:
1. M√¥ t·∫£ c√≥ th·ªÉ l√†: bi·ªÉu ƒë·ªì, c√¥ng th·ª©c, h√¨nh minh h·ªça, b·∫£ng bi·ªÉu
2. N·ªôi dung li√™n quan ƒë·∫øn gi√°o d·ª•c (khoa h·ªçc, to√°n h·ªçc, vƒÉn h·ªçc, etc.)
3. M·ª•c ƒë√≠ch gi√°o d·ª•c v√† c√°ch s·ª≠ d·ª•ng trong gi·∫£ng d·∫°y
4. M√¥ t·∫£ ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát

Tr·∫£ v·ªÅ m√¥ t·∫£ h√¨nh ·∫£nh gi√°o d·ª•c:
"""

            response = llm_service.model.generate_content(prompt)
            description = response.text.strip()

            return description if description else "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa"

        except Exception as e:
            logger.error(f"Error describing image with LLM: {e}")
            return "H√¨nh ·∫£nh trong s√°ch gi√°o khoa (kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£)"

    async def _process_chapter(
        self,
        chapter_data: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        lesson_start: int,
        lesson_end: int,
        images_path: Path
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω m·ªôt ch∆∞∆°ng trong b√†i h·ªçc"""

        chapter_start = max(chapter_data.get('start_page', lesson_start), lesson_start)
        chapter_end = min(chapter_data.get('end_page', lesson_end), lesson_end)

        # L·∫•y content t·ª´ c√°c trang c·ªßa ch∆∞∆°ng
        chapter_content = []
        image_counter = 1

        for page_num in range(chapter_start - 1, chapter_end):  # -1 v√¨ index t·ª´ 0
            if page_num >= len(pages_data):
                break

            page = pages_data[page_num]

            # Th√™m text content
            if page['text'].strip():
                chapter_content.append({
                    "type": "text",
                    "data": page['text'].strip()
                })

            # X·ª≠ l√Ω images trong trang
            for img_data in page['images']:
                try:
                    # L∆∞u image
                    img_filename = f"img{image_counter}.png"
                    img_path = images_path / img_filename

                    # Decode v√† l∆∞u image
                    img_bytes = base64.b64decode(img_data['data'])
                    with open(img_path, 'wb') as f:
                        f.write(img_bytes)

                    # T·∫°o m√¥ t·∫£ image b·∫±ng LLM
                    img_description = await self._describe_image(img_bytes)

                    # L∆∞u description
                    desc_path = images_path / f"img{image_counter}_description.txt"
                    with open(desc_path, 'w', encoding='utf-8') as f:
                        f.write(img_description)

                    # Th√™m v√†o content (ch·ªâ m√¥ t·∫£, kh√¥ng l∆∞u base64)
                    chapter_content.append({
                        "type": "image",
                        "description": img_description,
                        "local_path": f"images/{images_path.name}/img{image_counter}.png"
                    })

                    image_counter += 1

                except Exception as e:
                    logger.error(f"Error processing image {image_counter}: {e}")

        return {
            "chapter_id": chapter_data['chapter_id'],
            "title": chapter_data['title'],
            "content": chapter_content
        }

    async def _describe_image(self, img_bytes: bytes) -> str:
        """T·∫°o m√¥ t·∫£ cho h√¨nh ·∫£nh b·∫±ng LLM"""

        if not llm_service.is_available():
            return "H√¨nh ·∫£nh trong s√°ch gi√°o khoa (LLM kh√¥ng kh·∫£ d·ª•ng ƒë·ªÉ m√¥ t·∫£ chi ti·∫øt)"

        try:
            # Convert image to base64 for LLM
            img_base64 = base64.b64encode(img_bytes).decode()

            prompt = """
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch h√¨nh ·∫£nh trong s√°ch gi√°o khoa. H√£y m√¥ t·∫£ h√¨nh ·∫£nh n√†y m·ªôt c√°ch chi ti·∫øt v√† c√≥ √≠ch cho vi·ªác t·∫°o gi√°o √°n.

Y√äU C·∫¶U M√î T·∫¢:
1. N·ªôi dung ch√≠nh c·ªßa h√¨nh ·∫£nh
2. C√°c y·∫øu t·ªë quan tr·ªçng (bi·ªÉu ƒë·ªì, c√¥ng th·ª©c, minh h·ªça)
3. M·ª•c ƒë√≠ch gi√°o d·ª•c c·ªßa h√¨nh ·∫£nh
4. C√°ch s·ª≠ d·ª•ng trong gi·∫£ng d·∫°y

Tr·∫£ v·ªÅ m√¥ t·∫£ ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát:
"""

            # Note: Gemini vision API call would go here
            # For now, return a placeholder
            return "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa - c·∫ßn c·∫≠p nh·∫≠t m√¥ t·∫£ chi ti·∫øt"

        except Exception as e:
            logger.error(f"Error describing image: {e}")
            return "H√¨nh ·∫£nh trong s√°ch gi√°o khoa (kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£)"

    async def get_book_structure(self, book_id: str) -> Optional[Dict[str, Any]]:
        """L·∫•y c·∫•u tr√∫c c·ªßa m·ªôt cu·ªën s√°ch ƒë√£ x·ª≠ l√Ω"""

        book_path = self.output_base_path / book_id
        metadata_file = book_path / "metadata.json"

        if not metadata_file.exists():
            return None

        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            # L·∫•y danh s√°ch lessons
            lessons_path = book_path / "lessons"
            lessons = []

            if lessons_path.exists():
                for lesson_file in sorted(lessons_path.glob("*.json")):
                    with open(lesson_file, 'r', encoding='utf-8') as f:
                        lesson_data = json.load(f)
                        lessons.append({
                            "lesson_id": lesson_data['lesson_id'],
                            "title": lesson_data['title'],
                            "chapters_count": len(lesson_data.get('chapters', []))
                        })

            return {
                "metadata": metadata,
                "lessons": lessons,
                "book_path": str(book_path)
            }

        except Exception as e:
            logger.error(f"Error reading book structure: {e}")
            return None

    async def get_lesson_content(self, book_id: str, lesson_id: str) -> Optional[Dict[str, Any]]:
        """L·∫•y n·ªôi dung chi ti·∫øt c·ªßa m·ªôt b√†i h·ªçc"""

        book_path = self.output_base_path / book_id
        lesson_file = book_path / "lessons" / f"{lesson_id}.json"

        if not lesson_file.exists():
            return None

        try:
            with open(lesson_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error reading lesson content: {e}")
            return None

# Global instance
textbook_parser_service = TextbookParserService()
