"""
Service ƒë·ªÉ import ƒë·ªÅ thi t·ª´ file DOCX v√† chuy·ªÉn ƒë·ªïi th√†nh JSON
"""

import logging
import json
import re
import time
import uuid
from typing import Dict, Any, Optional, List
from docx import Document
import io
from datetime import datetime

from app.services.openrouter_service import get_openrouter_service
from app.models.exam_import_models import (
    ExamImportRequest,
    ExamImportResponse,
    ExamImportError,
    ImportedExamData,
    ExamImportStatistics
)
from app.constants.difficulty_levels import DifficultyLevel

logger = logging.getLogger(__name__)


class ExamImportService:
    """Service ƒë·ªÉ import v√† x·ª≠ l√Ω ƒë·ªÅ thi t·ª´ file DOCX"""

    def __init__(self):
        self.model_name = "google/gemini-2.0-flash-001"
        logger.info("üîÑ ExamImportService: First-time initialization triggered")

    async def import_exam_from_docx_content(
        self, file_content: bytes, filename: str = "exam.docx", staff_import: bool = False
    ) -> Dict[str, Any]:
        """
        Import ƒë·ªÅ thi t·ª´ n·ªôi dung file DOCX

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes
            filename: T√™n file g·ªëc
            staff_import: True n·∫øu import cho staff (format SpringBoot), False cho frontend

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ import
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting DOCX import for file: {filename}")
            
            # 1. Extract text t·ª´ DOCX
            extracted_text = self._extract_text_from_docx_bytes(file_content)
            
            if not extracted_text or len(extracted_text.strip()) < 100:
                return {
                    "statusCode": 400,
                    "message": "File extraction failed",
                    "error": "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ file DOCX ho·∫∑c n·ªôi dung qu√° ng·∫Øn",
                    "details": {"filename": filename, "extracted_length": len(extracted_text)}
                }

            logger.info(f"Extracted {len(extracted_text)} characters from DOCX")

            # 2. Validate format ƒë·ªÅ thi tr∆∞·ªõc khi g·ªçi LLM
            logger.info("Validating exam format...")
            format_validation = self._validate_exam_format(extracted_text)

            if not format_validation["is_valid"]:
                return {
                    "statusCode": 400,
                    "message": "Invalid exam format",
                    "error": f"ƒê·ªÅ thi kh√¥ng ƒë√∫ng format chu·∫©n: {format_validation['error']}",
                    "details": {
                        "filename": filename,
                        "validation_details": format_validation["details"]
                    }
                }

            # L∆∞u th√¥ng tin warnings ƒë·ªÉ tr·∫£ v·ªÅ sau
            format_warnings = format_validation.get("warnings", [])
            missing_parts = format_validation.get("details", {}).get("missing_parts", [])

            logger.info(f"Exam format validation passed with warnings: {format_warnings}")

            # 3. G·ª≠i cho LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi
            logger.info("Sending content to LLM for analysis...")
            llm_result = await self._analyze_exam_with_llm(extracted_text, filename)

            if not llm_result.get("success", False):
                return {
                    "statusCode": 500,
                    "message": "LLM analysis failed",
                    "error": f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi: {llm_result.get('error', 'Unknown error')}",
                    "details": {"filename": filename}
                }

            # 3. Parse JSON response t·ª´ LLM
            exam_data = llm_result.get("data")
            if not exam_data:
                return {
                    "statusCode": 500,
                    "message": "No exam data returned",
                    "error": "LLM kh√¥ng tr·∫£ v·ªÅ d·ªØ li·ªáu ƒë·ªÅ thi",
                    "details": {"filename": filename}
                }

            # 4. Validate v√† clean d·ªØ li·ªáu t·ª´ LLM
            logger.info("Validating and cleaning LLM data...")
            logger.info(f"Exam data type: {type(exam_data)}")
            logger.info(f"Exam data preview: {str(exam_data)[:500]}...")

            validation_result = self._validate_and_clean_exam_data(exam_data)

            if not validation_result["is_valid"]:
                return {
                    "statusCode": 422,
                    "message": "Invalid exam data from LLM",
                    "error": f"D·ªØ li·ªáu t·ª´ LLM kh√¥ng h·ª£p l·ªá: {validation_result['error']}",
                    "details": {
                        "filename": filename,
                        "validation_details": validation_result["details"],
                        "exam_data_type": str(type(exam_data)),
                        "exam_data_preview": str(exam_data)[:200]
                    }
                }

            # S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean
            exam_data = validation_result["cleaned_data"]

            # 5. Chuy·ªÉn ƒë·ªïi sang format ph√π h·ª£p
            if staff_import:
                # Format cho SpringBoot staff
                formatted_data = self._convert_to_staff_format(exam_data)
                success_message = "Question bank data imported successfully"
            else:
                # Format cho Frontend
                formatted_data = self._convert_to_fe_format(exam_data)
                success_message = "Template updated successfully"

            # 6. Validate v√† t·∫°o response
            processing_time = time.time() - start_time

            # T·∫°o message v·ªõi th√¥ng tin v·ªÅ c√°c ph·∫ßn thi·∫øu
            if format_warnings:
                success_message += f" (L∆∞u √Ω: {'; '.join(format_warnings)})"

            # T·∫°o response theo format ph√π h·ª£p
            response_data = {
                "statusCode": 200,
                "message": success_message,
                "data": formatted_data
            }

            return response_data

        except Exception as e:
            logger.error(f"Error importing exam from DOCX: {e}")
            processing_time = time.time() - start_time
            
            return {
                "statusCode": 500,
                "message": "Import failed",
                "error": f"L·ªói trong qu√° tr√¨nh import: {str(e)}",
                "details": {
                    "filename": filename,
                    "processing_time": processing_time,
                    "error_type": type(e).__name__
                }
            }

    def _extract_text_from_docx_bytes(self, file_content: bytes) -> str:
        """
        Tr√≠ch xu·∫•t text t·ª´ file DOCX bytes

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes

        Returns:
            Text ƒë√£ tr√≠ch xu·∫•t
        """
        try:
            # T·∫°o file-like object t·ª´ bytes
            file_stream = io.BytesIO(file_content)
            
            # ƒê·ªçc document
            doc = Document(file_stream)
            
            # Tr√≠ch xu·∫•t text t·ª´ t·∫•t c·∫£ paragraphs
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            # Tr√≠ch xu·∫•t text t·ª´ tables
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        full_text.append(" | ".join(row_text))
            
            extracted_text = "\n".join(full_text)
            logger.info(f"Successfully extracted {len(extracted_text)} characters from DOCX")
            
            return extracted_text

        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    def _validate_exam_format(self, exam_text: str) -> Dict[str, Any]:
        """
        Validate format ƒë·ªÅ thi c∆° b·∫£n - ki·ªÉm tra c√≥ √≠t nh·∫•t 1 ph·∫ßn v√† ƒë√°p √°n

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation
        """
        try:
            # Normalize text ƒë·ªÉ d·ªÖ ki·ªÉm tra
            normalized_text = exam_text.upper().replace('\n', ' ').replace('\r', '')

            validation_result = {
                "is_valid": True,
                "error": "",
                "details": {},
                "warnings": []
            }

            # 1. Ki·ªÉm tra c·∫•u tr√∫c c√°c ph·∫ßn
            all_parts = ["PH·∫¶N I", "PH·∫¶N II", "PH·∫¶N III"]
            found_parts = []
            missing_parts = []

            for part in all_parts:
                if part in normalized_text:
                    found_parts.append(part)
                else:
                    missing_parts.append(part)

            # Ch·ªâ y√™u c·∫ßu c√≥ √≠t nh·∫•t 1 ph·∫ßn
            if not found_parts:
                validation_result["is_valid"] = False
                validation_result["error"] = "Kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c ph·∫ßn n√†o (PH·∫¶N I, II, III)"
                validation_result["details"]["missing_parts"] = missing_parts
                return validation_result

            # Ghi nh·∫≠n c√°c ph·∫ßn thi·∫øu nh∆∞ warning
            if missing_parts:
                validation_result["warnings"].append(f"Thi·∫øu c√°c ph·∫ßn: {', '.join(missing_parts)}")
                validation_result["details"]["missing_parts"] = missing_parts
                validation_result["details"]["found_parts"] = found_parts

            # 2. B·ªè qua ki·ªÉm tra ph·∫ßn ƒë√°p √°n (kh√¥ng b·∫Øt bu·ªôc)
            # if "ƒê√ÅP √ÅN" not in normalized_text:
            #     validation_result["warnings"].append("Kh√¥ng t√¨m th·∫•y ph·∫ßn ƒë√°p √°n")
            #     validation_result["details"]["missing_answer_section"] = True

            logger.info(f"Exam format validation passed - Found parts: {found_parts}, Missing: {missing_parts}")
            return validation_result

        except Exception as e:
            logger.error(f"Error in format validation: {e}")
            return {
                "is_valid": False,
                "error": f"L·ªói trong qu√° tr√¨nh validate format: {str(e)}",
                "details": {"exception": str(e)},
                "warnings": []
            }

    async def _analyze_exam_with_llm(self, exam_text: str, filename: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi ƒë·ªÅ thi th√†nh JSON

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract
            filename: T√™n file g·ªëc

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        try:
            # T·∫°o prompt cho LLM
            prompt = self._create_analysis_prompt(exam_text, filename)
            
            # G·ªçi LLM
            openrouter_service = get_openrouter_service()
            response = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=8000
            )

            if not response.get("success", False):
                return {
                    "success": False,
                    "error": f"LLM request failed: {response.get('error', 'Unknown error')}"
                }

            # Parse JSON t·ª´ response
            llm_content = response.get("text", "") or response.get("content", "")
            logger.info(f"LLM response length: {len(llm_content)}")
            logger.info(f"LLM response preview: {llm_content[:500]}...")

            # T√¨m JSON trong response
            json_data = self._extract_json_from_response(llm_content)
            
            if not json_data:
                return {
                    "success": False,
                    "error": "Kh√¥ng th·ªÉ t√¨m th·∫•y JSON h·ª£p l·ªá trong response c·ªßa LLM"
                }

            return {
                "success": True,
                "data": json_data
            }

        except Exception as e:
            logger.error(f"Error in LLM analysis: {e}")
            return {
                "success": False,
                "error": f"LLM analysis error: {str(e)}"
            }

    def _create_analysis_prompt(self, exam_text: str, filename: str) -> str:
        """
        T·∫°o prompt cho LLM ƒë·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi
            filename: T√™n file

        Returns:
            Prompt string
        """
        prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch ƒë·ªÅ thi. H√£y ph√¢n t√≠ch n·ªôi dung ƒë·ªÅ thi sau v√† chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng JSON ch√≠nh x√°c.

N·ªòI DUNG ƒê·ªÄ THI:
{exam_text}

Y√äU C·∫¶U:
1. Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t th√¥ng tin ƒë·ªÅ thi th√†nh JSON v·ªõi c·∫•u tr√∫c ch√≠nh x√°c nh∆∞ m·∫´u
2. X√°c ƒë·ªãnh m√¥n h·ªçc, l·ªõp, th·ªùi gian l√†m b√†i, t√™n tr∆∞·ªùng:
   - subject: PH·∫¢I x√°c ƒë·ªãnh m√¥n h·ªçc (VD: "H√≥a h·ªçc", "To√°n h·ªçc", "V·∫≠t l√Ω"), KH√îNG ƒë∆∞·ª£c ƒë·ªÉ null
   - grade: PH·∫¢I x√°c ƒë·ªãnh l·ªõp (s·ªë t·ª´ 1-12), n·∫øu kh√¥ng r√µ th√¨ m·∫∑c ƒë·ªãnh l√† 12, KH√îNG ƒë∆∞·ª£c ƒë·ªÉ null
   - duration_minutes: PH·∫¢I x√°c ƒë·ªãnh th·ªùi gian (s·ªë ph√∫t), n·∫øu kh√¥ng r√µ th√¨ m·∫∑c ƒë·ªãnh 90, KH√îNG ƒë∆∞·ª£c ƒë·ªÉ null
   - school: T√¨m v√† tr√≠ch xu·∫•t t√™n tr∆∞·ªùng t·ª´ ph·∫ßn ƒë·∫ßu ƒë·ªÅ thi (th∆∞·ªùng n·∫±m d∆∞·ªõi "B·ªò GI√ÅO D·ª§C V√Ä ƒê√ÄO T·∫†O")
   - V√≠ d·ª•: "TR∆Ø·ªúNG THPT HONG THINH" ‚Üí "TR∆Ø·ªúNG THPT HONG THINH"
3. Ph√¢n chia c√¢u h·ªèi theo c√°c ph·∫ßn c√≥ s·∫µn trong ƒë·ªÅ thi:
   - Ph·∫ßn I: Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn (A, B, C, D) - n·∫øu c√≥
   - Ph·∫ßn II: Tr·∫Øc nghi·ªám ƒë√∫ng/sai (a, b, c, d v·ªõi true/false) - n·∫øu c√≥
   - Ph·∫ßn III: Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn (ch·ªâ s·ªë) - n·∫øu c√≥
4. Ch·ªâ x·ª≠ l√Ω c√°c ph·∫ßn th·ª±c s·ª± c√≥ trong ƒë·ªÅ thi, b·ªè qua ph·∫ßn kh√¥ng c√≥
5. Tr√≠ch xu·∫•t ƒë√°p √°n ch√≠nh x√°c t·ª´ ph·∫ßn ƒë√°p √°n (n·∫øu c√≥)
6. N·∫øu l√† m√¥n H√≥a h·ªçc, tr√≠ch xu·∫•t b·∫£ng nguy√™n t·ª≠ kh·ªëi (n·∫øu c√≥)

ƒê·ªäNH D·∫†NG JSON MONG MU·ªêN:
{{
  "subject": "H√≥a h·ªçc",
  "grade": 12,
  "duration_minutes": 90,
  "school": "TR∆Ø·ªúNG THPT HONG THINH",
  "exam_code": "1234",
  "atomic_masses": "H = 1; C = 12; N = 14; O = 16",
  "parts": [
    {{
      "part": "Ph·∫ßn I",
      "title": "Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn",
      "description": "M·ªói c√¢u tr·∫£ l·ªùi ƒë√∫ng th√≠ sinh ƒë∆∞·ª£c 0,25 ƒëi·ªÉm",
      "questions": [
        {{
          "id": 1,
          "question": "Nguy√™n t·ª≠ carbon c√≥ bao nhi√™u electron?",
          "options": {{
            "A": "4",
            "B": "6",
            "C": "8",
            "D": "12"
          }},
          "answer": "B"
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn II",
      "title": "Tr·∫Øc nghi·ªám ƒë√∫ng/sai",
      "description": "Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u X. M·ªói c√¢u c√≥ 4 ph√°t bi·ªÉu a), b), c), d)",
      "questions": [
        {{
          "id": 1,
          "question": "Cho c√°c ph√°t bi·ªÉu v·ªÅ nguy√™n t·ª≠ carbon:",
          "statements": {{
            "a": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 6 proton",
              "answer": true
            }},
            "b": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 8 neutron",
              "answer": false
            }},
            "c": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 6 electron",
              "answer": true
            }},
            "d": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ kh·ªëi l∆∞·ª£ng 14u",
              "answer": false
            }}
          }}
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn III",
      "title": "Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn",
      "description": "Ch·ªâ ghi s·ªë, kh√¥ng ghi ƒë∆°n v·ªã. T·ªëi ƒëa 4 k√Ω t·ª±.",
      "questions": [
        {{
          "id": 1,
          "question": "S·ªë electron trong nguy√™n t·ª≠ carbon l√† bao nhi√™u?",
          "answer": "6"
        }}
      ]
    }}
  ]
}}

L∆∞U √ù QUAN TR·ªåNG V·ªÄ C·∫§U TR√öC:
- Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng th√™m text gi·∫£i th√≠ch
- ƒê·∫£m b·∫£o t·∫•t c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n ƒë∆∞·ª£c tr√≠ch xu·∫•t ch√≠nh x√°c
- QUAN TR·ªåNG: KH√îNG ƒê∆Ø·ª¢C TR·∫¢ V·ªÄ NULL CHO C√ÅC TR∆Ø·ªúNG B·∫ÆT BU·ªòC:
  * subject: PH·∫¢I l√† string kh√¥ng r·ªóng (VD: "H√≥a h·ªçc")
  * grade: PH·∫¢I l√† s·ªë nguy√™n t·ª´ 1-12 (VD: 12)
  * duration_minutes: PH·∫¢I l√† s·ªë nguy√™n d∆∞∆°ng (VD: 90)
  * N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh thay v√¨ null
- QUAN TR·ªåNG: ID c√¢u h·ªèi trong m·ªói ph·∫ßn b·∫Øt ƒë·∫ßu t·ª´ 1
  * V√≠ d·ª•: Ph·∫ßn I c√≥ c√¢u 1-6, Ph·∫ßn II c√≥ c√¢u 1-6, Ph·∫ßn III c√≥ c√¢u 1-6
- QUAN TR·ªåNG: M·ªói lo·∫°i c√¢u h·ªèi c√≥ c·∫•u tr√∫c kh√°c nhau:

  * PH·∫¶N I (Tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn): PH·∫¢I c√≥ "options" v√† "answer"
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "options": {{"A": "...", "B": "...", "C": "...", "D": "..."}},
      "answer": "A"
    }}

  * PH·∫¶N II (ƒê√∫ng/sai): PH·∫¢I c√≥ "statements", KH√îNG c√≥ "answer" field
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "statements": {{
        "a": {{"text": "...", "answer": true}},
        "b": {{"text": "...", "answer": false}},
        "c": {{"text": "...", "answer": true}},
        "d": {{"text": "...", "answer": false}}
      }}
    }}

  * PH·∫¶N III (Tr·∫£ l·ªùi ng·∫Øn): PH·∫¢I c√≥ "answer" string, KH√îNG c√≥ "options" hay "statements"
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "answer": "6"
    }}

- QUAN TR·ªåNG: Ch·ªâ t·∫°o parts cho nh·ªØng ph·∫ßn c√≥ N·ªòI DUNG C√ÇU H·ªéI th·ª±c t·∫ø trong ƒë·ªÅ thi
- N·∫øu ph·∫ßn ƒë√°p √°n c√≥ nh∆∞ng kh√¥ng c√≥ n·ªôi dung c√¢u h·ªèi, KH√îNG t·∫°o part cho ph·∫ßn ƒë√≥
- M·∫£ng "parts" c√≥ th·ªÉ ch·ª©a 1, 2 ho·∫∑c 3 ph·∫ßn t√πy theo n·ªôi dung ƒë·ªÅ thi th·ª±c t·∫ø
- Kh√¥ng t·∫°o ra c√¢u h·ªèi gi·∫£ cho c√°c ph·∫ßn kh√¥ng c√≥ n·ªôi dung
- ƒê·∫£m b·∫£o field "question" lu√¥n l√† string kh√¥ng r·ªóng, kh√¥ng ƒë∆∞·ª£c null
- V√≠ d·ª•: N·∫øu ƒë·ªÅ thi ch·ªâ c√≥ "PH·∫¶N I" v·ªõi n·ªôi dung c√¢u h·ªèi, ch·ªâ t·∫°o 1 part cho Ph·∫ßn I, b·ªè qua Ph·∫ßn II v√† III d√π c√≥ trong ƒë√°p √°n
- QUAN TR·ªåNG: Gi·ªØ nguy√™n ƒë√°p √°n t·ª´ DOCX, KH√îNG ƒë∆∞·ª£c l√†m tr√≤n, format hay thay ƒë·ªïi g√¨
  * V√≠ d·ª•: N·∫øu ƒë√°p √°n l√† "1,66" th√¨ gi·ªØ nguy√™n "1,66", kh√¥ng l√†m tr√≤n th√†nh "2"
  * N·∫øu ƒë√°p √°n l√† "-1" th√¨ gi·ªØ nguy√™n "-1"
  * N·∫øu ƒë√°p √°n l√† "27" th√¨ gi·ªØ nguy√™n "27"
- QUAN TR·ªåNG: Tr√≠ch xu·∫•t ƒë√∫ng t√™n tr∆∞·ªùng t·ª´ ph·∫ßn ƒë·∫ßu ƒë·ªÅ thi
  * T√¨m d√≤ng ch·ª©a t√™n tr∆∞·ªùng (th∆∞·ªùng n·∫±m d∆∞·ªõi "B·ªò GI√ÅO D·ª§C V√Ä ƒê√ÄO T·∫†O")
  * V√≠ d·ª•: "TR∆Ø·ªúNG THPT ABC" ‚Üí school: "TR∆Ø·ªúNG THPT ABC"
  * N·∫øu kh√¥ng t√¨m th·∫•y, ƒë·ªÉ school: null

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON:
"""
        return prompt

    def _extract_json_from_response(self, response_content: str) -> Optional[Dict[str, Any]]:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ response c·ªßa LLM

        Args:
            response_content: N·ªôi dung response t·ª´ LLM

        Returns:
            Dict ch·ª©a JSON data ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            logger.info(f"Extracting JSON from response: {response_content[:200]}...")

            # 1. T√¨m JSON trong markdown code blocks
            markdown_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            markdown_matches = re.findall(markdown_pattern, response_content, re.DOTALL)

            for match in markdown_matches:
                try:
                    json_data = json.loads(match.strip())
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted JSON from markdown code block")
                        return json_data
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON from markdown: {e}")
                    continue

            # 2. T√¨m JSON trong response (c√≥ th·ªÉ c√≥ text bao quanh)
            json_pattern = r'\{.*\}'
            matches = re.findall(json_pattern, response_content, re.DOTALL)

            for match in matches:
                try:
                    # Th·ª≠ parse JSON
                    json_data = json.loads(match)

                    # Validate c·∫•u tr√∫c c∆° b·∫£n
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted and validated JSON from LLM response")
                        return json_data

                except json.JSONDecodeError:
                    continue

            # 3. N·∫øu kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá, th·ª≠ parse to√†n b·ªô response
            try:
                json_data = json.loads(response_content.strip())
                if self._validate_exam_json_structure(json_data):
                    logger.info("Successfully parsed entire response as JSON")
                    return json_data
            except json.JSONDecodeError:
                pass

            logger.error("Could not extract valid JSON from LLM response")
            logger.error(f"Response content: {response_content}")
            return None

        except Exception as e:
            logger.error(f"Error extracting JSON from response: {e}")
            return None

    def _validate_exam_json_structure(self, json_data: Dict[str, Any]) -> bool:
        """
        Validate c·∫•u tr√∫c JSON c·ªßa ƒë·ªÅ thi - linh ho·∫°t v·ªõi parts

        Args:
            json_data: JSON data c·∫ßn validate

        Returns:
            True n·∫øu c·∫•u tr√∫c h·ª£p l·ªá
        """
        try:
            required_fields = ["subject", "grade", "duration_minutes", "school", "parts"]

            # Ki·ªÉm tra c√°c field b·∫Øt bu·ªôc
            for field in required_fields:
                if field not in json_data:
                    logger.error(f"Missing required field: {field}")
                    return False

            # Ki·ªÉm tra parts - cho ph√©p empty list
            parts = json_data.get("parts", [])
            if not isinstance(parts, list):
                logger.error("Parts must be a list")
                return False

            # N·∫øu c√≥ parts, ki·ªÉm tra c·∫•u tr√∫c c·ªßa t·ª´ng part
            for i, part in enumerate(parts):
                if not isinstance(part, dict):
                    logger.error(f"Part {i} must be a dictionary")
                    return False

                part_required = ["part", "title", "questions"]
                for field in part_required:
                    if field not in part:
                        logger.error(f"Missing required field '{field}' in part {i}")
                        return False

                # Ki·ªÉm tra questions - cho ph√©p empty list
                questions = part.get("questions", [])
                if not isinstance(questions, list):
                    logger.error(f"Questions in part {i} must be a list")
                    return False

            logger.info(f"JSON structure validation passed - {len(parts)} parts found")
            return True

        except Exception as e:
            logger.error(f"Error validating JSON structure: {e}")
            return False

    def _validate_and_clean_exam_data(self, exam_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate v√† clean d·ªØ li·ªáu ƒë·ªÅ thi t·ª´ LLM

        Args:
            exam_data: D·ªØ li·ªáu th√¥ t·ª´ LLM

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            logger.info("Starting exam data validation and cleaning...")

            # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu tr∆∞·ªõc
            if not isinstance(exam_data, dict):
                return {
                    "is_valid": False,
                    "error": f"Exam data must be a dictionary, got {type(exam_data)}",
                    "details": {"data_type": str(type(exam_data))},
                    "cleaned_data": {}
                }

            logger.info(f"Raw exam data from LLM: {json.dumps(exam_data, ensure_ascii=False, indent=2)}")

            result = {
                "is_valid": True,
                "error": "",
                "details": {},
                "cleaned_data": {}
            }

            # 1. Validate basic fields
            required_fields = ["subject", "grade", "duration_minutes", "school", "parts"]
            for field in required_fields:
                if field not in exam_data:
                    result["is_valid"] = False
                    result["error"] = f"Missing required field: {field}"
                    return result

            # 2. Clean basic data
            # X·ª≠ l√Ω grade - ƒë·∫£m b·∫£o kh√¥ng b·ªã None
            grade_value = exam_data.get("grade")
            if grade_value is None or grade_value == "":
                grade_value = 12  # Default grade
            try:
                grade_value = int(grade_value)
                if grade_value < 1 or grade_value > 12:
                    grade_value = 12
            except (ValueError, TypeError):
                grade_value = 12

            # X·ª≠ l√Ω duration_minutes - ƒë·∫£m b·∫£o kh√¥ng b·ªã None
            duration_value = exam_data.get("duration_minutes")
            if duration_value is None or duration_value == "":
                duration_value = 90  # Default duration
            try:
                duration_value = int(duration_value)
                if duration_value <= 0:
                    duration_value = 90
            except (ValueError, TypeError):
                duration_value = 90

            cleaned_data = {
                "subject": str(exam_data.get("subject") or "").strip(),
                "grade": grade_value,
                "duration_minutes": duration_value,
                "school": str(exam_data.get("school") or "").strip(),
                "exam_code": str(exam_data.get("exam_code") or "").strip() if exam_data.get("exam_code") else None,
                "atomic_masses": str(exam_data.get("atomic_masses") or "").strip() if exam_data.get("atomic_masses") else None,
                "parts": []
            }

            # 3. Validate v√† clean parts
            parts = exam_data.get("parts", [])
            if not isinstance(parts, list):
                result["is_valid"] = False
                result["error"] = f"Parts must be a list, got {type(parts)}"
                result["details"]["parts_type"] = str(type(parts))
                return result

            cleaned_parts = []
            skipped_parts = []

            for i, part in enumerate(parts):
                try:
                    # Ki·ªÉm tra part c√≥ ph·∫£i dict kh√¥ng
                    if not isinstance(part, dict):
                        logger.warning(f"Part {i} is not a dictionary, got {type(part)}: {part}")
                        skipped_parts.append({
                            "part_name": f"Part {i}",
                            "error": f"Part must be a dictionary, got {type(part)}"
                        })
                        continue

                    cleaned_part = self._clean_exam_part(part, i)
                    if cleaned_part["is_valid"]:
                        cleaned_parts.append(cleaned_part["data"])
                    else:
                        # Log warning v√† skip part n√†y thay v√¨ fail to√†n b·ªô
                        part_name = part.get("part", f"Part {i}")
                        logger.warning(f"Skipping invalid part '{part_name}': {cleaned_part['error']}")
                        skipped_parts.append({
                            "part_name": part_name,
                            "error": cleaned_part["error"]
                        })
                except Exception as e:
                    logger.error(f"Error processing part {i}: {e}")
                    skipped_parts.append({
                        "part_name": f"Part {i}",
                        "error": f"Processing error: {str(e)}"
                    })

            # Ch·ªâ fail n·∫øu kh√¥ng c√≥ part n√†o h·ª£p l·ªá
            if not cleaned_parts:
                result["is_valid"] = False
                result["error"] = "No valid parts found in exam data"
                result["details"]["skipped_parts"] = skipped_parts
                return result

            cleaned_data["parts"] = cleaned_parts
            result["cleaned_data"] = cleaned_data

            # Th√™m th√¥ng tin v·ªÅ parts b·ªã skip
            if skipped_parts:
                result["details"]["skipped_parts"] = skipped_parts
                logger.info(f"Exam data validation completed - {len(cleaned_parts)} valid parts, {len(skipped_parts)} skipped parts")
            else:
                logger.info(f"Exam data validation completed - {len(cleaned_parts)} parts validated")

            return result

        except Exception as e:
            logger.error(f"Error in exam data validation: {e}")
            return {
                "is_valid": False,
                "error": f"Validation error: {str(e)}",
                "details": {"exception": str(e)},
                "cleaned_data": {}
            }

    def _clean_exam_part(self, part_data: Dict[str, Any], part_index: int) -> Dict[str, Any]:
        """
        Clean v√† validate m·ªôt ph·∫ßn c·ªßa ƒë·ªÅ thi

        Args:
            part_data: D·ªØ li·ªáu ph·∫ßn th√¥ t·ª´ LLM
            part_index: Index c·ªßa ph·∫ßn

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            result = {
                "is_valid": True,
                "error": "",
                "data": {}
            }

            # Validate basic part fields
            required_part_fields = ["part", "title", "questions"]
            for field in required_part_fields:
                if field not in part_data:
                    result["is_valid"] = False
                    result["error"] = f"Missing field '{field}' in part"
                    return result

            # Clean part basic data
            cleaned_part = {
                "part": str(part_data.get("part", "")).strip(),
                "title": str(part_data.get("title", "")).strip(),
                "description": str(part_data.get("description", "")).strip() if part_data.get("description") else "",
                "questions": []
            }

            # Validate v√† clean questions
            questions = part_data.get("questions", [])
            if not isinstance(questions, list):
                result["is_valid"] = False
                result["error"] = f"Questions must be a list, got {type(questions)}"
                return result

            cleaned_questions = []
            invalid_questions = []

            for j, question in enumerate(questions):
                try:
                    # Ki·ªÉm tra question c√≥ ph·∫£i dict kh√¥ng
                    if not isinstance(question, dict):
                        logger.warning(f"Question {j} in {cleaned_part['part']} is not a dictionary, got {type(question)}: {question}")
                        invalid_questions.append({
                            "question_index": j,
                            "error": f"Question must be a dictionary, got {type(question)}"
                        })
                        continue

                    cleaned_question = self._clean_question(question, cleaned_part["part"], j)
                    if cleaned_question["is_valid"]:
                        cleaned_questions.append(cleaned_question["data"])
                    else:
                        # Log invalid question nh∆∞ng kh√¥ng fail to√†n b·ªô part
                        logger.warning(f"Skipping invalid question {j} in {cleaned_part['part']}: {cleaned_question['error']}")
                        invalid_questions.append({
                            "question_index": j,
                            "error": cleaned_question["error"]
                        })
                except Exception as e:
                    logger.error(f"Error processing question {j} in {cleaned_part['part']}: {e}")
                    invalid_questions.append({
                        "question_index": j,
                        "error": f"Processing error: {str(e)}"
                    })

            # N·∫øu kh√¥ng c√≥ c√¢u h·ªèi h·ª£p l·ªá n√†o, fail part n√†y
            if not cleaned_questions:
                result["is_valid"] = False
                result["error"] = f"No valid questions found in part. Invalid questions: {len(invalid_questions)}"
                return result

            cleaned_part["questions"] = cleaned_questions
            result["data"] = cleaned_part

            return result

        except Exception as e:
            return {
                "is_valid": False,
                "error": f"Error cleaning part: {str(e)}",
                "data": {}
            }

    def _clean_question(self, question_data: Dict[str, Any], part_name: str, question_index: int) -> Dict[str, Any]:
        """
        Clean v√† validate m·ªôt c√¢u h·ªèi theo lo·∫°i ph·∫ßn

        Args:
            question_data: D·ªØ li·ªáu c√¢u h·ªèi th√¥ t·ª´ LLM
            part_name: T√™n ph·∫ßn (ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi)
            question_index: Index c·ªßa c√¢u h·ªèi

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            logger.info(f"Cleaning question {question_index} in {part_name}")
            logger.info(f"Question data: {json.dumps(question_data, ensure_ascii=False, indent=2)}")

            result = {
                "is_valid": True,
                "error": "",
                "data": {}
            }

            # Validate basic question fields
            if "id" not in question_data or "question" not in question_data:
                result["is_valid"] = False
                result["error"] = "Missing 'id' or 'question' field"
                return result

            # Clean basic question data
            question_text = question_data.get("question")
            if not question_text or question_text is None or str(question_text).strip() == "":
                result["is_valid"] = False
                result["error"] = f"Question text cannot be null or empty. Got: {question_text}"
                return result

            cleaned_question = {
                "id": int(question_data.get("id", question_index + 1)),
                "question": str(question_text)
            }

            # Clean theo lo·∫°i ph·∫ßn
            logger.info(f"Determining question type for part: '{part_name}'")

            # X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi d·ª±a tr√™n t√™n ph·∫ßn
            part_name_upper = part_name.upper().strip()
            logger.info(f"Part name after processing: '{part_name_upper}'")

            # S·ª≠ d·ª•ng logic ƒë∆°n gi·∫£n ƒë·ªÉ ph√¢n lo·∫°i ch√≠nh x√°c
            if part_name_upper == "PH·∫¶N I":
                logger.info("Processing as MultipleChoice question (PH·∫¶N I)")
                # MultipleChoiceQuestion
                if "options" not in question_data or "answer" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "MultipleChoice question missing 'options' or 'answer'"
                    return result

                options = question_data.get("options", {})
                if not isinstance(options, dict):
                    result["is_valid"] = False
                    result["error"] = f"Options must be a dictionary, got {type(options)}"
                    return result

                # Ki·ªÉm tra c√≥ ƒë·ªß options A, B, C, D kh√¥ng
                required_options = ["A", "B", "C", "D"]
                missing_options = [opt for opt in required_options if opt not in options]
                if missing_options:
                    result["is_valid"] = False
                    result["error"] = f"Options missing: {missing_options}"
                    return result

                cleaned_question["options"] = {
                    "A": str(options.get("A", "")),
                    "B": str(options.get("B", "")),
                    "C": str(options.get("C", "")),
                    "D": str(options.get("D", ""))
                }
                cleaned_question["answer"] = str(question_data.get("answer", ""))

            elif part_name_upper == "PH·∫¶N II":
                logger.info("Processing as TrueFalse question (PH·∫¶N II)")
                # TrueFalseQuestion
                if "statements" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "TrueFalse question missing 'statements'"
                    return result

                statements = question_data.get("statements", {})
                if not isinstance(statements, dict):
                    result["is_valid"] = False
                    result["error"] = f"Statements must be a dictionary, got {type(statements)}"
                    return result

                # Ki·ªÉm tra c√≥ ƒë·ªß statements a, b, c, d kh√¥ng
                required_statements = ["a", "b", "c", "d"]
                missing_statements = [stmt for stmt in required_statements if stmt not in statements]
                if missing_statements:
                    result["is_valid"] = False
                    result["error"] = f"Statements missing: {missing_statements}"
                    return result

                cleaned_statements = {}
                for key in ["a", "b", "c", "d"]:
                    stmt = statements.get(key, {})
                    if not isinstance(stmt, dict):
                        result["is_valid"] = False
                        result["error"] = f"Statement {key} must be a dictionary, got {type(stmt)}"
                        return result

                    if "text" not in stmt or "answer" not in stmt:
                        result["is_valid"] = False
                        result["error"] = f"Statement {key} missing 'text' or 'answer'"
                        return result

                    cleaned_statements[key] = {
                        "text": str(stmt.get("text", "")),
                        "answer": bool(stmt.get("answer", False))
                    }

                cleaned_question["statements"] = cleaned_statements

            elif part_name_upper == "PH·∫¶N III":
                logger.info("Processing as ShortAnswer question (PH·∫¶N III)")
                # ShortAnswerQuestion
                if "answer" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "ShortAnswer question missing 'answer'"
                    return result

                cleaned_question["answer"] = str(question_data.get("answer", ""))

            else:
                result["is_valid"] = False
                result["error"] = f"Unknown part type: {part_name}"
                return result

            # Th√™m explanation n·∫øu c√≥
            if "explanation" in question_data:
                cleaned_question["explanation"] = str(question_data.get("explanation", ""))

            result["data"] = cleaned_question
            return result

        except Exception as e:
            return {
                "is_valid": False,
                "error": f"Error cleaning question: {str(e)}",
                "data": {}
            }

    def _calculate_import_statistics(self, exam_data: Dict[str, Any]) -> ExamImportStatistics:
        """
        T√≠nh to√°n th·ªëng k√™ cho ƒë·ªÅ thi ƒë√£ import

        Args:
            exam_data: D·ªØ li·ªáu ƒë·ªÅ thi

        Returns:
            ExamImportStatistics object
        """
        try:
            parts = exam_data.get("parts", [])
            logger.info(f"Calculating statistics for {len(parts)} parts")

            total_questions = 0
            part_1_questions = 0
            part_2_questions = 0
            part_3_questions = 0
            
            for part in parts:
                questions = part.get("questions", [])
                part_name = part.get("part", "").upper().strip()

                # S·ª≠ d·ª•ng logic so s√°nh ch√≠nh x√°c nh∆∞ trong _clean_question
                if part_name == "PH·∫¶N I":
                    part_1_questions = len(questions)
                    logger.info(f"PH·∫¶N I: {len(questions)} questions")
                elif part_name == "PH·∫¶N II":
                    part_2_questions = len(questions)
                    logger.info(f"PH·∫¶N II: {len(questions)} questions")
                elif part_name == "PH·∫¶N III":
                    part_3_questions = len(questions)
                    logger.info(f"PH·∫¶N III: {len(questions)} questions")
                else:
                    logger.warning(f"Unknown part name for statistics: '{part_name}' with {len(questions)} questions")

                total_questions += len(questions)
            
            has_atomic_masses = bool(exam_data.get("atomic_masses"))
            
            # T√≠nh ch·∫•t l∆∞·ª£ng x·ª≠ l√Ω (d·ª±a tr√™n s·ªë c√¢u h·ªèi v√† c·∫•u tr√∫c)
            processing_quality = min(1.0, (total_questions / 20) * 0.8 + 0.2)
            
            return ExamImportStatistics(
                total_questions=total_questions,
                part_1_questions=part_1_questions,
                part_2_questions=part_2_questions,
                part_3_questions=part_3_questions,
                has_atomic_masses=has_atomic_masses,
                processing_quality=processing_quality
            )

        except Exception as e:
            logger.error(f"Error calculating import statistics: {e}")
            return ExamImportStatistics(
                total_questions=0,
                part_1_questions=0,
                part_2_questions=0,
                part_3_questions=0,
                has_atomic_masses=False,
                processing_quality=0.0
            )

    def _convert_to_fe_format(self, exam_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu exam sang format m√† FE mong mu·ªën

        Args:
            exam_data: D·ªØ li·ªáu exam ƒë√£ ƒë∆∞·ª£c clean

        Returns:
            Dict theo format FE
        """
        try:
            # T·∫°o UUID cho template

            # Chuy·ªÉn ƒë·ªïi parts sang format FE
            fe_parts = []
            grading_config = {}

            for part in exam_data.get("parts", []):
                part_name = part.get("part", "")
                part_title = part.get("title", "")
                questions = part.get("questions", [])

                # Chuy·ªÉn ƒë·ªïi questions v·ªõi UUID v√† questionNumber
                fe_questions = []
                for idx, question in enumerate(questions):
                    fe_question = {
                        "id": str(uuid.uuid4()),
                        "questionNumber": idx + 1,
                        "question": question.get("question", "")
                    }

                    # Th√™m fields t√πy theo lo·∫°i c√¢u h·ªèi
                    if "options" in question and "answer" in question:
                        # Multiple choice
                        fe_question["options"] = question["options"]
                        fe_question["answer"] = question["answer"]
                    elif "statements" in question:
                        # True/False
                        fe_question["statements"] = question["statements"]
                    elif "answer" in question and "options" not in question:
                        # Short answer
                        fe_question["answer"] = question["answer"]

                    # Th√™m difficultyLevel cho t·ª´ng c√¢u h·ªèi
                    fe_question["difficultyLevel"] = self._analyze_difficulty_level(question).value

                    fe_questions.append(fe_question)

                fe_part = {
                    "part": part_name,
                    "title": part_title,
                    "questions": fe_questions
                }

                fe_parts.append(fe_part)

                # T·∫°o grading config (m·∫∑c ƒë·ªãnh)
                if part_name == "PH·∫¶N I":
                    grading_config[part_name] = 0.25
                elif part_name == "PH·∫¶N II":
                    grading_config[part_name] = 1.0
                elif part_name == "PH·∫¶N III":
                    grading_config[part_name] = 0.25
                else:
                    grading_config[part_name] = 0.5

            # T√≠nh t·ªïng ƒëi·ªÉm
            total_score = 10.0

            # T·∫°o response theo format FE - s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean
            fe_data = {
                "name": f"Template {exam_data.get('subject', 'Ch∆∞a x√°c ƒë·ªãnh')}",
                "subject": exam_data.get("subject", "Ch∆∞a x√°c ƒë·ªãnh"),
                "grade": exam_data.get("grade", "Ch∆∞a x√°c ƒë·ªãnh"),  # S·ª≠ d·ª•ng gi√° tr·ªã ƒë√£ clean ho·∫∑c default
                "durationMinutes": exam_data.get("duration_minutes", 90),  # S·ª≠ d·ª•ng gi√° tr·ªã ƒë√£ clean ho·∫∑c default
                "parts": fe_parts,
                "totalScore": total_score,
                "version": 1,
                "createdAt": datetime.now().isoformat()
            }

            return fe_data

        except Exception as e:
            logger.error(f"Error converting to FE format: {e}")
            # Tr·∫£ v·ªÅ format c∆° b·∫£n n·∫øu c√≥ l·ªói
            return {
                "id": str(uuid.uuid4()),
                "name": "Template m·ªõi",
                "subject": "Ch∆∞a x√°c ƒë·ªãnh",
                "grade": 12,
                "durationMinutes": 90,
                "createdBy": str(uuid.uuid4()),
                "contentJson": {
                    "parts": []
                },
                "gradingConfig": {},
                "totalScore": 10.0,
                "version": 1,
                "createdAt": datetime.now().isoformat()
            }

    def _convert_to_staff_format(self, exam_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu exam sang format SpringBoot cho staff

        Args:
            exam_data: D·ªØ li·ªáu exam ƒë√£ ƒë∆∞·ª£c clean

        Returns:
            List[Dict] theo format SpringBoot QuestionBank v·ªõi lessonId = null
        """
        try:
            staff_questions = []

            for part in exam_data.get("parts", []):
                part_name = part.get("part", "")
                questions = part.get("questions", [])

                # X√°c ƒë·ªãnh questionType d·ª±a tr√™n part
                question_type = self._map_part_to_question_type(part_name)

                for question in questions:
                    # T·∫°o question cho SpringBoot format
                    staff_question = {
                        "lessonId": None,  # Staff t·ª± ch·ªçn
                        "questionType": question_type,
                        "difficultyLevel": self._analyze_difficulty_level(question).value,
                        "questionContent": self._format_question_content(question, question_type),
                        "explanation": question.get("explanation", ""),
                        "referenceSource": exam_data.get("school") or "Imported from DOCX",
                        "suggest": self._generate_lesson_suggestions(question, exam_data)
                    }

                    staff_questions.append(staff_question)

            logger.info(f"Converted {len(staff_questions)} questions to staff format")
            return staff_questions

        except Exception as e:
            logger.error(f"Error converting to staff format: {e}")
            return []



    def _generate_lesson_suggestions(self, question: Dict[str, Any], exam_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        T·∫°o g·ª£i √Ω v·ªÅ b√†i h·ªçc d·ª±a tr√™n n·ªôi dung c√¢u h·ªèi

        Args:
            question: D·ªØ li·ªáu c√¢u h·ªèi
            exam_data: D·ªØ li·ªáu ƒë·ªÅ thi

        Returns:
            Dict: Th√¥ng tin g·ª£i √Ω cho staff academic
        """
        try:
            question_text = question.get("question", "").lower()

            # Ph√¢n t√≠ch keywords trong c√¢u h·ªèi
            chemistry_topics = self._analyze_chemistry_topics(question_text)

            # T·∫°o suggestions
            suggestions = {
                "keywords": self._extract_key_concepts(question_text),
                "topics": chemistry_topics,
                "subject_name": exam_data.get("subject", ""),
                "grade_level": exam_data.get("grade", 12)
            }

            return suggestions

        except Exception as e:
            logger.error(f"Error generating lesson suggestions: {e}")
            return {
                "keywords": [],
                "topics": [],
                "subject_name": exam_data.get("subject", ""),
                "grade_level": exam_data.get("grade", 12),
                "error": "Could not generate suggestions"
            }

    def _analyze_chemistry_topics(self, question_text: str) -> List[str]:
        """
        Ph√¢n t√≠ch ch·ªß ƒë·ªÅ h√≥a h·ªçc t·ª´ c√¢u h·ªèi

        Args:
            question_text: N·ªôi dung c√¢u h·ªèi

        Returns:
            List[str]: Danh s√°ch ch·ªß ƒë·ªÅ h√≥a h·ªçc
        """
        topics = []

        # Mapping keywords to chemistry topics
        topic_keywords = {
            "C·∫•u t·∫°o nguy√™n t·ª≠": ["nguy√™n t·ª≠", "proton", "neutron", "electron", "h·∫°t nh√¢n", "l·ªõp v·ªè", "orbital"],
            "B·∫£ng tu·∫ßn ho√†n": ["b·∫£ng tu·∫ßn ho√†n", "chu k·ª≥", "nh√≥m", "kim lo·∫°i", "phi kim", "kh√≠ hi·∫øm"],
            "Li√™n k·∫øt h√≥a h·ªçc": ["li√™n k·∫øt", "ion", "c·ªông h√≥a tr·ªã", "kim lo·∫°i", "ph√¢n t·ª≠", "tinh th·ªÉ"],
            "Ph·∫£n ·ª©ng h√≥a h·ªçc": ["ph·∫£n ·ª©ng", "oxi h√≥a", "kh·ª≠", "c√¢n b·∫±ng", "t·ªëc ƒë·ªô ph·∫£n ·ª©ng"],
            "Dung d·ªãch": ["dung d·ªãch", "n·ªìng ƒë·ªô", "mol", "ƒë·ªô tan", "ph", "acid", "base"],
            "H√≥a h·ªØu c∆°": ["hydrocarbon", "alcohol", "acid carboxylic", "ester", "amin", "protein"],
            "Nhi·ªát h√≥a h·ªçc": ["enthalpy", "entropy", "nƒÉng l∆∞·ª£ng", "nhi·ªát ƒë·ªô", "ch√°y"],
            "ƒêi·ªán h√≥a": ["ƒëi·ªán ph√¢n", "pin", "th·∫ø ƒëi·ªán c·ª±c", "ƒÉn m√≤n ƒëi·ªán h√≥a"]
        }

        for topic, keywords in topic_keywords.items():
            if any(keyword in question_text for keyword in keywords):
                topics.append(topic)

        return topics

    def _extract_key_concepts(self, question_text: str) -> List[str]:
        """
        Tr√≠ch xu·∫•t c√°c kh√°i ni·ªám ch√≠nh t·ª´ c√¢u h·ªèi

        Args:
            question_text: N·ªôi dung c√¢u h·ªèi

        Returns:
            List[str]: Danh s√°ch kh√°i ni·ªám ch√≠nh
        """
        concepts = []

        # Common chemistry concepts
        concept_patterns = [
            r'(nguy√™n t·ª≠ \w+)', r'(ph√¢n t·ª≠ \w+)', r'(ion \w+)',
            r'(axit \w+)', r'(baz∆° \w+)', r'(mu·ªëi \w+)',
            r'(kim lo·∫°i \w+)', r'(phi kim \w+)',
            r'(ph·∫£n ·ª©ng \w+)', r'(dung d·ªãch \w+)',
            r'(n·ªìng ƒë·ªô \w+)', r'(kh·ªëi l∆∞·ª£ng \w+)'
        ]

        for pattern in concept_patterns:
            matches = re.findall(pattern, question_text, re.IGNORECASE)
            concepts.extend(matches)

        # Remove duplicates and limit
        return list(set(concepts))[:5]

    def _map_part_to_question_type(self, part_name: str) -> str:
        """
        Map t√™n ph·∫ßn sang questionType cho SpringBoot

        Args:
            part_name: T√™n ph·∫ßn (Ph·∫ßn I, II, III)

        Returns:
            str: Question type
        """
        part_mapping = {
            "PH·∫¶N I": "PART_I",
            "Ph·∫ßn I": "PART_I",
            "PH·∫¶N II": "PART_II",
            "Ph·∫ßn II": "PART_II",
            "PH·∫¶N III": "PART_III",
            "Ph·∫ßn III": "PART_III"
        }

        return part_mapping.get(part_name, "PART_I")

    def _analyze_difficulty_level(self, question: Dict[str, Any]) -> DifficultyLevel:
        """
        Ph√¢n t√≠ch m·ª©c ƒë·ªô kh√≥ c·ªßa c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung
        D·ª±a theo c·∫•u tr√∫c ƒë·ªÅ thi THPT 2025: 75-80% nh·∫≠n bi·∫øt-th√¥ng hi·ªÉu, 20-25% v·∫≠n d·ª•ng

        Args:
            question: D·ªØ li·ªáu c√¢u h·ªèi

        Returns:
            DifficultyLevel: Difficulty level enum (KNOWLEDGE, COMPREHENSION, APPLICATION)
        """
        try:
            question_text = question.get("question", "").lower()
            options_text = ""

            # L·∫•y text t·ª´ c√°c l·ª±a ch·ªçn n·∫øu c√≥
            if "options" in question:
                options = question.get("options", {})
                if isinstance(options, dict):
                    options_text = " ".join([str(v).lower() for v in options.values() if v])
                elif isinstance(options, list):
                    options_text = " ".join([str(opt).lower() for opt in options if opt])

            full_text = f"{question_text} {options_text}"

            # Keywords cho m·ª©c ƒë·ªô NH·∫¨N BI·∫æT (Knowledge) - 40% ƒë·ªÅ thi
            knowledge_keywords = [
                # ƒê·ªãnh nghƒ©a, kh√°i ni·ªám c∆° b·∫£n
                "l√† g√¨", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám", "t√™n g·ªçi", "k√Ω hi·ªáu", "c√¥ng th·ª©c ph√¢n t·ª≠",
                "c√¥ng th·ª©c c·∫•u t·∫°o", "t√™n h√≥a h·ªçc", "thu·ªôc lo·∫°i", "ƒë∆∞·ª£c g·ªçi l√†",
                # Nh·∫≠n bi·∫øt t√≠nh ch·∫•t
                "t√≠nh ch·∫•t", "ƒë·∫∑c ƒëi·ªÉm", "m√†u s·∫Øc", "tr·∫°ng th√°i", "m√πi", "v·ªã",
                # Ph√¢n lo·∫°i c∆° b·∫£n
                "thu·ªôc nh√≥m", "lo·∫°i h·ª£p ch·∫•t", "ph√¢n lo·∫°i", "nh√≥m ch·ª©c",
                # C√¥ng th·ª©c v√† k√Ω hi·ªáu
                "k√Ω hi·ªáu h√≥a h·ªçc", "s·ªë hi·ªáu nguy√™n t·ª≠", "kh·ªëi l∆∞·ª£ng nguy√™n t·ª≠",
                "c·∫•u h√¨nh electron", "s·ªë electron", "s·ªë proton", "s·ªë neutron"
            ]

            # Keywords cho m·ª©c ƒë·ªô TH√îNG HI·ªÇU (Comprehension) - 35-40% ƒë·ªÅ thi
            comprehension_keywords = [
                # Gi·∫£i th√≠ch hi·ªán t∆∞·ª£ng
                "gi·∫£i th√≠ch", "t·∫°i sao", "nguy√™n nh√¢n", "do ƒë√¢u", "v√¨ sao",
                "ƒëi·ªÅu ki·ªán", "y·∫øu t·ªë ·∫£nh h∆∞·ªüng", "c∆° ch·∫ø", "qu√° tr√¨nh",
                # So s√°nh, ph√¢n bi·ªát
                "so s√°nh", "ph√¢n bi·ªát", "kh√°c nhau", "gi·ªëng nhau", "t∆∞∆°ng t·ª±",
                "kh√°c bi·ªát", "ƒëi·ªÉm chung", "ƒëi·ªÉm kh√°c",
                # M·ªëi quan h·ªá
                "li√™n quan", "·∫£nh h∆∞·ªüng", "t√°c ƒë·ªông", "ph·ª• thu·ªôc", "t·ªâ l·ªá",
                # D·ª± ƒëo√°n t√≠nh ch·∫•t
                "d·ª± ƒëo√°n", "nh·∫≠n x√©t", "k·∫øt lu·∫≠n", "suy ra", "cho bi·∫øt"
            ]

            # Keywords cho m·ª©c ƒë·ªô V·∫¨N D·ª§NG (Application) - 20-25% ƒë·ªÅ thi
            application_keywords = [
                # T√≠nh to√°n ƒë·ªãnh l∆∞·ª£ng
                "t√≠nh", "t√≠nh to√°n", "x√°c ƒë·ªãnh", "t√¨m", "kh·ªëi l∆∞·ª£ng", "th·ªÉ t√≠ch",
                "n·ªìng ƒë·ªô", "s·ªë mol", "hi·ªáu su·∫•t", "ƒë·ªô tan", "ph", "poh",
                "ph·∫ßn trƒÉm", "t·ªâ l·ªá ph·∫ßn trƒÉm", "kh·ªëi l∆∞·ª£ng ri√™ng",
                # Ph√¢n t√≠ch v√† ƒë√°nh gi√°
                "ph√¢n t√≠ch", "ƒë√°nh gi√°", "nh·∫≠n ƒë·ªãnh", "b√¨nh lu·∫≠n", "th·∫£o lu·∫≠n",
                # Thi·∫øt k·∫ø th√≠ nghi·ªám
                "thi·∫øt k·∫ø", "th√≠ nghi·ªám", "ph∆∞∆°ng ph√°p", "c√°ch ti·∫øn h√†nh",
                "quy tr√¨nh", "c√°c b∆∞·ªõc", "th·ª±c hi·ªán",
                # ·ª®ng d·ª•ng th·ª±c t·∫ø
                "·ª©ng d·ª•ng", "s·ª≠ d·ª•ng", "√°p d·ª•ng", "trong th·ª±c t·∫ø", "trong ƒë·ªùi s·ªëng",
                "s·∫£n xu·∫•t", "c√¥ng nghi·ªáp", "ch·∫ø t·∫°o", "ƒëi·ªÅu ch·∫ø"
            ]

            # ƒê·∫øm ƒëi·ªÉm cho t·ª´ng m·ª©c ƒë·ªô
            knowledge_score = sum(1 for keyword in knowledge_keywords if keyword in full_text)
            comprehension_score = sum(1 for keyword in comprehension_keywords if keyword in full_text)
            application_score = sum(1 for keyword in application_keywords if keyword in full_text)

            # Ph√¢n t√≠ch b·ªï sung d·ª±a tr√™n c·∫•u tr√∫c c√¢u h·ªèi
            # C√¢u h·ªèi c√≥ s·ªë li·ªáu c·ª• th·ªÉ + ƒë∆°n v·ªã th∆∞·ªùng l√† v·∫≠n d·ª•ng
            if re.search(r'\d+[.,]\d+|\d+\s*(g|ml|l|mol|m|%|¬∞c)', full_text):
                application_score += 2

            # C√¢u h·ªèi c√≥ ph∆∞∆°ng tr√¨nh h√≥a h·ªçc th∆∞·ªùng l√† th√¥ng hi·ªÉu ho·∫∑c v·∫≠n d·ª•ng
            if re.search(r'[A-Z][a-z]?\s*\+|‚Üí|‚Üî|=', full_text):
                comprehension_score += 1

            # ∆Øu ti√™n m·∫°nh cho t·ª´ kh√≥a th√¥ng hi·ªÉu
            if re.search(r'gi·∫£i th√≠ch|so s√°nh|ph√¢n bi·ªát', full_text):
                comprehension_score += 3

            # C√¢u h·ªèi c√≥ t·ª´ "t√≠nh", "t√¨m" th∆∞·ªùng l√† v·∫≠n d·ª•ng
            elif re.search(r't√≠nh|t√¨m', full_text):
                application_score += 2

            # C√¢u h·ªèi c√≥ "x√°c ƒë·ªãnh" + s·ªë li·ªáu th∆∞·ªùng l√† v·∫≠n d·ª•ng
            elif re.search(r'x√°c ƒë·ªãnh', full_text) and re.search(r'\d+', full_text):
                application_score += 1

            # Quy·∫øt ƒë·ªãnh m·ª©c ƒë·ªô d·ª±a tr√™n ƒëi·ªÉm s·ªë
            if application_score > max(knowledge_score, comprehension_score):
                return DifficultyLevel.APPLICATION
            elif comprehension_score > knowledge_score:
                return DifficultyLevel.COMPREHENSION
            else:
                return DifficultyLevel.KNOWLEDGE

        except Exception as e:
            logger.error(f"Error analyzing difficulty level: {e}")
            return DifficultyLevel.KNOWLEDGE

    def _format_question_content(self, question: Dict[str, Any], question_type: str) -> Dict[str, Any]:
        """
        Format n·ªôi dung c√¢u h·ªèi theo t·ª´ng lo·∫°i

        Args:
            question: D·ªØ li·ªáu c√¢u h·ªèi
            question_type: Lo·∫°i c√¢u h·ªèi (PART_I, PART_II, PART_III)

        Returns:
            Dict: Question content theo format SpringBoot
        """
        try:
            base_content = {
                "question": question.get("question", ""),
                "image": None  # C√≥ th·ªÉ th√™m sau n·∫øu c·∫ßn
            }

            if question_type == "PART_I":
                # Multiple choice
                base_content.update({
                    "options": question.get("options", {}),
                    "answer": question.get("answer", "")
                })

            elif question_type == "PART_II":
                # True/False statements
                statements = question.get("statements", {})
                base_content["statements"] = statements

            elif question_type == "PART_III":
                # Short answer
                base_content["answer"] = question.get("answer", "")

            return base_content

        except Exception as e:
            logger.error(f"Error formatting question content: {e}")
            return {"question": question.get("question", ""), "image": None}


# Factory function ƒë·ªÉ t·∫°o ExamImportService instance
def get_exam_import_service() -> ExamImportService:
    """
    T·∫°o ExamImportService instance m·ªõi

    Returns:
        ExamImportService: Fresh instance
    """
    return ExamImportService()


