"""
Service ƒë·ªÉ import ƒë·ªÅ thi t·ª´ file DOCX v√† chuy·ªÉn ƒë·ªïi th√†nh JSON
"""

import logging
import json
import re
import time
from typing import Dict, Any, Optional
from docx import Document
import io

from app.services.openrouter_service import get_openrouter_service
from app.models.exam_import_models import (
    ExamImportRequest,
    ExamImportResponse,
    ExamImportError,
    ImportedExamData,
    ExamImportStatistics
)

logger = logging.getLogger(__name__)


class ExamImportService:
    """Service ƒë·ªÉ import v√† x·ª≠ l√Ω ƒë·ªÅ thi t·ª´ file DOCX"""

    def __init__(self):
        self.model_name = "google/gemini-2.0-flash-001"
        logger.info("üîÑ ExamImportService: First-time initialization triggered")

    async def import_exam_from_docx_content(
        self, file_content: bytes, filename: str = "exam.docx"
    ) -> Dict[str, Any]:
        """
        Import ƒë·ªÅ thi t·ª´ n·ªôi dung file DOCX

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes
            filename: T√™n file g·ªëc

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ import
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting DOCX import for file: {filename}")
            
            # 1. Extract text t·ª´ DOCX
            extracted_text = self._extract_text_from_docx_bytes(file_content)
            
            if not extracted_text or len(extracted_text.strip()) < 100:
                return ExamImportError(
                    message="File extraction failed",
                    error="Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ file DOCX ho·∫∑c n·ªôi dung qu√° ng·∫Øn",
                    error_code="EXTRACTION_FAILED",
                    details={"filename": filename, "extracted_length": len(extracted_text)}
                ).model_dump()

            logger.info(f"Extracted {len(extracted_text)} characters from DOCX")

            # 2. Validate format ƒë·ªÅ thi tr∆∞·ªõc khi g·ªçi LLM
            logger.info("Validating exam format...")
            format_validation = self._validate_exam_format(extracted_text)

            if not format_validation["is_valid"]:
                return ExamImportError(
                    message="Invalid exam format",
                    error=f"ƒê·ªÅ thi kh√¥ng ƒë√∫ng format chu·∫©n: {format_validation['error']}",
                    error_code="INVALID_FORMAT",
                    details={
                        "filename": filename,
                        "validation_details": format_validation["details"]
                    }
                ).model_dump()

            # L∆∞u th√¥ng tin warnings ƒë·ªÉ tr·∫£ v·ªÅ sau
            format_warnings = format_validation.get("warnings", [])
            missing_parts = format_validation.get("details", {}).get("missing_parts", [])

            logger.info(f"Exam format validation passed with warnings: {format_warnings}")

            # 3. G·ª≠i cho LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi
            logger.info("Sending content to LLM for analysis...")
            llm_result = await self._analyze_exam_with_llm(extracted_text, filename)

            if not llm_result.get("success", False):
                return ExamImportError(
                    message="LLM analysis failed",
                    error=f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi: {llm_result.get('error', 'Unknown error')}",
                    error_code="LLM_ANALYSIS_FAILED",
                    details={"filename": filename}
                ).model_dump()

            # 3. Parse JSON response t·ª´ LLM
            exam_data = llm_result.get("data")
            if not exam_data:
                return ExamImportError(
                    message="No exam data returned",
                    error="LLM kh√¥ng tr·∫£ v·ªÅ d·ªØ li·ªáu ƒë·ªÅ thi",
                    error_code="NO_EXAM_DATA",
                    details={"filename": filename}
                ).model_dump()

            # 4. Validate v√† clean d·ªØ li·ªáu t·ª´ LLM
            logger.info("Validating and cleaning LLM data...")
            validation_result = self._validate_and_clean_exam_data(exam_data)

            if not validation_result["is_valid"]:
                return ExamImportError(
                    message="Invalid exam data from LLM",
                    error=f"D·ªØ li·ªáu t·ª´ LLM kh√¥ng h·ª£p l·ªá: {validation_result['error']}",
                    error_code="INVALID_LLM_DATA",
                    details={
                        "filename": filename,
                        "validation_details": validation_result["details"]
                    }
                ).model_dump()

            # S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean
            exam_data = validation_result["cleaned_data"]

            # 4. Validate v√† t·∫°o response
            processing_time = time.time() - start_time

            # T√≠nh to√°n statistics
            statistics = self._calculate_import_statistics(exam_data)

            # T·∫°o message v·ªõi th√¥ng tin v·ªÅ c√°c ph·∫ßn thi·∫øu
            success_message = "ƒê·ªÅ thi ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng"
            if format_warnings:
                success_message += f" (L∆∞u √Ω: {'; '.join(format_warnings)})"

            # T·∫°o response v·ªõi th√¥ng tin b·ªï sung
            response_data = ExamImportResponse(
                success=True,
                message=success_message,
                data=ImportedExamData(**exam_data),
                processing_time=processing_time
            ).model_dump()

            # Th√™m th√¥ng tin v·ªÅ warnings v√† missing parts
            response_data["warnings"] = format_warnings
            response_data["missing_parts"] = missing_parts
            response_data["statistics"] = statistics.model_dump()

            return response_data

        except Exception as e:
            logger.error(f"Error importing exam from DOCX: {e}")
            processing_time = time.time() - start_time
            
            return ExamImportError(
                message="Import failed",
                error=f"L·ªói trong qu√° tr√¨nh import: {str(e)}",
                error_code="IMPORT_ERROR",
                details={
                    "filename": filename,
                    "processing_time": processing_time,
                    "error_type": type(e).__name__
                }
            ).model_dump()

    def _extract_text_from_docx_bytes(self, file_content: bytes) -> str:
        """
        Tr√≠ch xu·∫•t text t·ª´ file DOCX bytes

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes

        Returns:
            Text ƒë√£ tr√≠ch xu·∫•t
        """
        try:
            # T·∫°o file-like object t·ª´ bytes
            file_stream = io.BytesIO(file_content)
            
            # ƒê·ªçc document
            doc = Document(file_stream)
            
            # Tr√≠ch xu·∫•t text t·ª´ t·∫•t c·∫£ paragraphs
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            # Tr√≠ch xu·∫•t text t·ª´ tables
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        full_text.append(" | ".join(row_text))
            
            extracted_text = "\n".join(full_text)
            logger.info(f"Successfully extracted {len(extracted_text)} characters from DOCX")
            
            return extracted_text

        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    def _validate_exam_format(self, exam_text: str) -> Dict[str, Any]:
        """
        Validate format ƒë·ªÅ thi c∆° b·∫£n - ki·ªÉm tra c√≥ √≠t nh·∫•t 1 ph·∫ßn v√† ƒë√°p √°n

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation
        """
        try:
            # Normalize text ƒë·ªÉ d·ªÖ ki·ªÉm tra
            normalized_text = exam_text.upper().replace('\n', ' ').replace('\r', '')

            validation_result = {
                "is_valid": True,
                "error": "",
                "details": {},
                "warnings": []
            }

            # 1. Ki·ªÉm tra c·∫•u tr√∫c c√°c ph·∫ßn
            all_parts = ["PH·∫¶N I", "PH·∫¶N II", "PH·∫¶N III"]
            found_parts = []
            missing_parts = []

            for part in all_parts:
                if part in normalized_text:
                    found_parts.append(part)
                else:
                    missing_parts.append(part)

            # Ch·ªâ y√™u c·∫ßu c√≥ √≠t nh·∫•t 1 ph·∫ßn
            if not found_parts:
                validation_result["is_valid"] = False
                validation_result["error"] = "Kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c ph·∫ßn n√†o (PH·∫¶N I, II, III)"
                validation_result["details"]["missing_parts"] = missing_parts
                return validation_result

            # Ghi nh·∫≠n c√°c ph·∫ßn thi·∫øu nh∆∞ warning
            if missing_parts:
                validation_result["warnings"].append(f"Thi·∫øu c√°c ph·∫ßn: {', '.join(missing_parts)}")
                validation_result["details"]["missing_parts"] = missing_parts
                validation_result["details"]["found_parts"] = found_parts

            # 2. Ki·ªÉm tra ph·∫ßn ƒë√°p √°n (kh√¥ng b·∫Øt bu·ªôc)
            if "ƒê√ÅP √ÅN" not in normalized_text:
                validation_result["warnings"].append("Kh√¥ng t√¨m th·∫•y ph·∫ßn ƒë√°p √°n")
                validation_result["details"]["missing_answer_section"] = True

            logger.info(f"Exam format validation passed - Found parts: {found_parts}, Missing: {missing_parts}")
            return validation_result

        except Exception as e:
            logger.error(f"Error in format validation: {e}")
            return {
                "is_valid": False,
                "error": f"L·ªói trong qu√° tr√¨nh validate format: {str(e)}",
                "details": {"exception": str(e)},
                "warnings": []
            }

    async def _analyze_exam_with_llm(self, exam_text: str, filename: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi ƒë·ªÅ thi th√†nh JSON

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract
            filename: T√™n file g·ªëc

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        try:
            # T·∫°o prompt cho LLM
            prompt = self._create_analysis_prompt(exam_text, filename)
            
            # G·ªçi LLM
            openrouter_service = get_openrouter_service()
            response = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=8000
            )

            if not response.get("success", False):
                return {
                    "success": False,
                    "error": f"LLM request failed: {response.get('error', 'Unknown error')}"
                }

            # Parse JSON t·ª´ response
            llm_content = response.get("text", "") or response.get("content", "")
            logger.info(f"LLM response length: {len(llm_content)}")
            logger.info(f"LLM response preview: {llm_content[:500]}...")

            # T√¨m JSON trong response
            json_data = self._extract_json_from_response(llm_content)
            
            if not json_data:
                return {
                    "success": False,
                    "error": "Kh√¥ng th·ªÉ t√¨m th·∫•y JSON h·ª£p l·ªá trong response c·ªßa LLM"
                }

            return {
                "success": True,
                "data": json_data
            }

        except Exception as e:
            logger.error(f"Error in LLM analysis: {e}")
            return {
                "success": False,
                "error": f"LLM analysis error: {str(e)}"
            }

    def _create_analysis_prompt(self, exam_text: str, filename: str) -> str:
        """
        T·∫°o prompt cho LLM ƒë·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi
            filename: T√™n file

        Returns:
            Prompt string
        """
        prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch ƒë·ªÅ thi. H√£y ph√¢n t√≠ch n·ªôi dung ƒë·ªÅ thi sau v√† chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng JSON ch√≠nh x√°c.

N·ªòI DUNG ƒê·ªÄ THI:
{exam_text}

Y√äU C·∫¶U:
1. Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t th√¥ng tin ƒë·ªÅ thi th√†nh JSON v·ªõi c·∫•u tr√∫c ch√≠nh x√°c nh∆∞ m·∫´u
2. X√°c ƒë·ªãnh m√¥n h·ªçc, l·ªõp, th·ªùi gian l√†m b√†i, t√™n tr∆∞·ªùng
3. Ph√¢n chia c√¢u h·ªèi theo c√°c ph·∫ßn c√≥ s·∫µn trong ƒë·ªÅ thi:
   - Ph·∫ßn I: Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn (A, B, C, D) - n·∫øu c√≥
   - Ph·∫ßn II: Tr·∫Øc nghi·ªám ƒë√∫ng/sai (a, b, c, d v·ªõi true/false) - n·∫øu c√≥
   - Ph·∫ßn III: Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn (ch·ªâ s·ªë) - n·∫øu c√≥
4. Ch·ªâ x·ª≠ l√Ω c√°c ph·∫ßn th·ª±c s·ª± c√≥ trong ƒë·ªÅ thi, b·ªè qua ph·∫ßn kh√¥ng c√≥
5. Tr√≠ch xu·∫•t ƒë√°p √°n ch√≠nh x√°c t·ª´ ph·∫ßn ƒë√°p √°n (n·∫øu c√≥)
6. N·∫øu l√† m√¥n H√≥a h·ªçc, tr√≠ch xu·∫•t b·∫£ng nguy√™n t·ª≠ kh·ªëi (n·∫øu c√≥)

ƒê·ªäNH D·∫†NG JSON MONG MU·ªêN:
{{
  "subject": "H√≥a h·ªçc",
  "grade": 12,
  "duration_minutes": 90,
  "school": "Tr∆∞·ªùng THPT Hong Thinh",
  "exam_code": "1234",
  "atomic_masses": "H = 1; C = 12; N = 14; O = 16",
  "parts": [
    {{
      "part": "Ph·∫ßn I",
      "title": "Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn",
      "description": "M·ªói c√¢u tr·∫£ l·ªùi ƒë√∫ng th√≠ sinh ƒë∆∞·ª£c 0,25 ƒëi·ªÉm",
      "questions": [
        {{
          "id": 1,
          "question": "Nguy√™n t·ª≠ carbon c√≥ bao nhi√™u electron?",
          "options": {{
            "A": "4",
            "B": "6",
            "C": "8",
            "D": "12"
          }},
          "answer": "B"
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn II",
      "title": "Tr·∫Øc nghi·ªám ƒë√∫ng/sai",
      "description": "Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u X. M·ªói c√¢u c√≥ 4 ph√°t bi·ªÉu a), b), c), d)",
      "questions": [
        {{
          "id": 1,
          "question": "Cho c√°c ph√°t bi·ªÉu v·ªÅ nguy√™n t·ª≠ carbon:",
          "statements": {{
            "a": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 6 proton",
              "answer": true
            }},
            "b": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 8 neutron",
              "answer": false
            }},
            "c": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ 6 electron",
              "answer": true
            }},
            "d": {{
              "text": "Nguy√™n t·ª≠ carbon c√≥ kh·ªëi l∆∞·ª£ng 14u",
              "answer": false
            }}
          }}
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn III",
      "title": "Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn",
      "description": "Ch·ªâ ghi s·ªë, kh√¥ng ghi ƒë∆°n v·ªã. T·ªëi ƒëa 4 k√Ω t·ª±.",
      "questions": [
        {{
          "id": 1,
          "question": "S·ªë electron trong nguy√™n t·ª≠ carbon l√† bao nhi√™u?",
          "answer": "6"
        }}
      ]
    }}
  ]
}}

L∆∞U √ù QUAN TR·ªåNG V·ªÄ C·∫§U TR√öC:
- Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng th√™m text gi·∫£i th√≠ch
- ƒê·∫£m b·∫£o t·∫•t c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n ƒë∆∞·ª£c tr√≠ch xu·∫•t ch√≠nh x√°c
- QUAN TR·ªåNG: M·ªói lo·∫°i c√¢u h·ªèi c√≥ c·∫•u tr√∫c kh√°c nhau:

  * PH·∫¶N I (Tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn): PH·∫¢I c√≥ "options" v√† "answer"
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "options": {{"A": "...", "B": "...", "C": "...", "D": "..."}},
      "answer": "A"
    }}

  * PH·∫¶N II (ƒê√∫ng/sai): PH·∫¢I c√≥ "statements", KH√îNG c√≥ "answer" field
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "statements": {{
        "a": {{"text": "...", "answer": true}},
        "b": {{"text": "...", "answer": false}},
        "c": {{"text": "...", "answer": true}},
        "d": {{"text": "...", "answer": false}}
      }}
    }}

  * PH·∫¶N III (Tr·∫£ l·ªùi ng·∫Øn): PH·∫¢I c√≥ "answer" string, KH√îNG c√≥ "options" hay "statements"
    {{
      "id": 1,
      "question": "C√¢u h·ªèi c·ª• th·ªÉ...",
      "answer": "6"
    }}

- QUAN TR·ªåNG: Ch·ªâ t·∫°o parts cho nh·ªØng ph·∫ßn c√≥ N·ªòI DUNG C√ÇU H·ªéI th·ª±c t·∫ø trong ƒë·ªÅ thi
- N·∫øu ph·∫ßn ƒë√°p √°n c√≥ nh∆∞ng kh√¥ng c√≥ n·ªôi dung c√¢u h·ªèi, KH√îNG t·∫°o part cho ph·∫ßn ƒë√≥
- M·∫£ng "parts" c√≥ th·ªÉ ch·ª©a 1, 2 ho·∫∑c 3 ph·∫ßn t√πy theo n·ªôi dung ƒë·ªÅ thi th·ª±c t·∫ø
- Kh√¥ng t·∫°o ra c√¢u h·ªèi gi·∫£ cho c√°c ph·∫ßn kh√¥ng c√≥ n·ªôi dung
- ƒê·∫£m b·∫£o field "question" lu√¥n l√† string kh√¥ng r·ªóng, kh√¥ng ƒë∆∞·ª£c null
- V√≠ d·ª•: N·∫øu ƒë·ªÅ thi ch·ªâ c√≥ "PH·∫¶N I" v·ªõi n·ªôi dung c√¢u h·ªèi, ch·ªâ t·∫°o 1 part cho Ph·∫ßn I, b·ªè qua Ph·∫ßn II v√† III d√π c√≥ trong ƒë√°p √°n

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON:
"""
        return prompt

    def _extract_json_from_response(self, response_content: str) -> Optional[Dict[str, Any]]:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ response c·ªßa LLM

        Args:
            response_content: N·ªôi dung response t·ª´ LLM

        Returns:
            Dict ch·ª©a JSON data ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            logger.info(f"Extracting JSON from response: {response_content[:200]}...")

            # 1. T√¨m JSON trong markdown code blocks
            markdown_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            markdown_matches = re.findall(markdown_pattern, response_content, re.DOTALL)

            for match in markdown_matches:
                try:
                    json_data = json.loads(match.strip())
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted JSON from markdown code block")
                        return json_data
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON from markdown: {e}")
                    continue

            # 2. T√¨m JSON trong response (c√≥ th·ªÉ c√≥ text bao quanh)
            json_pattern = r'\{.*\}'
            matches = re.findall(json_pattern, response_content, re.DOTALL)

            for match in matches:
                try:
                    # Th·ª≠ parse JSON
                    json_data = json.loads(match)

                    # Validate c·∫•u tr√∫c c∆° b·∫£n
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted and validated JSON from LLM response")
                        return json_data

                except json.JSONDecodeError:
                    continue

            # 3. N·∫øu kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá, th·ª≠ parse to√†n b·ªô response
            try:
                json_data = json.loads(response_content.strip())
                if self._validate_exam_json_structure(json_data):
                    logger.info("Successfully parsed entire response as JSON")
                    return json_data
            except json.JSONDecodeError:
                pass

            logger.error("Could not extract valid JSON from LLM response")
            logger.error(f"Response content: {response_content}")
            return None

        except Exception as e:
            logger.error(f"Error extracting JSON from response: {e}")
            return None

    def _validate_exam_json_structure(self, json_data: Dict[str, Any]) -> bool:
        """
        Validate c·∫•u tr√∫c JSON c·ªßa ƒë·ªÅ thi - linh ho·∫°t v·ªõi parts

        Args:
            json_data: JSON data c·∫ßn validate

        Returns:
            True n·∫øu c·∫•u tr√∫c h·ª£p l·ªá
        """
        try:
            required_fields = ["subject", "grade", "duration_minutes", "school", "parts"]

            # Ki·ªÉm tra c√°c field b·∫Øt bu·ªôc
            for field in required_fields:
                if field not in json_data:
                    logger.error(f"Missing required field: {field}")
                    return False

            # Ki·ªÉm tra parts - cho ph√©p empty list
            parts = json_data.get("parts", [])
            if not isinstance(parts, list):
                logger.error("Parts must be a list")
                return False

            # N·∫øu c√≥ parts, ki·ªÉm tra c·∫•u tr√∫c c·ªßa t·ª´ng part
            for i, part in enumerate(parts):
                if not isinstance(part, dict):
                    logger.error(f"Part {i} must be a dictionary")
                    return False

                part_required = ["part", "title", "description", "questions"]
                for field in part_required:
                    if field not in part:
                        logger.error(f"Missing required field '{field}' in part {i}")
                        return False

                # Ki·ªÉm tra questions - cho ph√©p empty list
                questions = part.get("questions", [])
                if not isinstance(questions, list):
                    logger.error(f"Questions in part {i} must be a list")
                    return False

            logger.info(f"JSON structure validation passed - {len(parts)} parts found")
            return True

        except Exception as e:
            logger.error(f"Error validating JSON structure: {e}")
            return False

    def _validate_and_clean_exam_data(self, exam_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate v√† clean d·ªØ li·ªáu ƒë·ªÅ thi t·ª´ LLM

        Args:
            exam_data: D·ªØ li·ªáu th√¥ t·ª´ LLM

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            logger.info("Starting exam data validation and cleaning...")

            result = {
                "is_valid": True,
                "error": "",
                "details": {},
                "cleaned_data": {}
            }

            # 1. Validate basic fields
            required_fields = ["subject", "grade", "duration_minutes", "school", "parts"]
            for field in required_fields:
                if field not in exam_data:
                    result["is_valid"] = False
                    result["error"] = f"Missing required field: {field}"
                    return result

            # 2. Clean basic data
            cleaned_data = {
                "subject": str(exam_data.get("subject", "")).strip(),
                "grade": int(exam_data.get("grade", 12)),
                "duration_minutes": int(exam_data.get("duration_minutes", 90)),
                "school": str(exam_data.get("school", "")).strip(),
                "exam_code": str(exam_data.get("exam_code", "")).strip() if exam_data.get("exam_code") else None,
                "atomic_masses": str(exam_data.get("atomic_masses", "")).strip() if exam_data.get("atomic_masses") else None,
                "parts": []
            }

            # 3. Validate v√† clean parts
            parts = exam_data.get("parts", [])
            if not isinstance(parts, list):
                result["is_valid"] = False
                result["error"] = "Parts must be a list"
                return result

            cleaned_parts = []
            skipped_parts = []

            for i, part in enumerate(parts):
                cleaned_part = self._clean_exam_part(part, i)
                if cleaned_part["is_valid"]:
                    cleaned_parts.append(cleaned_part["data"])
                else:
                    # Log warning v√† skip part n√†y thay v√¨ fail to√†n b·ªô
                    part_name = part.get("part", f"Part {i}")
                    logger.warning(f"Skipping invalid part '{part_name}': {cleaned_part['error']}")
                    skipped_parts.append({
                        "part_name": part_name,
                        "error": cleaned_part["error"]
                    })

            # Ch·ªâ fail n·∫øu kh√¥ng c√≥ part n√†o h·ª£p l·ªá
            if not cleaned_parts:
                result["is_valid"] = False
                result["error"] = "No valid parts found in exam data"
                result["details"]["skipped_parts"] = skipped_parts
                return result

            cleaned_data["parts"] = cleaned_parts
            result["cleaned_data"] = cleaned_data

            # Th√™m th√¥ng tin v·ªÅ parts b·ªã skip
            if skipped_parts:
                result["details"]["skipped_parts"] = skipped_parts
                logger.info(f"Exam data validation completed - {len(cleaned_parts)} valid parts, {len(skipped_parts)} skipped parts")
            else:
                logger.info(f"Exam data validation completed - {len(cleaned_parts)} parts validated")

            return result

        except Exception as e:
            logger.error(f"Error in exam data validation: {e}")
            return {
                "is_valid": False,
                "error": f"Validation error: {str(e)}",
                "details": {"exception": str(e)},
                "cleaned_data": {}
            }

    def _clean_exam_part(self, part_data: Dict[str, Any], part_index: int) -> Dict[str, Any]:
        """
        Clean v√† validate m·ªôt ph·∫ßn c·ªßa ƒë·ªÅ thi

        Args:
            part_data: D·ªØ li·ªáu ph·∫ßn th√¥ t·ª´ LLM
            part_index: Index c·ªßa ph·∫ßn

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            result = {
                "is_valid": True,
                "error": "",
                "data": {}
            }

            # Validate basic part fields
            required_part_fields = ["part", "title", "description", "questions"]
            for field in required_part_fields:
                if field not in part_data:
                    result["is_valid"] = False
                    result["error"] = f"Missing field '{field}' in part"
                    return result

            # Clean part basic data
            cleaned_part = {
                "part": str(part_data.get("part", "")).strip(),
                "title": str(part_data.get("title", "")).strip(),
                "description": str(part_data.get("description", "")).strip(),
                "questions": []
            }

            # Validate v√† clean questions
            questions = part_data.get("questions", [])
            if not isinstance(questions, list):
                result["is_valid"] = False
                result["error"] = "Questions must be a list"
                return result

            cleaned_questions = []
            invalid_questions = []

            for j, question in enumerate(questions):
                cleaned_question = self._clean_question(question, cleaned_part["part"], j)
                if cleaned_question["is_valid"]:
                    cleaned_questions.append(cleaned_question["data"])
                else:
                    # Log invalid question nh∆∞ng kh√¥ng fail to√†n b·ªô part
                    logger.warning(f"Skipping invalid question {j} in {cleaned_part['part']}: {cleaned_question['error']}")
                    invalid_questions.append({
                        "question_index": j,
                        "error": cleaned_question["error"]
                    })

            # N·∫øu kh√¥ng c√≥ c√¢u h·ªèi h·ª£p l·ªá n√†o, fail part n√†y
            if not cleaned_questions:
                result["is_valid"] = False
                result["error"] = f"No valid questions found in part. Invalid questions: {len(invalid_questions)}"
                return result

            cleaned_part["questions"] = cleaned_questions
            result["data"] = cleaned_part

            return result

        except Exception as e:
            return {
                "is_valid": False,
                "error": f"Error cleaning part: {str(e)}",
                "data": {}
            }

    def _clean_question(self, question_data: Dict[str, Any], part_name: str, question_index: int) -> Dict[str, Any]:
        """
        Clean v√† validate m·ªôt c√¢u h·ªèi theo lo·∫°i ph·∫ßn

        Args:
            question_data: D·ªØ li·ªáu c√¢u h·ªèi th√¥ t·ª´ LLM
            part_name: T√™n ph·∫ßn (ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi)
            question_index: Index c·ªßa c√¢u h·ªèi

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† d·ªØ li·ªáu ƒë√£ clean
        """
        try:
            result = {
                "is_valid": True,
                "error": "",
                "data": {}
            }

            # Validate basic question fields
            if "id" not in question_data or "question" not in question_data:
                result["is_valid"] = False
                result["error"] = "Missing 'id' or 'question' field"
                return result

            # Clean basic question data
            question_text = question_data.get("question")
            if not question_text or question_text is None or str(question_text).strip() == "":
                result["is_valid"] = False
                result["error"] = f"Question text cannot be null or empty. Got: {question_text}"
                return result

            cleaned_question = {
                "id": int(question_data.get("id", question_index + 1)),
                "question": str(question_text).strip()
            }

            # Clean theo lo·∫°i ph·∫ßn
            if "Ph·∫ßn I" in part_name or "PH·∫¶N I" in part_name:
                # MultipleChoiceQuestion
                if "options" not in question_data or "answer" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "MultipleChoice question missing 'options' or 'answer'"
                    return result

                options = question_data.get("options", {})
                if not isinstance(options, dict) or not all(k in options for k in ["A", "B", "C", "D"]):
                    result["is_valid"] = False
                    result["error"] = "Options must contain A, B, C, D"
                    return result

                cleaned_question["options"] = {
                    "A": str(options.get("A", "")).strip(),
                    "B": str(options.get("B", "")).strip(),
                    "C": str(options.get("C", "")).strip(),
                    "D": str(options.get("D", "")).strip()
                }
                cleaned_question["answer"] = str(question_data.get("answer", "")).strip()

            elif "Ph·∫ßn II" in part_name or "PH·∫¶N II" in part_name:
                # TrueFalseQuestion
                if "statements" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "TrueFalse question missing 'statements'"
                    return result

                statements = question_data.get("statements", {})
                if not isinstance(statements, dict) or not all(k in statements for k in ["a", "b", "c", "d"]):
                    result["is_valid"] = False
                    result["error"] = "Statements must contain a, b, c, d"
                    return result

                cleaned_statements = {}
                for key in ["a", "b", "c", "d"]:
                    stmt = statements.get(key, {})
                    if not isinstance(stmt, dict) or "text" not in stmt or "answer" not in stmt:
                        result["is_valid"] = False
                        result["error"] = f"Statement {key} missing 'text' or 'answer'"
                        return result

                    cleaned_statements[key] = {
                        "text": str(stmt.get("text", "")).strip(),
                        "answer": bool(stmt.get("answer", False))
                    }

                cleaned_question["statements"] = cleaned_statements

            elif "Ph·∫ßn III" in part_name or "PH·∫¶N III" in part_name:
                # ShortAnswerQuestion
                if "answer" not in question_data:
                    result["is_valid"] = False
                    result["error"] = "ShortAnswer question missing 'answer'"
                    return result

                cleaned_question["answer"] = str(question_data.get("answer", "")).strip()

            else:
                result["is_valid"] = False
                result["error"] = f"Unknown part type: {part_name}"
                return result

            result["data"] = cleaned_question
            return result

        except Exception as e:
            return {
                "is_valid": False,
                "error": f"Error cleaning question: {str(e)}",
                "data": {}
            }

    def _calculate_import_statistics(self, exam_data: Dict[str, Any]) -> ExamImportStatistics:
        """
        T√≠nh to√°n th·ªëng k√™ cho ƒë·ªÅ thi ƒë√£ import

        Args:
            exam_data: D·ªØ li·ªáu ƒë·ªÅ thi

        Returns:
            ExamImportStatistics object
        """
        try:
            parts = exam_data.get("parts", [])
            
            total_questions = 0
            part_1_questions = 0
            part_2_questions = 0
            part_3_questions = 0
            
            for part in parts:
                questions = part.get("questions", [])
                part_name = part.get("part", "").lower()
                
                if "i" in part_name or "1" in part_name:
                    part_1_questions = len(questions)
                elif "ii" in part_name or "2" in part_name:
                    part_2_questions = len(questions)
                elif "iii" in part_name or "3" in part_name:
                    part_3_questions = len(questions)
                
                total_questions += len(questions)
            
            has_atomic_masses = bool(exam_data.get("atomic_masses"))
            
            # T√≠nh ch·∫•t l∆∞·ª£ng x·ª≠ l√Ω (d·ª±a tr√™n s·ªë c√¢u h·ªèi v√† c·∫•u tr√∫c)
            processing_quality = min(1.0, (total_questions / 20) * 0.8 + 0.2)
            
            return ExamImportStatistics(
                total_questions=total_questions,
                part_1_questions=part_1_questions,
                part_2_questions=part_2_questions,
                part_3_questions=part_3_questions,
                has_atomic_masses=has_atomic_masses,
                processing_quality=processing_quality
            )

        except Exception as e:
            logger.error(f"Error calculating import statistics: {e}")
            return ExamImportStatistics(
                total_questions=0,
                part_1_questions=0,
                part_2_questions=0,
                part_3_questions=0,
                has_atomic_masses=False,
                processing_quality=0.0
            )


# Lazy loading global instance ƒë·ªÉ tr√°nh kh·ªüi t·∫°o ngay khi import
_exam_import_service_instance = None

def get_exam_import_service() -> ExamImportService:
    """
    L·∫•y singleton instance c·ªßa ExamImportService
    Lazy initialization

    Returns:
        ExamImportService: Service instance
    """
    global _exam_import_service_instance
    if _exam_import_service_instance is None:
        _exam_import_service_instance = ExamImportService()
    return _exam_import_service_instance

# Backward compatibility - deprecated, s·ª≠ d·ª•ng get_exam_import_service() thay th·∫ø
# Lazy loading ƒë·ªÉ tr√°nh kh·ªüi t·∫°o ngay khi import
def _get_exam_import_service_lazy():
    """Lazy loading cho backward compatibility"""
    return get_exam_import_service()

# T·∫°o proxy object ƒë·ªÉ lazy loading
class _ExamImportServiceProxy:
    def __getattr__(self, name):
        return getattr(_get_exam_import_service_lazy(), name)

    def __call__(self, *args, **kwargs):
        return _get_exam_import_service_lazy()(*args, **kwargs)

exam_import_service = _ExamImportServiceProxy()
