"""
Service ƒë·ªÉ import ƒë·ªÅ thi t·ª´ file DOCX v√† chuy·ªÉn ƒë·ªïi th√†nh JSON
"""

import logging
import json
import re
import time
from typing import Dict, Any, Optional
from docx import Document
import io

from app.services.openrouter_service import get_openrouter_service
from app.models.exam_import_models import (
    ExamImportRequest,
    ExamImportResponse,
    ExamImportError,
    ImportedExamData,
    ExamImportStatistics
)

logger = logging.getLogger(__name__)


class ExamImportService:
    """Service ƒë·ªÉ import v√† x·ª≠ l√Ω ƒë·ªÅ thi t·ª´ file DOCX"""

    def __init__(self):
        self.model_name = "google/gemini-2.0-flash-001"
        logger.info("üîÑ ExamImportService: First-time initialization triggered")

    async def import_exam_from_docx_content(
        self, file_content: bytes, filename: str = "exam.docx"
    ) -> Dict[str, Any]:
        """
        Import ƒë·ªÅ thi t·ª´ n·ªôi dung file DOCX

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes
            filename: T√™n file g·ªëc

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ import
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting DOCX import for file: {filename}")
            
            # 1. Extract text t·ª´ DOCX
            extracted_text = self._extract_text_from_docx_bytes(file_content)
            
            if not extracted_text or len(extracted_text.strip()) < 100:
                return ExamImportError(
                    message="File extraction failed",
                    error="Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ file DOCX ho·∫∑c n·ªôi dung qu√° ng·∫Øn",
                    error_code="EXTRACTION_FAILED",
                    details={"filename": filename, "extracted_length": len(extracted_text)}
                ).model_dump()

            logger.info(f"Extracted {len(extracted_text)} characters from DOCX")

            # 2. Validate format ƒë·ªÅ thi tr∆∞·ªõc khi g·ªçi LLM
            logger.info("Validating exam format...")
            format_validation = self._validate_exam_format(extracted_text)

            if not format_validation["is_valid"]:
                return ExamImportError(
                    message="Invalid exam format",
                    error=f"ƒê·ªÅ thi kh√¥ng ƒë√∫ng format chu·∫©n: {format_validation['error']}",
                    error_code="INVALID_FORMAT",
                    details={
                        "filename": filename,
                        "validation_details": format_validation["details"]
                    }
                ).model_dump()

            logger.info("Exam format validation passed")

            # 3. G·ª≠i cho LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi
            logger.info("Sending content to LLM for analysis...")
            llm_result = await self._analyze_exam_with_llm(extracted_text, filename)

            if not llm_result.get("success", False):
                return ExamImportError(
                    message="LLM analysis failed",
                    error=f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi: {llm_result.get('error', 'Unknown error')}",
                    error_code="LLM_ANALYSIS_FAILED",
                    details={"filename": filename}
                ).model_dump()

            # 3. Parse JSON response t·ª´ LLM
            exam_data = llm_result.get("data")
            if not exam_data:
                return ExamImportError(
                    message="No exam data returned",
                    error="LLM kh√¥ng tr·∫£ v·ªÅ d·ªØ li·ªáu ƒë·ªÅ thi",
                    error_code="NO_EXAM_DATA",
                    details={"filename": filename}
                ).model_dump()

            # 4. Validate v√† t·∫°o response
            processing_time = time.time() - start_time
            
            # T√≠nh to√°n statistics
            statistics = self._calculate_import_statistics(exam_data)
            
            return ExamImportResponse(
                success=True,
                message="ƒê·ªÅ thi ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng",
                data=ImportedExamData(**exam_data),
                processing_time=processing_time
            ).model_dump()

        except Exception as e:
            logger.error(f"Error importing exam from DOCX: {e}")
            processing_time = time.time() - start_time
            
            return ExamImportError(
                message="Import failed",
                error=f"L·ªói trong qu√° tr√¨nh import: {str(e)}",
                error_code="IMPORT_ERROR",
                details={
                    "filename": filename,
                    "processing_time": processing_time,
                    "error_type": type(e).__name__
                }
            ).model_dump()

    def _extract_text_from_docx_bytes(self, file_content: bytes) -> str:
        """
        Tr√≠ch xu·∫•t text t·ª´ file DOCX bytes

        Args:
            file_content: N·ªôi dung file DOCX d∆∞·ªõi d·∫°ng bytes

        Returns:
            Text ƒë√£ tr√≠ch xu·∫•t
        """
        try:
            # T·∫°o file-like object t·ª´ bytes
            file_stream = io.BytesIO(file_content)
            
            # ƒê·ªçc document
            doc = Document(file_stream)
            
            # Tr√≠ch xu·∫•t text t·ª´ t·∫•t c·∫£ paragraphs
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            # Tr√≠ch xu·∫•t text t·ª´ tables
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        full_text.append(" | ".join(row_text))
            
            extracted_text = "\n".join(full_text)
            logger.info(f"Successfully extracted {len(extracted_text)} characters from DOCX")
            
            return extracted_text

        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    def _validate_exam_format(self, exam_text: str) -> Dict[str, Any]:
        """
        Validate format ƒë·ªÅ thi c∆° b·∫£n - ch·ªâ ki·ªÉm tra c·∫•u tr√∫c 3 ph·∫ßn v√† ƒë√°p √°n

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation
        """
        try:
            # Normalize text ƒë·ªÉ d·ªÖ ki·ªÉm tra
            normalized_text = exam_text.upper().replace('\n', ' ').replace('\r', '')

            validation_result = {
                "is_valid": True,
                "error": "",
                "details": {}
            }

            # 1. Ki·ªÉm tra c·∫•u tr√∫c 3 ph·∫ßn
            required_parts = ["PH·∫¶N I", "PH·∫¶N II", "PH·∫¶N III"]
            missing_parts = []

            for part in required_parts:
                if part not in normalized_text:
                    missing_parts.append(part)

            if missing_parts:
                validation_result["is_valid"] = False
                validation_result["error"] = f"Thi·∫øu c·∫•u tr√∫c ph·∫ßn: {', '.join(missing_parts)}"
                validation_result["details"]["missing_parts"] = missing_parts
                return validation_result

            # 2. Ki·ªÉm tra ph·∫ßn ƒë√°p √°n
            if "ƒê√ÅP √ÅN" not in normalized_text:
                validation_result["is_valid"] = False
                validation_result["error"] = "Thi·∫øu ph·∫ßn ƒë√°p √°n"
                validation_result["details"]["missing_answer_section"] = True
                return validation_result

            logger.info("Exam format validation passed successfully")
            return validation_result

        except Exception as e:
            logger.error(f"Error in format validation: {e}")
            return {
                "is_valid": False,
                "error": f"L·ªói trong qu√° tr√¨nh validate format: {str(e)}",
                "details": {"exception": str(e)}
            }

    async def _analyze_exam_with_llm(self, exam_text: str, filename: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi ƒë·ªÅ thi th√†nh JSON

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi ƒë√£ extract
            filename: T√™n file g·ªëc

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        try:
            # T·∫°o prompt cho LLM
            prompt = self._create_analysis_prompt(exam_text, filename)
            
            # G·ªçi LLM
            openrouter_service = get_openrouter_service()
            response = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=8000
            )

            if not response.get("success", False):
                return {
                    "success": False,
                    "error": f"LLM request failed: {response.get('error', 'Unknown error')}"
                }

            # Parse JSON t·ª´ response
            llm_content = response.get("text", "") or response.get("content", "")
            logger.info(f"LLM response length: {len(llm_content)}")
            logger.info(f"LLM response preview: {llm_content[:500]}...")

            # T√¨m JSON trong response
            json_data = self._extract_json_from_response(llm_content)
            
            if not json_data:
                return {
                    "success": False,
                    "error": "Kh√¥ng th·ªÉ t√¨m th·∫•y JSON h·ª£p l·ªá trong response c·ªßa LLM"
                }

            return {
                "success": True,
                "data": json_data
            }

        except Exception as e:
            logger.error(f"Error in LLM analysis: {e}")
            return {
                "success": False,
                "error": f"LLM analysis error: {str(e)}"
            }

    def _create_analysis_prompt(self, exam_text: str, filename: str) -> str:
        """
        T·∫°o prompt cho LLM ƒë·ªÉ ph√¢n t√≠ch ƒë·ªÅ thi

        Args:
            exam_text: N·ªôi dung ƒë·ªÅ thi
            filename: T√™n file

        Returns:
            Prompt string
        """
        prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch ƒë·ªÅ thi. H√£y ph√¢n t√≠ch n·ªôi dung ƒë·ªÅ thi sau v√† chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng JSON ch√≠nh x√°c.

N·ªòI DUNG ƒê·ªÄ THI:
{exam_text}

Y√äU C·∫¶U:
1. Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t th√¥ng tin ƒë·ªÅ thi th√†nh JSON v·ªõi c·∫•u tr√∫c ch√≠nh x√°c nh∆∞ m·∫´u
2. X√°c ƒë·ªãnh m√¥n h·ªçc, l·ªõp, th·ªùi gian l√†m b√†i, t√™n tr∆∞·ªùng
3. Ph√¢n chia c√¢u h·ªèi th√†nh 3 ph·∫ßn:
   - Ph·∫ßn I: Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn (A, B, C, D)
   - Ph·∫ßn II: Tr·∫Øc nghi·ªám ƒë√∫ng/sai (a, b, c, d v·ªõi true/false)
   - Ph·∫ßn III: Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn (ch·ªâ s·ªë)
4. Tr√≠ch xu·∫•t ƒë√°p √°n ch√≠nh x√°c t·ª´ ph·∫ßn ƒë√°p √°n
5. N·∫øu l√† m√¥n H√≥a h·ªçc, tr√≠ch xu·∫•t b·∫£ng nguy√™n t·ª≠ kh·ªëi

ƒê·ªäNH D·∫†NG JSON MONG MU·ªêN:
{{
  "subject": "H√≥a h·ªçc",
  "grade": 12,
  "duration_minutes": 90,
  "school": "Tr∆∞·ªùng THPT Hong Thinh",
  "exam_code": "1234",
  "atomic_masses": "H = 1; C = 12; N = 14; O = 16",
  "parts": [
    {{
      "part": "Ph·∫ßn I",
      "title": "Tr·∫Øc nghi·ªám nhi·ªÅu ph∆∞∆°ng √°n l·ª±a ch·ªçn",
      "description": "M·ªói c√¢u tr·∫£ l·ªùi ƒë√∫ng th√≠ sinh ƒë∆∞·ª£c 0,25 ƒëi·ªÉm",
      "questions": [
        {{
          "id": 1,
          "question": "N·ªôi dung c√¢u h·ªèi...",
          "options": {{
            "A": "ƒê√°p √°n A",
            "B": "ƒê√°p √°n B", 
            "C": "ƒê√°p √°n C",
            "D": "ƒê√°p √°n D"
          }},
          "answer": "C"
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn II",
      "title": "Tr·∫Øc nghi·ªám ƒë√∫ng/sai",
      "description": "Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u X. M·ªói c√¢u c√≥ 4 ph√°t bi·ªÉu a), b), c), d)",
      "questions": [
        {{
          "id": 1,
          "question": "N·ªôi dung c√¢u h·ªèi...",
          "statements": {{
            "a": {{
              "text": "Ph√°t bi·ªÉu a",
              "answer": true
            }},
            "b": {{
              "text": "Ph√°t bi·ªÉu b", 
              "answer": false
            }},
            "c": {{
              "text": "Ph√°t bi·ªÉu c",
              "answer": true
            }},
            "d": {{
              "text": "Ph√°t bi·ªÉu d",
              "answer": false
            }}
          }}
        }}
      ]
    }},
    {{
      "part": "Ph·∫ßn III",
      "title": "Tr·∫Øc nghi·ªám tr·∫£ l·ªùi ng·∫Øn",
      "description": "Ch·ªâ ghi s·ªë, kh√¥ng ghi ƒë∆°n v·ªã. T·ªëi ƒëa 4 k√Ω t·ª±.",
      "questions": [
        {{
          "id": 1,
          "question": "N·ªôi dung c√¢u h·ªèi...",
          "answer": "1"
        }}
      ]
    }}
  ]
}}

L∆∞U √ù QUAN TR·ªåNG:
- Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng th√™m text gi·∫£i th√≠ch
- ƒê·∫£m b·∫£o t·∫•t c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n ƒë∆∞·ª£c tr√≠ch xu·∫•t ch√≠nh x√°c
- V·ªõi c√¢u ƒë√∫ng/sai, x√°c ƒë·ªãnh ch√≠nh x√°c true/false cho t·ª´ng ph√°t bi·ªÉu
- V·ªõi c√¢u tr·∫£ l·ªùi ng·∫Øn, ch·ªâ l·∫•y s·ªë, t·ªëi ƒëa 4 k√Ω t·ª±
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin n√†o, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh h·ª£p l√Ω

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON:
"""
        return prompt

    def _extract_json_from_response(self, response_content: str) -> Optional[Dict[str, Any]]:
        """
        Tr√≠ch xu·∫•t JSON t·ª´ response c·ªßa LLM

        Args:
            response_content: N·ªôi dung response t·ª´ LLM

        Returns:
            Dict ch·ª©a JSON data ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            logger.info(f"Extracting JSON from response: {response_content[:200]}...")

            # 1. T√¨m JSON trong markdown code blocks
            markdown_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            markdown_matches = re.findall(markdown_pattern, response_content, re.DOTALL)

            for match in markdown_matches:
                try:
                    json_data = json.loads(match.strip())
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted JSON from markdown code block")
                        return json_data
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON from markdown: {e}")
                    continue

            # 2. T√¨m JSON trong response (c√≥ th·ªÉ c√≥ text bao quanh)
            json_pattern = r'\{.*\}'
            matches = re.findall(json_pattern, response_content, re.DOTALL)

            for match in matches:
                try:
                    # Th·ª≠ parse JSON
                    json_data = json.loads(match)

                    # Validate c·∫•u tr√∫c c∆° b·∫£n
                    if self._validate_exam_json_structure(json_data):
                        logger.info("Successfully extracted and validated JSON from LLM response")
                        return json_data

                except json.JSONDecodeError:
                    continue

            # 3. N·∫øu kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá, th·ª≠ parse to√†n b·ªô response
            try:
                json_data = json.loads(response_content.strip())
                if self._validate_exam_json_structure(json_data):
                    logger.info("Successfully parsed entire response as JSON")
                    return json_data
            except json.JSONDecodeError:
                pass

            logger.error("Could not extract valid JSON from LLM response")
            logger.error(f"Response content: {response_content}")
            return None

        except Exception as e:
            logger.error(f"Error extracting JSON from response: {e}")
            return None

    def _validate_exam_json_structure(self, json_data: Dict[str, Any]) -> bool:
        """
        Validate c·∫•u tr√∫c JSON c·ªßa ƒë·ªÅ thi

        Args:
            json_data: JSON data c·∫ßn validate

        Returns:
            True n·∫øu c·∫•u tr√∫c h·ª£p l·ªá
        """
        try:
            required_fields = ["subject", "grade", "duration_minutes", "school", "parts"]
            
            # Ki·ªÉm tra c√°c field b·∫Øt bu·ªôc
            for field in required_fields:
                if field not in json_data:
                    logger.error(f"Missing required field: {field}")
                    return False
            
            # Ki·ªÉm tra parts
            parts = json_data.get("parts", [])
            if not isinstance(parts, list) or len(parts) == 0:
                logger.error("Parts must be a non-empty list")
                return False
            
            # Ki·ªÉm tra c·∫•u tr√∫c c·ªßa t·ª´ng part
            for part in parts:
                if not isinstance(part, dict):
                    return False
                
                part_required = ["part", "title", "description", "questions"]
                for field in part_required:
                    if field not in part:
                        logger.error(f"Missing required field in part: {field}")
                        return False
                
                # Ki·ªÉm tra questions
                questions = part.get("questions", [])
                if not isinstance(questions, list):
                    return False
            
            logger.info("JSON structure validation passed")
            return True

        except Exception as e:
            logger.error(f"Error validating JSON structure: {e}")
            return False

    def _calculate_import_statistics(self, exam_data: Dict[str, Any]) -> ExamImportStatistics:
        """
        T√≠nh to√°n th·ªëng k√™ cho ƒë·ªÅ thi ƒë√£ import

        Args:
            exam_data: D·ªØ li·ªáu ƒë·ªÅ thi

        Returns:
            ExamImportStatistics object
        """
        try:
            parts = exam_data.get("parts", [])
            
            total_questions = 0
            part_1_questions = 0
            part_2_questions = 0
            part_3_questions = 0
            
            for part in parts:
                questions = part.get("questions", [])
                part_name = part.get("part", "").lower()
                
                if "i" in part_name or "1" in part_name:
                    part_1_questions = len(questions)
                elif "ii" in part_name or "2" in part_name:
                    part_2_questions = len(questions)
                elif "iii" in part_name or "3" in part_name:
                    part_3_questions = len(questions)
                
                total_questions += len(questions)
            
            has_atomic_masses = bool(exam_data.get("atomic_masses"))
            
            # T√≠nh ch·∫•t l∆∞·ª£ng x·ª≠ l√Ω (d·ª±a tr√™n s·ªë c√¢u h·ªèi v√† c·∫•u tr√∫c)
            processing_quality = min(1.0, (total_questions / 20) * 0.8 + 0.2)
            
            return ExamImportStatistics(
                total_questions=total_questions,
                part_1_questions=part_1_questions,
                part_2_questions=part_2_questions,
                part_3_questions=part_3_questions,
                has_atomic_masses=has_atomic_masses,
                processing_quality=processing_quality
            )

        except Exception as e:
            logger.error(f"Error calculating import statistics: {e}")
            return ExamImportStatistics(
                total_questions=0,
                part_1_questions=0,
                part_2_questions=0,
                part_3_questions=0,
                has_atomic_masses=False,
                processing_quality=0.0
            )


# Lazy loading global instance ƒë·ªÉ tr√°nh kh·ªüi t·∫°o ngay khi import
_exam_import_service_instance = None

def get_exam_import_service() -> ExamImportService:
    """
    L·∫•y singleton instance c·ªßa ExamImportService
    Lazy initialization

    Returns:
        ExamImportService: Service instance
    """
    global _exam_import_service_instance
    if _exam_import_service_instance is None:
        _exam_import_service_instance = ExamImportService()
    return _exam_import_service_instance

# Backward compatibility - deprecated, s·ª≠ d·ª•ng get_exam_import_service() thay th·∫ø
# Lazy loading ƒë·ªÉ tr√°nh kh·ªüi t·∫°o ngay khi import
def _get_exam_import_service_lazy():
    """Lazy loading cho backward compatibility"""
    return get_exam_import_service()

# T·∫°o proxy object ƒë·ªÉ lazy loading
class _ExamImportServiceProxy:
    def __getattr__(self, name):
        return getattr(_get_exam_import_service_lazy(), name)

    def __call__(self, *args, **kwargs):
        return _get_exam_import_service_lazy()(*args, **kwargs)

exam_import_service = _ExamImportServiceProxy()
