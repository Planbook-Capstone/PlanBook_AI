"""
Enhanced Textbook Service - Cáº£i tiáº¿n xá»­ lÃ½ sÃ¡ch giÃ¡o khoa vá»›i OCR vÃ  LLM
Tráº£ vá» cáº¥u trÃºc: SÃ¡ch â†’ ChÆ°Æ¡ng â†’ BÃ i â†’ Ná»™i dung
"""

import logging
import asyncio
import re
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import fitz  # PyMuPDF
from PIL import Image
import io

from app.services.simple_ocr_service import simple_ocr_service

logger = logging.getLogger(__name__)


class EnhancedTextbookService:
    """Service cáº£i tiáº¿n Ä‘á»ƒ xá»­ lÃ½ sÃ¡ch giÃ¡o khoa vá»›i OCR vÃ  LLM"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def process_textbook_to_structure(
        self,
        pdf_content: bytes,
        filename: str,
        book_metadata: Optional[Dict[str, Any]] = None,
        lesson_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ PDF sÃ¡ch giÃ¡o khoa vÃ  tráº£ vá» cáº¥u trÃºc hoÃ n chá»‰nh

        Args:
            pdf_content: Ná»™i dung PDF
            filename: TÃªn file
            book_metadata: Metadata sÃ¡ch (title, subject, grade, etc.)
            lesson_id: ID bÃ i há»c tÃ¹y chá»n Ä‘á»ƒ liÃªn káº¿t vá»›i lesson cá»¥ thá»ƒ

        Returns:
            Dict vá»›i cáº¥u trÃºc: book -> chapters -> lessons -> content
        """
        try:
            logger.info(f"ğŸš€ Starting enhanced textbook processing: {filename}")

            # Step 1: Extract all pages with OCR
            logger.info("ğŸ“„ Extracting pages with OCR...")
            pages_data = await self._extract_pages_with_ocr(pdf_content)
            logger.info(f"âœ… Extracted {len(pages_data)} pages")

            # Skip image analysis to improve speed
            logger.info("âš¡ Skipping image analysis for faster processing")

            # Step 2: Combine all text from pages
            logger.info("ğŸ“ Combining text from all pages...")
            full_text = ""
            all_page_numbers = []
            for page in pages_data:
                full_text += page.get("text", "") + "\n"
                all_page_numbers.append(page.get("page_number", 0))

            logger.info(f"ğŸ“„ ALl Data {len(pages_data)} pages")
            logger.info(f"ğŸ“„ Combined text from {len(pages_data)} pages")

            # Step 3: Refine content directly with OpenRouter LLM
            logger.info("ğŸ¤– Refining content with OpenRouter LLM...")
            refined_content = await self.refine_raw_content_with_llm(full_text)
            logger.info("âœ… Content refinement completed")
            logger.info("âœ… Content refinement {refined_content}")
            # Step 4: Return clean text content directly
            logger.info("âœ… Content processing completed")

            # Skip image extraction for faster processing
            images_data = []
            logger.info("âš¡ Skipping image extraction for faster processing")

            return {
                "success": True,
                "clean_book_structure": refined_content,  # Return clean text directly
                "images_data": images_data,  # Empty array
                "total_pages": len(pages_data),
                "message": f"Textbook processed successfully with LLM content refinement",
            }

        except Exception as e:
            logger.error(f"âŒ Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "Failed to process textbook",
            }

    async def _extract_pages_with_ocr(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract táº¥t cáº£ pages vá»›i OCR náº¿u cáº§n"""

        def extract_page_data():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text normally first
                text = page.get_text("text")  # type: ignore

                # Skip image extraction for faster processing
                images = []

                pages_data.append(
                    {
                        "page_number": page_num + 1,
                        "text": text,
                        "images": images,
                        "has_text": len(text.strip()) > 50,
                    }
                )

            doc.close()
            return pages_data

        # Extract pages in background thread
        pages_data = await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_page_data
        )

        # Apply OCR to pages with insufficient text
        ocr_tasks = []
        for page in pages_data:
            if not page["has_text"]:
                ocr_tasks.append(self._apply_ocr_to_page(page, pdf_content))

        if ocr_tasks:
            logger.info(
                f"ğŸ” Applying OCR to {len(ocr_tasks)} pages with insufficient text"
            )
            ocr_results = await asyncio.gather(*ocr_tasks, return_exceptions=True)

            # Update pages with OCR results
            ocr_index = 0
            for page in pages_data:
                if not page["has_text"] and ocr_index < len(ocr_results):
                    if not isinstance(ocr_results[ocr_index], Exception):
                        page["text"] = ocr_results[ocr_index]
                        page["ocr_applied"] = True
                    ocr_index += 1

        return pages_data

    async def _apply_ocr_to_page(
        self, page_data: Dict[str, Any], pdf_content: bytes
    ) -> str:
        """Apply OCR to a specific page"""
        try:
            # Use existing OCR service for this page
            page_num = page_data["page_number"]

            # Extract just this page as bytes
            def extract_single_page():
                doc = fitz.open(stream=pdf_content, filetype="pdf")
                page = doc[page_num - 1]

                # Convert page to image
                mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better OCR
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                doc.close()

                return img_data

            img_data = await asyncio.get_event_loop().run_in_executor(
                self.executor, extract_single_page
            )

            # Apply OCR using SimpleOCRService
            image = Image.open(io.BytesIO(img_data))

            # Use SimpleOCRService's _ocr_image method which handles EasyOCR initialization
            text = await simple_ocr_service._ocr_image(image, page_data['page_number'])
            return text

        except Exception as e:
            logger.error(f"OCR failed for page {page_data['page_number']}: {e}")
            return ""





















    def clean_text_content(self, text: str) -> str:
        """LÃ m sáº¡ch ná»™i dung text - loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, format khÃ´ng cáº§n thiáº¿t"""
        if not text:
            return ""

        logger.info("ğŸ§¹ Cleaning text content...")

        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  format markdown
        cleaned_text = text

        # Loáº¡i bá» dáº¥u * (markdown bold/italic)
        cleaned_text = re.sub(r'\*+', '', cleaned_text)

        # Loáº¡i bá» dáº¥u # (markdown headers)
        cleaned_text = re.sub(r'#+\s*', '', cleaned_text)

        # Loáº¡i bá» dáº¥u _ (markdown underline)
        cleaned_text = re.sub(r'_+', '', cleaned_text)

        # Loáº¡i bá» dáº¥u ` (markdown code)
        cleaned_text = re.sub(r'`+', '', cleaned_text)

        # Loáº¡i bá» dáº¥u [] vÃ  () (markdown links)
        cleaned_text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', cleaned_text)

        # Loáº¡i bá» kÃ½ tá»± \b (backspace)
        cleaned_text = cleaned_text.replace('\b', '')

        # Loáº¡i bá» kÃ½ tá»± \r
        cleaned_text = cleaned_text.replace('\r', '')

        # Thay tháº¿ nhiá»u dáº¥u xuá»‘ng dÃ²ng liÃªn tiáº¿p báº±ng 1 dáº¥u xuá»‘ng dÃ²ng
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)

        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a á»Ÿ Ä‘áº§u vÃ  cuá»‘i má»—i dÃ²ng
        lines = cleaned_text.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip()]

        # GhÃ©p láº¡i thÃ nh má»™t Ä‘oáº¡n text liÃªn tá»¥c
        final_text = ' '.join(cleaned_lines)

        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        final_text = re.sub(r'\s+', ' ', final_text).strip()

        logger.info(f"ğŸ§¹ Text cleaned: {len(text)} â†’ {len(final_text)} chars")
        return final_text

    async def refine_raw_content_with_llm(self, raw_text: str) -> str:
        """Gá»­i text thÃ´ trá»±c tiáº¿p Ä‘áº¿n OpenRouter LLM Ä‘á»ƒ lá»c vÃ  chá»‰nh sá»­a ná»™i dung"""
        try:
            from app.services.openrouter_service import get_openrouter_service

            openrouter_service = get_openrouter_service()
            if not openrouter_service.available:
                logger.warning("OpenRouter service not available, returning original content")
                return self.clean_text_content(raw_text)

            logger.info("ğŸ¤– Sending raw content to OpenRouter for refinement...")

            prompt = f"""
Báº¡n lÃ  chuyÃªn gia giÃ¡o dá»¥c, hÃ£y lá»c vÃ  chá»‰nh sá»­a ná»™i dung sÃ¡ch giÃ¡o khoa sau Ä‘Ã¢y.

YÃŠU Cáº¦U:
1. Giá»¯ láº¡i toÃ n bá»™ ná»™i dung giÃ¡o dá»¥c quan trá»ng (khÃ¡i niá»‡m, Ä‘á»‹nh nghÄ©a, cÃ´ng thá»©c, vÃ­ dá»¥)
2. Loáº¡i bá» cÃ¡c thÃ´ng tin khÃ´ng liÃªn quan (header, footer, sá»‘ trang, watermark)
3. Sáº¯p xáº¿p láº¡i ná»™i dung theo logic rÃµ rÃ ng
4. Giá»¯ nguyÃªn thuáº­t ngá»¯ chuyÃªn mÃ´n
5. Äáº£m báº£o ná»™i dung Ä‘áº§y Ä‘á»§ vÃ  dá»… hiá»ƒu cho há»c sinh
6. Tráº£ vá» CHá»ˆ Ná»˜I DUNG THUáº¦N TÃšY, khÃ´ng cÃ³ format markdown, khÃ´ng cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t

Ná»˜I DUNG Cáº¦N CHá»ˆNH Sá»¬A:
{raw_text[:8000]}

Tráº£ vá» ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c lá»c vÃ  chá»‰nh sá»­a (chá»‰ text thuáº§n tÃºy):"""

            result = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=2048
            )

            if result.get("success") and result.get("text"):
                refined_content = result.get("text", "").strip()
                # LÃ m sáº¡ch ná»™i dung sau khi nháº­n tá»« OpenRouter
                clean_content = self.clean_text_content(refined_content)
                logger.info(f"ğŸ¤–{prompt} ")
                logger.info(f"ğŸ¤–{refined_content} ")
                logger.info(f"âœ… Content refined and cleaned: {len(raw_text)} â†’ {len(clean_content)} chars")
                return clean_content
            else:
                logger.warning("OpenRouter returned insufficient content, using original")
                return self.clean_text_content(raw_text)

        except Exception as e:
            logger.error(f"âŒ Error refining content with OpenRouter: {e}")
            return self.clean_text_content(raw_text)




# Global instance
enhanced_textbook_service = EnhancedTextbookService()
