"""
Enhanced Textbook Service - C·∫£i ti·∫øn x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM
Tr·∫£ v·ªÅ c·∫•u tr√∫c: S√°ch ‚Üí Ch∆∞∆°ng ‚Üí B√†i ‚Üí N·ªôi dung
"""

import logging
import asyncio
import re
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import fitz  # PyMuPDF
from PIL import Image
import io

from app.services.simple_ocr_service import get_simple_ocr_service

logger = logging.getLogger(__name__)


class EnhancedTextbookService:
    """Service c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.ocr_service = get_simple_ocr_service()

    async def process_textbook_to_structure(
        self,
        pdf_content: bytes,
        filename: str,
        book_metadata: Optional[Dict[str, Any]] = None,
        lesson_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω PDF s√°ch gi√°o khoa v√† tr·∫£ v·ªÅ c·∫•u tr√∫c ho√†n ch·ªânh

        Args:
            pdf_content: N·ªôi dung PDF
            filename: T√™n file
            book_metadata: Metadata s√°ch (title, subject, grade, etc.)
            lesson_id: ID b√†i h·ªçc t√πy ch·ªçn ƒë·ªÉ li√™n k·∫øt v·ªõi lesson c·ª• th·ªÉ

        Returns:
            Dict v·ªõi c·∫•u tr√∫c: book -> chapters -> lessons -> content
        """
        try:
            logger.info(f"üöÄ Starting enhanced textbook processing: {filename}")

            # Step 1: Extract all pages with OCR
            logger.info("üìÑ ƒêang tr√≠ch xu·∫•t trang v·ªõi OCR...")
            pages_data = await self._extract_pages_with_ocr(pdf_content)
            logger.info(f"‚úÖ Extracted {len(pages_data)} pages")

            # Skip image analysis to improve speed
            logger.info("‚ö° B·ªè qua ph√¢n t√≠ch h√¨nh ·∫£nh ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n")

            # Step 2: Combine all text from pages
            logger.info("üìù ƒêang k·∫øt h·ª£p vƒÉn b·∫£n t·ª´ t·∫•t c·∫£ c√°c trang...")
            full_text = ""
            all_page_numbers = []
            for page in pages_data:
                full_text += page.get("text", "") + "\n"
                all_page_numbers.append(page.get("page_number", 0))

            logger.info(f"üìÑ ALl Data {len(pages_data)} pages")
            logger.info(f"üìÑ Combined text from {len(pages_data)} pages")

            # Step 3: Refine content directly with OpenRouter LLM
            logger.info("ü§ñ ƒêang tinh ch·ªânh n·ªôi dung v·ªõi OpenRouter LLM...")
            refined_content = await self.refine_raw_content_with_llm(full_text)
            logger.info("‚úÖ Ho√†n th√†nh tinh ch·ªânh n·ªôi dung")
            logger.info("‚úÖ Content refinement {refined_content}")
            # Step 4: Return clean text content directly
            logger.info("‚úÖ Ho√†n th√†nh x·ª≠ l√Ω n·ªôi dung")

            # Skip image extraction for faster processing
            images_data = []
            logger.info("‚ö° B·ªè qua tr√≠ch xu·∫•t h√¨nh ·∫£nh ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n")

            return {
                "success": True,
                "clean_book_structure": refined_content,  # Return clean text directly
                "images_data": images_data,  # Empty array
                "total_pages": len(pages_data),
                "message": f"X·ª≠ l√Ω s√°ch gi√°o khoa th√†nh c√¥ng v·ªõi tinh ch·ªânh n·ªôi dung LLM",
            }

        except Exception as e:
            logger.error(f"‚ùå Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "Failed to process textbook",
            }

    async def _extract_pages_with_ocr(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract t·∫•t c·∫£ pages v·ªõi OCR n·∫øu c·∫ßn"""

        def extract_page_data():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text normally first
                text = page.get_text("text")  # type: ignore

                # Skip image extraction for faster processing
                images = []

                pages_data.append(
                    {
                        "page_number": page_num + 1,
                        "text": text,
                        "images": images,
                        "has_text": len(text.strip()) > 50,
                    }
                )

            doc.close()
            return pages_data

        # Extract pages in background thread
        pages_data = await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_page_data
        )

        # Apply OCR to pages with insufficient text
        ocr_tasks = []
        for page in pages_data:
            if not page["has_text"]:
                ocr_tasks.append(self._apply_ocr_to_page(page, pdf_content))

        if ocr_tasks:
            logger.info(
                f"üîç ƒêang √°p d·ª•ng OCR cho {len(ocr_tasks)} trang c√≥ vƒÉn b·∫£n kh√¥ng ƒë·ªß"
            )
            ocr_results = await asyncio.gather(*ocr_tasks, return_exceptions=True)

            # Update pages with OCR results
            ocr_index = 0
            for page in pages_data:
                if not page["has_text"] and ocr_index < len(ocr_results):
                    if not isinstance(ocr_results[ocr_index], Exception):
                        page["text"] = ocr_results[ocr_index]
                        page["ocr_applied"] = True
                    ocr_index += 1

        return pages_data

    async def _apply_ocr_to_page(
        self, page_data: Dict[str, Any], pdf_content: bytes
    ) -> str:
        """Apply OCR to a specific page"""
        try:
            # Use existing OCR service for this page
            page_num = page_data["page_number"]

            # Extract just this page as bytes
            def extract_single_page():
                doc = fitz.open(stream=pdf_content, filetype="pdf")
                page = doc[page_num - 1]

                # Convert page to image
                mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better OCR
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                doc.close()

                return img_data

            img_data = await asyncio.get_event_loop().run_in_executor(
                self.executor, extract_single_page
            )

            # Apply OCR using SimpleOCRService
            image = Image.open(io.BytesIO(img_data))

            # Use SimpleOCRService's _ocr_image method which handles EasyOCR initialization
            text = await self.ocr_service._ocr_image(image, page_data['page_number'])
            return text

        except Exception as e:
            logger.error(f"OCR failed for page {page_data['page_number']}: {e}")
            return ""





















    def clean_text_content(self, text: str) -> str:
        """L√†m s·∫°ch n·ªôi dung text - format ƒë∆°n gi·∫£n, d·ªÖ ƒë·ªçc"""
        if not text:
            return ""

        logger.info("üßπ Cleaning text content...")

        # Lo·∫°i b·ªè markdown v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        cleaned_text = text.strip()
        cleaned_text = re.sub(r'\*+', '', cleaned_text)
        cleaned_text = re.sub(r'#+\s*', '', cleaned_text)
        cleaned_text = re.sub(r'_+', '', cleaned_text)
        cleaned_text = re.sub(r'`+', '', cleaned_text)
        cleaned_text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', cleaned_text)
        cleaned_text = cleaned_text.replace('\b', '').replace('\r', '')

        # Lo·∫°i b·ªè c√°c tham chi·∫øu SGK v√† h∆∞·ªõng d·∫´n tham kh·∫£o
        sgk_patterns = [
            r'SGK\s+trang\s+\d+',  # SGK trang X
            r'Xem\s+b·∫£ng\s+\d+\.\d+\s*\([^)]*SGK[^)]*\)',  # Xem b·∫£ng X.X (SGK trang Y)
            r'Quan\s+s√°t\s+h√¨nh\s+\d+\.\d+\s*\([^)]*SGK[^)]*\)',  # Quan s√°t h√¨nh X.X (SGK trang Y)
            r'T√¨m\s+hi·ªÉu\s+v·ªÅ[^.]*\([^)]*SGK[^)]*\)',  # T√¨m hi·ªÉu v·ªÅ... (SGK trang Y)
            r'Xem\s+b·∫£ng\s+\d+\.\d+[^.]*',  # Xem b·∫£ng X.X v·ªÅ...
            r'Quan\s+s√°t\s+h√¨nh\s+\d+\.\d+[^.]*',  # Quan s√°t h√¨nh X.X v·ªÅ...
            r'\([^)]*SGK\s+trang[^)]*\)',  # (SGK trang X)
            r'- Xem b·∫£ng[^.]*\.',  # - Xem b·∫£ng...
            r'- Quan s√°t[^.]*\.',  # - Quan s√°t...
            r'- T√¨m hi·ªÉu[^.]*\.',  # - T√¨m hi·ªÉu...
        ]

        for pattern in sgk_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)

        # Lo·∫°i b·ªè c√°c d√≤ng ch·ªâ ch·ª©a tham chi·∫øu ho·∫∑c h∆∞·ªõng d·∫´n
        lines = cleaned_text.split('\n')
        filtered_lines = []
        for line in lines:
            line_stripped = line.strip()
            # B·ªè qua c√°c d√≤ng ch·ªâ ch·ª©a tham chi·∫øu
            if (line_stripped.lower().startswith(('xem ', 'quan s√°t ', 't√¨m hi·ªÉu ')) and
                len(line_stripped) < 100):  # C√°c c√¢u h∆∞·ªõng d·∫´n th∆∞·ªùng ng·∫Øn
                continue
            if line_stripped:  # Ch·ªâ gi·ªØ c√°c d√≤ng c√≥ n·ªôi dung
                filtered_lines.append(line)
        cleaned_text = '\n'.join(filtered_lines)

        # Lo·∫°i b·ªè b·∫£ng markdown (x·ª≠ l√Ω sau khi ƒë√£ l·ªçc tham chi·∫øu SGK)
        lines = cleaned_text.split('\n')
        final_filtered_lines = []
        for line in lines:
            line_stripped = line.strip()
            if (line_stripped.count('|') >= 2 or
                (line_stripped.count('-') >= 3 and '|' in line_stripped)):
                continue
            final_filtered_lines.append(line)
        cleaned_text = '\n'.join(final_filtered_lines)

        # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng h√≥a h·ªçc
        cleaned_text = self._normalize_chemistry_format(cleaned_text)

        # Thay th·∫ø xu·ªëng d√≤ng b·∫±ng <br/>
        cleaned_text = cleaned_text.replace('\n', '<br/>')

        # FORMAT M·ªöI: ƒê∆°n gi·∫£n v√† d·ªÖ ƒë·ªçc
        # 1. ƒê·ªãnh nghƒ©a: Gi·ªØ nguy√™n tr√™n 1 d√≤ng
        cleaned_text = re.sub(r'ƒê·ªãnh nghƒ©a:\s*<br/>', 'ƒê·ªãnh nghƒ©a: ', cleaned_text)

        # 2. C√°c ti√™u ƒë·ªÅ ch√≠nh: In ƒë·∫≠m, kh√¥ng th·ª•t l·ªÅ
        main_sections = ['Bi·ªÉu hi·ªán', 'V√≠ d·ª•', '√ù nghƒ©a', 'Nh·∫≠n bi·∫øt', 'Tr√¨nh b√†y', 'V·∫≠n d·ª•ng', 'Ph√¢n t√≠ch', 'ƒê√°nh gi√°']
        for section in main_sections:
            cleaned_text = cleaned_text.replace(f'<br/>{section}:', f'<br/><strong>{section}:</strong>')
            if cleaned_text.startswith(f'{section}:'):
                cleaned_text = cleaned_text.replace(f'{section}:', f'<strong>{section}:</strong>', 1)

        # 3. C√°c m·ª•c con: D√πng bullet point (‚Ä¢) thay v√¨ th·ª•t l·ªÅ nhi·ªÅu
        lines = cleaned_text.split('<br/>')
        result_lines = []

        for line in lines:
            line = line.strip()
            if line:
                # Ch·ªâ th√™m bullet cho c√°c m·ª•c con (kh√¥ng ph·∫£i ti√™u ƒë·ªÅ)
                if (re.match(r'^[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê]', line)
                    and '<strong>' not in line
                    and not line.startswith('ƒê·ªãnh nghƒ©a:')):
                    result_lines.append('‚Ä¢ ' + line)
                else:
                    result_lines.append(line)

        cleaned_text = '<br/>'.join(result_lines)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† <br/> li√™n ti·∫øp
        cleaned_text = re.sub(r'[ \t]+', ' ', cleaned_text).strip()
        cleaned_text = re.sub(r'(<br/>){2,}', '<br/>', cleaned_text)

        logger.info(f"üßπ Text cleaned: {len(text)} ‚Üí {len(cleaned_text)} chars")
        return cleaned_text

    def _normalize_chemistry_format(self, text: str) -> str:
        """
        Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng h√≥a h·ªçc t·ª´ HTML sang ƒë·ªãnh d·∫°ng chu·∫©n
        VD: <sup>6</sup>Li -> ‚Å∂Li, S<sub>8</sub> -> S‚Çà, Fe<sup>2+</sup> -> Fe¬≤‚Å∫
        """
        if not text:
            return text

        # Chuy·ªÉn ƒë·ªïi superscript v·ªõi s·ªë v√† k√Ω hi·ªáu (ch·ªâ s·ªë tr√™n)
        # Ch·ªâ x·ª≠ l√Ω d·∫•u + v√† - khi ch√∫ng n·∫±m trong th·∫ª <sup>
        sup_pattern = r'<sup>([^<]+)</sup>'

        def replace_sup(match):
            content = match.group(1)
            result = ''
            superscript_map = {
                '0': '‚Å∞', '1': '¬π', '2': '¬≤', '3': '¬≥', '4': '‚Å¥',
                '5': '‚Åµ', '6': '‚Å∂', '7': '‚Å∑', '8': '‚Å∏', '9': '‚Åπ',
                '+': '‚Å∫', '-': '‚Åª'
            }
            for char in content:
                result += superscript_map.get(char, char)
            return result

        text = re.sub(sup_pattern, replace_sup, text)

        # Chuy·ªÉn ƒë·ªïi subscript (ch·ªâ s·ªë d∆∞·ªõi)
        sub_pattern = r'<sub>(\d+)</sub>'
        subscript_map = {
            '0': '‚ÇÄ', '1': '‚ÇÅ', '2': '‚ÇÇ', '3': '‚ÇÉ', '4': '‚ÇÑ',
            '5': '‚ÇÖ', '6': '‚ÇÜ', '7': '‚Çá', '8': '‚Çà', '9': '‚Çâ'
        }

        def replace_sub(match):
            number = match.group(1)
            return ''.join(subscript_map.get(digit, digit) for digit in number)

        text = re.sub(sub_pattern, replace_sub, text)

        # Chuy·ªÉn ƒë·ªïi c√°c c√¥ng th·ª©c h√≥a h·ªçc th√¥ (kh√¥ng c√≥ HTML tags)
        # Pattern: chuy·ªÉn s·ªë th√†nh subscript cho t·∫•t c·∫£ k√Ω hi·ªáu nguy√™n t·ªë
        # VD: CH3, H2O, C6H12O6, Ca(OH)2, Al2(SO4)3, C2(H2O)2
        # T·∫•t c·∫£ s·ªë sau d·∫•u ngo·∫∑c ƒë√≥ng ƒë·ªÅu chuy·ªÉn th√†nh subscript

        # Kh√¥ng c·∫ßn b·∫£o v·ªá g√¨ c·∫£ - t·∫•t c·∫£ s·ªë ƒë·ªÅu chuy·ªÉn th√†nh subscript
        protected_text = text

        # 2. Chuy·ªÉn ƒë·ªïi subscript cho t·∫•t c·∫£ k√Ω hi·ªáu nguy√™n t·ªë
        # Pattern 1: S·ªë ngay sau k√Ω hi·ªáu nguy√™n t·ªë (VD: H2, O2, Ca2)
        chemistry_pattern = r'([A-Z][a-z]?)(\d+)'

        # Pattern 2: S·ªë sau d·∫•u ngo·∫∑c ƒë√≥ng (VD: (OH)2, (SO4)3)
        parenthesis_pattern = r'\)(\d+)'

        def replace_chemistry(match):
            element = match.group(1)
            number = match.group(2)
            # Chuy·ªÉn s·ªë th√†nh subscript
            subscript_number = ''.join(subscript_map.get(digit, digit) for digit in number)
            return element + subscript_number

        def replace_parenthesis(match):
            number = match.group(1)
            # Chuy·ªÉn s·ªë th√†nh subscript
            subscript_number = ''.join(subscript_map.get(digit, digit) for digit in number)
            return ')' + subscript_number

        # √Åp d·ª•ng c·∫£ hai pattern
        protected_text = re.sub(chemistry_pattern, replace_chemistry, protected_text)
        text = re.sub(parenthesis_pattern, replace_parenthesis, protected_text)

        return text

    async def refine_raw_content_with_llm(self, raw_text: str) -> str:
        """G·ª≠i text th√¥ tr·ª±c ti·∫øp ƒë·∫øn OpenRouter LLM ƒë·ªÉ l·ªçc v√† ch·ªânh s·ª≠a n·ªôi dung"""
        try:
            from app.services.openrouter_service import get_openrouter_service

            openrouter_service = get_openrouter_service()
            logger.info("ü§ñ Sending raw content to OpenRouter for refinement...")

            prompt = f"""
B·∫°n l√† chuy√™n gia gi√°o d·ª•c, h√£y l·ªçc v√† c·∫•u tr√∫c l·∫°i n·ªôi dung s√°ch gi√°o khoa ƒë·ªÉ t·ªëi ∆∞u cho h·ªá th·ªëng chunking th√¥ng minh.

Y√äU C·∫¶U C·∫§U TR√öC:
1. ƒê·ªäNH NGHƒ®A: B·∫Øt ƒë·∫ßu b·∫±ng "ƒê·ªãnh nghƒ©a:" ho·∫∑c s·ª≠ d·ª•ng c·∫•u tr√∫c "X l√†..." r√µ r√†ng
2. B√ÄI T·∫¨P/V√ç D·ª§: ƒê√°nh s·ªë r√µ r√†ng "B√†i 1.", "V√≠ d·ª• 1:", "H√£y cho bi·∫øt..."
3. TI·ªÇU M·ª§C: S·ª≠ d·ª•ng "I.", "II.", "1.", "2." cho c√°c ph·∫ßn ch√≠nh

Y√äU C·∫¶U N·ªòI DUNG:
- Gi·ªØ l·∫°i to√†n b·ªô kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, c√¥ng th·ª©c, v√≠ d·ª• quan tr·ªçng
- Lo·∫°i b·ªè header, footer, s·ªë trang, watermark kh√¥ng li√™n quan
- LO·∫†I B·ªé HO√ÄN TO√ÄN t·∫•t c·∫£ c√°c b·∫£ng d·∫°ng markdown (| col1 | col2 |) v√¨ kh√≥ hi·ªÉn th·ªã tr√™n UI
- LO·∫†I B·ªé HO√ÄN TO√ÄN c√°c tham chi·∫øu s√°ch gi√°o khoa nh∆∞ "SGK trang X", "Xem b·∫£ng X.X (SGK trang Y)", "Quan s√°t h√¨nh X.X (SGK trang Y)" v√¨ ch√∫ng ·∫£nh h∆∞·ªüng ƒë·∫øn ch·∫•t l∆∞·ª£ng lesson plan
- LO·∫†I B·ªé c√°c c√¢u ch·ªâ d·∫´n tham chi·∫øu nh∆∞ "T√¨m hi·ªÉu v·ªÅ...", "Xem b·∫£ng...", "Quan s√°t h√¨nh..." m√† kh√¥ng c√≥ n·ªôi dung c·ª• th·ªÉ
- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ khoa h·ªçc v√† c√¥ng th·ª©c (H2O, 1.672 x 10^-27, etc.)
- ƒê·∫£m b·∫£o m·ªói b√†i t·∫≠p/v√≠ d·ª• c√≥ ƒë·∫ßy ƒë·ªß ƒë·ªÅ b√†i v√† l·ªùi gi·∫£i
- T·∫≠p trung v√†o n·ªôi dung ki·∫øn th·ª©c c·ªët l√µi, kh√¥ng c·∫ßn c√°c ph·∫ßn h∆∞·ªõng d·∫´n tham kh·∫£o

ƒê·ªäNH D·∫†NG XU·∫§T:
- Text thu·∫ßn t√∫y, kh√¥ng markdown, KH√îNG c√≥ b·∫£ng
- M·ªói ph·∫ßn c√°ch nhau b·∫±ng d√≤ng tr·ªëng
- Gi·ªØ nguy√™n k√Ω hi·ªáu khoa h·ªçc (^, ¬≤, ¬≥, ‚Üí, ‚Üê, etc.)
- N·ªôi dung ph·∫£i ƒë·ªôc l·∫≠p, kh√¥ng c·∫ßn tham chi·∫øu external

N·ªòI DUNG C·∫¶N CH·ªàNH S·ª¨A:
{raw_text[:8000]}

Tr·∫£ v·ªÅ n·ªôi dung ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c l·∫°i theo y√™u c·∫ßu (KH√îNG bao g·ªìm b·∫£ng v√† tham chi·∫øu SGK):"""

            result = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=2048
            )

            if result.get("success") and result.get("text"):
                refined_content = result.get("text", "").strip()
                # L√†m s·∫°ch n·ªôi dung sau khi nh·∫≠n t·ª´ OpenRouter
                clean_content = self.clean_text_content(refined_content)
                logger.info(f"ü§ñ{prompt} ")
                logger.info(f"ü§ñ{refined_content} ")
                logger.info(f"‚úÖ Content refined and cleaned: {len(raw_text)} ‚Üí {len(clean_content)} chars")
                return clean_content
            else:
                logger.warning("OpenRouter returned insufficient content, using original")
                return self.clean_text_content(raw_text)

        except Exception as e:
            logger.error(f"‚ùå L·ªói tinh ch·ªânh n·ªôi dung v·ªõi OpenRouter: {e}")
            return self.clean_text_content(raw_text)




# Factory function ƒë·ªÉ t·∫°o EnhancedTextbookService instance
def get_enhanced_textbook_service() -> EnhancedTextbookService:
    """
    T·∫°o EnhancedTextbookService instance m·ªõi

    Returns:
        EnhancedTextbookService: Fresh instance
    """
    return EnhancedTextbookService()
