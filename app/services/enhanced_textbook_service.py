"""
Enhanced Textbook Service - C·∫£i ti·∫øn x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM
Tr·∫£ v·ªÅ c·∫•u tr√∫c: S√°ch ‚Üí Ch∆∞∆°ng ‚Üí B√†i ‚Üí N·ªôi dung
"""

import logging
import asyncio
import json
import base64
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
import fitz  # PyMuPDF
from PIL import Image
import io

from app.services.simple_ocr_service import simple_ocr_service
from app.services.llm_service import llm_service

logger = logging.getLogger(__name__)


class EnhancedTextbookService:
    """Service c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def process_textbook_to_structure(
        self,
        pdf_content: bytes,
        filename: str,
        book_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω PDF s√°ch gi√°o khoa v√† tr·∫£ v·ªÅ c·∫•u tr√∫c ho√†n ch·ªânh

        Args:
            pdf_content: N·ªôi dung PDF
            filename: T√™n file
            book_metadata: Metadata s√°ch (title, subject, grade, etc.)

        Returns:
            Dict v·ªõi c·∫•u tr√∫c: book -> chapters -> lessons -> content
        """
        try:
            logger.info(f"üöÄ Starting enhanced textbook processing: {filename}")

            # Step 1: Extract all pages with OCR
            logger.info("üìÑ Extracting pages with OCR...")
            pages_data = await self._extract_pages_with_ocr(pdf_content)
            logger.info(f"‚úÖ Extracted {len(pages_data)} pages")

            # Step 2: Analyze book structure with LLM
            logger.info("üß† Analyzing book structure...")
            book_structure = await self._analyze_book_structure_enhanced(
                pages_data, book_metadata
            )
            logger.info(
                f"üìö Detected {len(book_structure.get('chapters', []))} chapters"
            )

            # Step 3: Process content for each lesson
            logger.info("üîÑ Processing lesson content...")
            processed_book = await self._process_lessons_content(
                book_structure, pages_data
            )

            logger.info("‚úÖ Textbook processing completed successfully")

            return {
                "success": True,
                "book": processed_book,
                "total_pages": len(pages_data),
                "total_chapters": len(processed_book.get("chapters", [])),
                "total_lessons": sum(
                    len(ch.get("lessons", []))
                    for ch in processed_book.get("chapters", [])
                ),
                "message": "Textbook processed successfully",
            }

        except Exception as e:
            logger.error(f"‚ùå Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "Failed to process textbook",
            }

    async def _extract_pages_with_ocr(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract t·∫•t c·∫£ pages v·ªõi OCR n·∫øu c·∫ßn"""

        def extract_page_data():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text normally first
                text = page.get_text("text")  # type: ignore

                # Extract images
                images = []
                image_list = page.get_images()

                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)

                        if pix.n - pix.alpha < 4:  # GRAY or RGB
                            img_data = pix.tobytes("png")
                            img_base64 = base64.b64encode(img_data).decode()

                            images.append(
                                {
                                    "index": img_index,
                                    "data": img_base64,
                                    "format": "png",
                                    "page": page_num + 1,
                                }
                            )

                        pix = None
                    except Exception as e:
                        logger.warning(
                            f"Error extracting image {img_index} from page {page_num}: {e}"
                        )

                pages_data.append(
                    {
                        "page_number": page_num + 1,
                        "text": text,
                        "images": images,
                        "has_text": len(text.strip()) > 50,
                    }
                )

            doc.close()
            return pages_data

        # Extract pages in background thread
        pages_data = await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_page_data
        )

        # Apply OCR to pages with insufficient text
        ocr_tasks = []
        for page in pages_data:
            if not page["has_text"]:
                ocr_tasks.append(self._apply_ocr_to_page(page, pdf_content))

        if ocr_tasks:
            logger.info(
                f"üîç Applying OCR to {len(ocr_tasks)} pages with insufficient text"
            )
            ocr_results = await asyncio.gather(*ocr_tasks, return_exceptions=True)

            # Update pages with OCR results
            ocr_index = 0
            for page in pages_data:
                if not page["has_text"] and ocr_index < len(ocr_results):
                    if not isinstance(ocr_results[ocr_index], Exception):
                        page["text"] = ocr_results[ocr_index]
                        page["ocr_applied"] = True
                    ocr_index += 1

        return pages_data

    async def _apply_ocr_to_page(
        self, page_data: Dict[str, Any], pdf_content: bytes
    ) -> str:
        """Apply OCR to a specific page"""
        try:
            # Use existing OCR service for this page
            page_num = page_data["page_number"]

            # Extract just this page as bytes
            def extract_single_page():
                doc = fitz.open(stream=pdf_content, filetype="pdf")
                page = doc[page_num - 1]

                # Convert page to image
                mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better OCR
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                doc.close()

                return img_data

            img_data = await asyncio.get_event_loop().run_in_executor(
                self.executor, extract_single_page
            )

            # Apply OCR using PIL and simple_ocr_service logic
            image = Image.open(io.BytesIO(img_data))

            # Use simple OCR service's OCR logic
            if (
                hasattr(simple_ocr_service, "easyocr_reader")
                and simple_ocr_service.easyocr_reader
            ):
                import numpy as np

                results = simple_ocr_service.easyocr_reader.readtext(np.array(image))
                text_parts = [str(result[1]) for result in results if len(result) >= 2]
                return " ".join(text_parts)
            else:
                # Fallback to Tesseract
                import pytesseract

                return pytesseract.image_to_string(
                    image, config=simple_ocr_service.tesseract_config
                )

        except Exception as e:
            logger.error(f"OCR failed for page {page_data['page_number']}: {e}")
            return ""

    async def _analyze_book_structure_enhanced(
        self,
        pages_data: List[Dict[str, Any]],
        book_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫•u tr√∫c s√°ch v·ªõi LLM c·∫£i ti·∫øn"""

        if not llm_service.is_available():
            logger.warning("LLM not available, using pattern-based analysis")
            return await self._pattern_based_structure_analysis(
                pages_data, book_metadata
            )

        # T·∫°o text sample t·ª´ c√°c trang ƒë·ªÉ ph√¢n t√≠ch
        sample_text = ""
        for i, page in enumerate(pages_data[:20]):  # L·∫•y 20 trang ƒë·∫ßu ƒë·ªÉ ph√¢n t√≠ch
            if page["text"].strip():
                sample_text += f"\n--- Trang {page['page_number']} ---\n{page['text'][:500]}"  # 500 chars per page

        prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch s√°ch gi√°o khoa Vi·ªát Nam. Ph√¢n t√≠ch n·ªôi dung v√† tr·∫£ v·ªÅ c·∫•u tr√∫c ch√≠nh x√°c.

TH√îNG TIN S√ÅCH:
- T·ªïng s·ªë trang: {len(pages_data)}
- Metadata: {json.dumps(book_metadata or {}, ensure_ascii=False)}

N·ªòI DUNG SAMPLE:
{sample_text}

Y√äU C·∫¶U:
1. X√°c ƒë·ªãnh ti√™u ƒë·ªÅ s√°ch, m√¥n h·ªçc, l·ªõp
2. T√¨m t·∫•t c·∫£ CH∆Ø∆†NG (Chapter) trong s√°ch
3. T√¨m t·∫•t c·∫£ B√ÄI H·ªåC (Lesson) trong m·ªói ch∆∞∆°ng
4. X√°c ƒë·ªãnh trang b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c cho m·ªói ch∆∞∆°ng/b√†i
5. Tr·∫£ v·ªÅ JSON chu·∫©n

JSON FORMAT:
{{
  "book_info": {{
    "title": "T√™n s√°ch ch√≠nh x√°c",
    "subject": "M√¥n h·ªçc (To√°n/L√Ω/H√≥a/...)",
    "grade": "L·ªõp (10/11/12)",
    "total_pages": {len(pages_data)}
  }},
  "chapters": [
    {{
      "chapter_id": "chapter_01",
      "chapter_title": "T√™n ch∆∞∆°ng ch√≠nh x√°c",
      "start_page": 1,
      "end_page": 20,
      "lessons": [
        {{
          "lesson_id": "lesson_01_01",
          "lesson_title": "T√™n b√†i h·ªçc ch√≠nh x√°c",
          "start_page": 1,
          "end_page": 5
        }}
      ]
    }}
  ]
}}

Tr·∫£ v·ªÅ JSON:"""

        try:
            if not llm_service.model:
                raise Exception("LLM model not available")

            response = llm_service.model.generate_content(prompt)
            json_text = response.text.strip()

            # Clean JSON - c·∫£i thi·ªán vi·ªác x·ª≠ l√Ω
            if json_text.startswith("```json"):
                json_text = json_text[7:]
            if json_text.startswith("```"):
                json_text = json_text[3:]
            if json_text.endswith("```"):
                json_text = json_text[:-3]

            # T√¨m JSON h·ª£p l·ªá trong response
            json_text = json_text.strip()

            # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa JSON
            start_idx = json_text.find("{")
            if start_idx == -1:
                raise ValueError("No JSON object found in response")

            # T√¨m v·ªã tr√≠ k·∫øt th√∫c JSON b·∫±ng c√°ch ƒë·∫øm d·∫•u ngo·∫∑c
            brace_count = 0
            end_idx = start_idx
            for i, char in enumerate(json_text[start_idx:], start_idx):
                if char == "{":
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

            # Extract JSON h·ª£p l·ªá
            clean_json = json_text[start_idx:end_idx]

            structure = json.loads(clean_json)

            # Validate structure
            if "chapters" in structure and len(structure["chapters"]) > 0:
                logger.info(f"LLM detected {len(structure['chapters'])} chapters")
                return structure
            else:
                logger.warning("LLM returned invalid structure, using fallback")
                return await self._pattern_based_structure_analysis(
                    pages_data, book_metadata
                )

        except Exception as e:
            logger.error(f"LLM structure analysis failed: {e}")
            logger.debug(
                f"Raw LLM response: {response.text[:500] if 'response' in locals() else 'No response'}"
            )
            return await self._pattern_based_structure_analysis(
                pages_data, book_metadata
            )

    async def _pattern_based_structure_analysis(
        self,
        pages_data: List[Dict[str, Any]],
        book_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫•u tr√∫c d·ª±a tr√™n pattern matching"""

        total_pages = len(pages_data)

        # Extract book info from metadata or first pages
        book_info = {
            "title": book_metadata.get("title", "S√°ch gi√°o khoa")
            if book_metadata
            else "S√°ch gi√°o khoa",
            "subject": book_metadata.get("subject", "Ch∆∞a x√°c ƒë·ªãnh")
            if book_metadata
            else "Ch∆∞a x√°c ƒë·ªãnh",
            "grade": book_metadata.get("grade", "Ch∆∞a x√°c ƒë·ªãnh")
            if book_metadata
            else "Ch∆∞a x√°c ƒë·ªãnh",
            "total_pages": total_pages,
        }

        # Find chapters and lessons using pattern matching
        chapters = []
        current_chapter = None
        current_lesson = None

        for page in pages_data:
            lines = page["text"].split("\n")

            # Look for chapter patterns
            for line in lines:
                line_clean = line.strip()
                if len(line_clean) > 5 and len(line_clean) < 100:
                    # Chapter detection
                    if any(
                        pattern in line_clean.lower()
                        for pattern in ["ch∆∞∆°ng", "chapter", "ph·∫ßn", "b√†i t·∫≠p ch∆∞∆°ng"]
                    ):
                        # Save previous chapter
                        if current_chapter:
                            chapters.append(current_chapter)

                        # Start new chapter
                        chapter_num = len(chapters) + 1
                        current_chapter = {
                            "chapter_id": f"chapter_{chapter_num:02d}",
                            "chapter_title": line_clean,
                            "start_page": page["page_number"],
                            "end_page": page["page_number"],
                            "lessons": [],
                        }
                        current_lesson = None

                    # Lesson detection
                    elif (
                        any(
                            pattern in line_clean.lower()
                            for pattern in ["b√†i", "lesson", "ti·∫øt"]
                        )
                        and current_chapter
                    ):
                        # Save previous lesson
                        if current_lesson:
                            current_chapter["lessons"].append(current_lesson)

                        # Start new lesson
                        lesson_num = len(current_chapter["lessons"]) + 1
                        current_lesson = {
                            "lesson_id": f"lesson_{len(chapters)+1:02d}_{lesson_num:02d}",
                            "lesson_title": line_clean,
                            "start_page": page["page_number"],
                            "end_page": page["page_number"],
                        }

            # Update end pages
            if current_chapter:
                current_chapter["end_page"] = page["page_number"]
            if current_lesson:
                current_lesson["end_page"] = page["page_number"]

        # Add final chapter and lesson
        if current_lesson and current_chapter:
            current_chapter["lessons"].append(current_lesson)
        if current_chapter:
            chapters.append(current_chapter)

        # If no chapters found, create default structure
        if not chapters:
            chapters = self._create_default_structure(total_pages)

        return {"book_info": book_info, "chapters": chapters}

    def _create_default_structure(self, total_pages: int) -> List[Dict[str, Any]]:
        """T·∫°o c·∫•u tr√∫c m·∫∑c ƒë·ªãnh khi kh√¥ng detect ƒë∆∞·ª£c"""

        chapters = []
        pages_per_chapter = max(total_pages // 3, 10)  # √çt nh·∫•t 3 ch∆∞∆°ng

        for chapter_num in range(1, 4):  # 3 ch∆∞∆°ng
            start_page = (chapter_num - 1) * pages_per_chapter + 1
            end_page = min(chapter_num * pages_per_chapter, total_pages)

            if start_page > total_pages:
                break

            # T·∫°o 2-3 b√†i trong m·ªói ch∆∞∆°ng
            lessons = []
            pages_per_lesson = max((end_page - start_page + 1) // 3, 3)

            for lesson_num in range(1, 4):  # 3 b√†i m·ªói ch∆∞∆°ng
                lesson_start = start_page + (lesson_num - 1) * pages_per_lesson
                lesson_end = min(
                    start_page + lesson_num * pages_per_lesson - 1, end_page
                )

                if lesson_start > end_page:
                    break

                lessons.append(
                    {
                        "lesson_id": f"lesson_{chapter_num:02d}_{lesson_num:02d}",
                        "lesson_title": f"B√†i {lesson_num}",
                        "start_page": lesson_start,
                        "end_page": lesson_end,
                    }
                )

            chapters.append(
                {
                    "chapter_id": f"chapter_{chapter_num:02d}",
                    "chapter_title": f"Ch∆∞∆°ng {chapter_num}",
                    "start_page": start_page,
                    "end_page": end_page,
                    "lessons": lessons,
                }
            )

        return chapters

    async def _process_lessons_content(
        self, book_structure: Dict[str, Any], pages_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω n·ªôi dung chi ti·∫øt cho t·ª´ng b√†i h·ªçc"""

        processed_book = {
            "book_info": book_structure.get("book_info", {}),
            "chapters": [],
        }

        for chapter in book_structure.get("chapters", []):
            processed_chapter = {
                "chapter_id": chapter["chapter_id"],
                "chapter_title": chapter["chapter_title"],
                "start_page": chapter["start_page"],
                "end_page": chapter["end_page"],
                "lessons": [],
            }

            for lesson in chapter.get("lessons", []):
                logger.info(f"Processing lesson: {lesson['lesson_title']}")

                # Extract content for this lesson
                lesson_content = await self._extract_lesson_content(lesson, pages_data)

                processed_lesson = {
                    "lesson_id": lesson["lesson_id"],
                    "lesson_title": lesson["lesson_title"],
                    "start_page": lesson["start_page"],
                    "end_page": lesson["end_page"],
                    "content": lesson_content,
                }

                processed_chapter["lessons"].append(processed_lesson)

            processed_book["chapters"].append(processed_chapter)

        return processed_book

    async def _extract_lesson_content(
        self, lesson: Dict[str, Any], pages_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Extract n·ªôi dung chi ti·∫øt c·ªßa m·ªôt b√†i h·ªçc"""

        start_page = lesson["start_page"]
        end_page = lesson["end_page"]

        # Collect all text and images for this lesson
        lesson_text = ""
        lesson_images = []
        lesson_pages = []

        for page_num in range(start_page, end_page + 1):
            # Find page data (pages_data is 0-indexed but page_number is 1-indexed)
            page_data = None
            for page in pages_data:
                if page["page_number"] == page_num:
                    page_data = page
                    break

            if not page_data:
                continue

            lesson_pages.append(page_num)

            # Add text content
            if page_data["text"].strip():
                # Clean text with LLM if available
                cleaned_text = await self._clean_text_with_llm(page_data["text"])
                lesson_text += f"\n--- Trang {page_num} ---\n{cleaned_text}\n"

            # Add images with LLM descriptions only
            for img in page_data.get("images", []):
                # Describe image with LLM using base64 data
                img_description = await self._describe_image_with_llm(img["data"])

                lesson_images.append(
                    {
                        "page": page_num,
                        "index": img["index"],
                        "format": img["format"],
                        "description": img_description,
                        # Note: Removed base64 data to reduce response size
                    }
                )

        return {
            "text": lesson_text.strip(),
            "images": lesson_images,
            "pages": lesson_pages,
            "total_pages": len(lesson_pages),
            "has_images": len(lesson_images) > 0,
        }

    async def _clean_text_with_llm(self, raw_text: str) -> str:
        """Clean v√† format text b·∫±ng LLM"""

        if not llm_service.is_available() or not raw_text.strip():
            return raw_text.strip()

        try:
            prompt = f"""
B·∫°n l√† chuy√™n gia x·ª≠ l√Ω text t·ª´ s√°ch gi√°o khoa. H√£y l√†m s·∫°ch v√† format text sau:

Y√äU C·∫¶U:
1. S·ª≠a l·ªói OCR (k√Ω t·ª± nh·∫≠n d·∫°ng sai)
2. Lo·∫°i b·ªè k√Ω t·ª± l·∫°, kho·∫£ng tr·∫Øng th·ª´a
3. S·∫Øp x·∫øp ƒëo·∫°n vƒÉn cho d·ªÖ ƒë·ªçc
4. Gi·ªØ nguy√™n √Ω nghƒ©a v√† c·∫•u tr√∫c
5. Tr·∫£ v·ªÅ text ti·∫øng Vi·ªát chu·∫©n

Text g·ªëc:
{raw_text[:1000]}  # Limit to 1000 chars

Text ƒë√£ l√†m s·∫°ch:"""

            if not llm_service.model:
                return raw_text.strip()

            response = llm_service.model.generate_content(prompt)
            cleaned_text = response.text.strip()

            return cleaned_text if cleaned_text else raw_text.strip()

        except Exception as e:
            logger.error(f"Text cleaning failed: {e}")
            return raw_text.strip()

    async def _describe_image_with_llm(self, image_base64: str) -> str:
        """M√¥ t·∫£ h√¨nh ·∫£nh b·∫±ng LLM v·ªõi Gemini Vision API"""

        if not llm_service.is_available():
            return "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa"

        try:
            if not llm_service.model:
                return "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa"

            # S·ª≠ d·ª•ng Gemini ƒë·ªÉ m√¥ t·∫£ h√¨nh ·∫£nh
            prompt = """
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch h√¨nh ·∫£nh trong s√°ch gi√°o khoa Vi·ªát Nam.
H√£y m√¥ t·∫£ h√¨nh ·∫£nh n√†y m·ªôt c√°ch chi ti·∫øt v√† h·ªØu √≠ch cho vi·ªác t·∫°o gi√°o √°n.

Y√äU C·∫¶U M√î T·∫¢:
1. X√°c ƒë·ªãnh lo·∫°i h√¨nh ·∫£nh: bi·ªÉu ƒë·ªì, c√¥ng th·ª©c, s∆° ƒë·ªì, h√¨nh minh h·ªça, b·∫£ng bi·ªÉu, th√≠ nghi·ªám
2. M√¥ t·∫£ n·ªôi dung ch√≠nh v√† c√°c y·∫øu t·ªë quan tr·ªçng
3. Gi·∫£i th√≠ch m·ª•c ƒë√≠ch gi√°o d·ª•c v√† c√°ch s·ª≠ d·ª•ng trong gi·∫£ng d·∫°y
4. ƒê·ªÅ xu·∫•t c√°ch gi·∫£i th√≠ch cho h·ªçc sinh
5. M√¥ t·∫£ ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát (t·ªëi ƒëa 200 t·ª´)

V√≠ d·ª• format mong mu·ªën:
"Bi·ªÉu ƒë·ªì chu tr√¨nh n∆∞·ªõc trong t·ª± nhi√™n, minh h·ªça qu√° tr√¨nh bay h∆°i, ng∆∞ng t·ª• v√† m∆∞a.
H√¨nh ·∫£nh n√†y gi√∫p h·ªçc sinh hi·ªÉu r√µ c√°c giai ƒëo·∫°n c·ªßa chu tr√¨nh n∆∞·ªõc v√† vai tr√≤ c·ªßa
m·∫∑t tr·ªùi trong qu√° tr√¨nh n√†y. C√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i th√≠ch hi·ªán t∆∞·ª£ng th·ªùi ti·∫øt v√†
t·∫ßm quan tr·ªçng c·ªßa n∆∞·ªõc trong h·ªá sinh th√°i."

H√£y m√¥ t·∫£ h√¨nh ·∫£nh:"""

            # T·∫°o image part cho Gemini
            import base64
            from PIL import Image
            import io

            # Decode base64 ƒë·ªÉ ki·ªÉm tra v√† resize n·∫øu c·∫ßn
            img_data = base64.b64decode(image_base64)
            image = Image.open(io.BytesIO(img_data))

            # Resize n·∫øu ·∫£nh qu√° l·ªõn ƒë·ªÉ ti·∫øt ki·ªám API cost
            max_size = (800, 800)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)

                # Convert back to base64
                buffer = io.BytesIO()
                image.save(buffer, format="PNG")
                img_data = buffer.getvalue()
                image_base64 = base64.b64encode(img_data).decode()

            # T·∫°o content v·ªõi image ƒë·ªÉ Gemini ph√¢n t√≠ch
            image_part = {
                "mime_type": "image/png",
                "data": base64.b64decode(image_base64),
            }

            response = llm_service.model.generate_content([prompt, image_part])
            description = response.text.strip()

            # Validate v√† clean description
            if description and len(description) > 10:
                # Gi·ªõi h·∫°n ƒë·ªô d√†i m√¥ t·∫£
                if len(description) > 500:
                    description = description[:500] + "..."
                return description
            else:
                return "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa (kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£ chi ti·∫øt)"

        except Exception as e:
            logger.error(f"Image description with LLM failed: {e}")
            # Fallback descriptions based on context
            fallback_descriptions = [
                "Bi·ªÉu ƒë·ªì ho·∫∑c s∆° ƒë·ªì minh h·ªça kh√°i ni·ªám trong b√†i h·ªçc",
                "H√¨nh ·∫£nh th√≠ nghi·ªám ho·∫∑c th·ª±c h√†nh trong ph√≤ng lab",
                "C√¥ng th·ª©c to√°n h·ªçc ho·∫∑c ph∆∞∆°ng tr√¨nh h√≥a h·ªçc",
                "B·∫£ng bi·ªÉu th·ªëng k√™ ho·∫∑c d·ªØ li·ªáu khoa h·ªçc",
                "H√¨nh minh h·ªça c·∫•u tr√∫c ho·∫∑c quy tr√¨nh t·ª± nhi√™n",
                "S∆° ƒë·ªì t∆∞ duy ho·∫∑c b·∫£n ƒë·ªì kh√°i ni·ªám",
            ]
            import random

            return random.choice(fallback_descriptions)

    async def _add_image_descriptions(self, pages_data: List[Dict[str, Any]]) -> None:
        """Add LLM-generated descriptions for all images in pages_data"""

        if not llm_service.is_available():
            logger.warning("LLM not available for image descriptions")
            return

        image_tasks = []
        for page in pages_data:
            for img in page.get("images", []):
                if img.get("data"):
                    image_tasks.append(self._describe_image_with_llm(img["data"]))

        if image_tasks:
            logger.info(f"üñºÔ∏è Generating descriptions for {len(image_tasks)} images...")
            descriptions = await asyncio.gather(*image_tasks, return_exceptions=True)
    
            # Apply descriptions back to images
            desc_index = 0
            for page in pages_data:
                for img in page.get("images", []):
                    if img.get("data") and desc_index < len(descriptions):
                        if not isinstance(descriptions[desc_index], Exception):
                            img["description"] = descriptions[desc_index]
                            img["description_method"] = "llm_generated"
                        else:
                            img["description"] = (
                                "H√¨nh ·∫£nh minh h·ªça trong s√°ch gi√°o khoa"
                            )
                            img["description_method"] = "fallback"
                        desc_index += 1

    async def _build_final_structure(
        self,
        analysis_result: Dict[str, Any],
        pages_data: List[Dict[str, Any]],
        book_metadata: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Build final book structure from analysis result"""

        book_structure = {
            "title": analysis_result.get("book_info", {}).get(
                "title", book_metadata["title"]
            ),
            "subject": analysis_result.get("book_info", {}).get(
                "subject", book_metadata["subject"]
            ),
            "grade": analysis_result.get("book_info", {}).get(
                "grade", book_metadata["grade"]
            ),
            "chapters": [],
        }

        # Process chapters and lessons
        for chapter in analysis_result.get("chapters", []):
            chapter_obj = {
                "title": chapter.get("chapter_title", "Ch∆∞∆°ng kh√¥ng x√°c ƒë·ªãnh"),
                "lessons": [],
            }

            # Extract lessons
            for lesson in chapter.get("lessons", []):
                lesson_content = ""
                lesson_images = []
                start_page = lesson.get("start_page", 1)
                end_page = lesson.get("end_page", start_page)

                # Collect content and images from lesson pages
                for page in pages_data:
                    page_num = page.get("page_number", 0)
                    if start_page <= page_num <= end_page:
                        lesson_content += page.get("text", "") + "\n"
                        # Add images with descriptions
                        for img in page.get("images", []):
                            lesson_images.append(
                                {
                                    "page": page_num,
                                    "description": img.get(
                                        "description", "H√¨nh ·∫£nh minh h·ªça"
                                    ),
                                    "format": img.get("format", "png"),
                                }
                            )

                lesson_obj = {
                    "title": lesson.get("lesson_title", "B√†i h·ªçc kh√¥ng x√°c ƒë·ªãnh"),
                    "content": lesson_content.strip(),
                    "page_numbers": list(range(start_page, end_page + 1)),
                    "images": lesson_images,
                }
                chapter_obj["lessons"].append(lesson_obj)

            book_structure["chapters"].append(chapter_obj)

        return book_structure


# Global instance
enhanced_textbook_service = EnhancedTextbookService()
