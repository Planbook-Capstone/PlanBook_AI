"""
Enhanced Textbook Service - C·∫£i ti·∫øn x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM
Tr·∫£ v·ªÅ c·∫•u tr√∫c: S√°ch ‚Üí Ch∆∞∆°ng ‚Üí B√†i ‚Üí N·ªôi dung
"""

import logging
import asyncio
import re
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import fitz  # PyMuPDF
from PIL import Image
import io

from app.services.simple_ocr_service import get_simple_ocr_service

logger = logging.getLogger(__name__)


class EnhancedTextbookService:
    """Service c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.ocr_service = get_simple_ocr_service()

    async def process_textbook_to_structure(
        self,
        pdf_content: bytes,
        filename: str,
        book_metadata: Optional[Dict[str, Any]] = None,
        lesson_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω PDF s√°ch gi√°o khoa v√† tr·∫£ v·ªÅ c·∫•u tr√∫c ho√†n ch·ªânh

        Args:
            pdf_content: N·ªôi dung PDF
            filename: T√™n file
            book_metadata: Metadata s√°ch (title, subject, grade, etc.)
            lesson_id: ID b√†i h·ªçc t√πy ch·ªçn ƒë·ªÉ li√™n k·∫øt v·ªõi lesson c·ª• th·ªÉ

        Returns:
            Dict v·ªõi c·∫•u tr√∫c: book -> chapters -> lessons -> content
        """
        try:
            logger.info(f"üöÄ Starting enhanced textbook processing: {filename}")

            # Step 1: Extract all pages with OCR
            logger.info("üìÑ Extracting pages with OCR...")
            pages_data = await self._extract_pages_with_ocr(pdf_content)
            logger.info(f"‚úÖ Extracted {len(pages_data)} pages")

            # Skip image analysis to improve speed
            logger.info("‚ö° Skipping image analysis for faster processing")

            # Step 2: Combine all text from pages
            logger.info("üìù Combining text from all pages...")
            full_text = ""
            all_page_numbers = []
            for page in pages_data:
                full_text += page.get("text", "") + "\n"
                all_page_numbers.append(page.get("page_number", 0))

            logger.info(f"üìÑ ALl Data {len(pages_data)} pages")
            logger.info(f"üìÑ Combined text from {len(pages_data)} pages")

            # Step 3: Refine content directly with OpenRouter LLM
            logger.info("ü§ñ Refining content with OpenRouter LLM...")
            refined_content = await self.refine_raw_content_with_llm(full_text)
            logger.info("‚úÖ Content refinement completed")
            logger.info("‚úÖ Content refinement {refined_content}")
            # Step 4: Return clean text content directly
            logger.info("‚úÖ Content processing completed")

            # Skip image extraction for faster processing
            images_data = []
            logger.info("‚ö° Skipping image extraction for faster processing")

            return {
                "success": True,
                "clean_book_structure": refined_content,  # Return clean text directly
                "images_data": images_data,  # Empty array
                "total_pages": len(pages_data),
                "message": f"Textbook processed successfully with LLM content refinement",
            }

        except Exception as e:
            logger.error(f"‚ùå Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "Failed to process textbook",
            }

    async def _extract_pages_with_ocr(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract t·∫•t c·∫£ pages v·ªõi OCR n·∫øu c·∫ßn"""

        def extract_page_data():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text normally first
                text = page.get_text("text")  # type: ignore

                # Skip image extraction for faster processing
                images = []

                pages_data.append(
                    {
                        "page_number": page_num + 1,
                        "text": text,
                        "images": images,
                        "has_text": len(text.strip()) > 50,
                    }
                )

            doc.close()
            return pages_data

        # Extract pages in background thread
        pages_data = await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_page_data
        )

        # Apply OCR to pages with insufficient text
        ocr_tasks = []
        for page in pages_data:
            if not page["has_text"]:
                ocr_tasks.append(self._apply_ocr_to_page(page, pdf_content))

        if ocr_tasks:
            logger.info(
                f"üîç Applying OCR to {len(ocr_tasks)} pages with insufficient text"
            )
            ocr_results = await asyncio.gather(*ocr_tasks, return_exceptions=True)

            # Update pages with OCR results
            ocr_index = 0
            for page in pages_data:
                if not page["has_text"] and ocr_index < len(ocr_results):
                    if not isinstance(ocr_results[ocr_index], Exception):
                        page["text"] = ocr_results[ocr_index]
                        page["ocr_applied"] = True
                    ocr_index += 1

        return pages_data

    async def _apply_ocr_to_page(
        self, page_data: Dict[str, Any], pdf_content: bytes
    ) -> str:
        """Apply OCR to a specific page"""
        try:
            # Use existing OCR service for this page
            page_num = page_data["page_number"]

            # Extract just this page as bytes
            def extract_single_page():
                doc = fitz.open(stream=pdf_content, filetype="pdf")
                page = doc[page_num - 1]

                # Convert page to image
                mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better OCR
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                doc.close()

                return img_data

            img_data = await asyncio.get_event_loop().run_in_executor(
                self.executor, extract_single_page
            )

            # Apply OCR using SimpleOCRService
            image = Image.open(io.BytesIO(img_data))

            # Use SimpleOCRService's _ocr_image method which handles EasyOCR initialization
            text = await self.ocr_service._ocr_image(image, page_data['page_number'])
            return text

        except Exception as e:
            logger.error(f"OCR failed for page {page_data['page_number']}: {e}")
            return ""





















    def clean_text_content(self, text: str) -> str:
        """L√†m s·∫°ch n·ªôi dung text - lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, format kh√¥ng c·∫ßn thi·∫øt"""
        if not text:
            return ""

        logger.info("üßπ Cleaning text content...")

        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát v√† format markdown
        cleaned_text = text

        # Lo·∫°i b·ªè d·∫•u * (markdown bold/italic)
        cleaned_text = re.sub(r'\*+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u # (markdown headers)
        cleaned_text = re.sub(r'#+\s*', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u _ (markdown underline)
        cleaned_text = re.sub(r'_+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u ` (markdown code)
        cleaned_text = re.sub(r'`+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u [] v√† () (markdown links)
        cleaned_text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', cleaned_text)

        # Lo·∫°i b·ªè k√Ω t·ª± \b (backspace)
        cleaned_text = cleaned_text.replace('\b', '')

        # Lo·∫°i b·ªè k√Ω t·ª± \r
        cleaned_text = cleaned_text.replace('\r', '')

        # Thay th·∫ø nhi·ªÅu d·∫•u xu·ªëng d√≤ng li√™n ti·∫øp b·∫±ng 1 d·∫•u xu·ªëng d√≤ng
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi m·ªói d√≤ng
        lines = cleaned_text.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip()]

        # Gh√©p l·∫°i th√†nh m·ªôt ƒëo·∫°n text li√™n t·ª•c
        final_text = ' '.join(cleaned_lines)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        final_text = re.sub(r'\s+', ' ', final_text).strip()

        logger.info(f"üßπ Text cleaned: {len(text)} ‚Üí {len(final_text)} chars")
        return final_text

    async def refine_raw_content_with_llm(self, raw_text: str) -> str:
        """G·ª≠i text th√¥ tr·ª±c ti·∫øp ƒë·∫øn OpenRouter LLM ƒë·ªÉ l·ªçc v√† ch·ªânh s·ª≠a n·ªôi dung"""
        try:
            from app.services.openrouter_service import get_openrouter_service

            openrouter_service = get_openrouter_service()
            logger.info("ü§ñ Sending raw content to OpenRouter for refinement...")

            prompt = f"""
B·∫°n l√† chuy√™n gia gi√°o d·ª•c, h√£y l·ªçc v√† c·∫•u tr√∫c l·∫°i n·ªôi dung s√°ch gi√°o khoa ƒë·ªÉ t·ªëi ∆∞u cho h·ªá th·ªëng chunking th√¥ng minh.

Y√äU C·∫¶U C·∫§U TR√öC:
1. ƒê·ªäNH NGHƒ®A: B·∫Øt ƒë·∫ßu b·∫±ng "ƒê·ªãnh nghƒ©a:" ho·∫∑c s·ª≠ d·ª•ng c·∫•u tr√∫c "X l√†..." r√µ r√†ng
2. B√ÄI T·∫¨P/V√ç D·ª§: ƒê√°nh s·ªë r√µ r√†ng "B√†i 1.", "V√≠ d·ª• 1:", "H√£y cho bi·∫øt..."
3. B·∫¢NG BI·ªÇU: B·∫Øt ƒë·∫ßu b·∫±ng "B·∫£ng X:" v√† gi·ªØ nguy√™n c·∫•u tr√∫c b·∫£ng
4. TI·ªÇU M·ª§C: S·ª≠ d·ª•ng "I.", "II.", "1.", "2." cho c√°c ph·∫ßn ch√≠nh

Y√äU C·∫¶U N·ªòI DUNG:
- Gi·ªØ l·∫°i to√†n b·ªô kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, c√¥ng th·ª©c, v√≠ d·ª• quan tr·ªçng
- Lo·∫°i b·ªè header, footer, s·ªë trang, watermark kh√¥ng li√™n quan
- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ khoa h·ªçc v√† c√¥ng th·ª©c (H2O, 1.672 x 10^-27, etc.)
- ƒê·∫£m b·∫£o m·ªói b√†i t·∫≠p/v√≠ d·ª• c√≥ ƒë·∫ßy ƒë·ªß ƒë·ªÅ b√†i v√† l·ªùi gi·∫£i
- B·∫£ng ph·∫£i ho√†n ch·ªânh v·ªõi ti√™u ƒë·ªÅ v√† n·ªôi dung

ƒê·ªäNH D·∫†NG XU·∫§T:
- Text thu·∫ßn t√∫y, kh√¥ng markdown
- M·ªói ph·∫ßn c√°ch nhau b·∫±ng d√≤ng tr·ªëng
- Gi·ªØ nguy√™n k√Ω hi·ªáu khoa h·ªçc (^, ¬≤, ¬≥, ‚Üí, ‚Üê, etc.)

N·ªòI DUNG C·∫¶N CH·ªàNH S·ª¨A:
{raw_text[:8000]}

Tr·∫£ v·ªÅ n·ªôi dung ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c l·∫°i theo y√™u c·∫ßu:"""

            result = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=2048
            )

            if result.get("success") and result.get("text"):
                refined_content = result.get("text", "").strip()
                # L√†m s·∫°ch n·ªôi dung sau khi nh·∫≠n t·ª´ OpenRouter
                clean_content = self.clean_text_content(refined_content)
                logger.info(f"ü§ñ{prompt} ")
                logger.info(f"ü§ñ{refined_content} ")
                logger.info(f"‚úÖ Content refined and cleaned: {len(raw_text)} ‚Üí {len(clean_content)} chars")
                return clean_content
            else:
                logger.warning("OpenRouter returned insufficient content, using original")
                return self.clean_text_content(raw_text)

        except Exception as e:
            logger.error(f"‚ùå Error refining content with OpenRouter: {e}")
            return self.clean_text_content(raw_text)




# Factory function ƒë·ªÉ t·∫°o EnhancedTextbookService instance
def get_enhanced_textbook_service() -> EnhancedTextbookService:
    """
    T·∫°o EnhancedTextbookService instance m·ªõi

    Returns:
        EnhancedTextbookService: Fresh instance
    """
    return EnhancedTextbookService()
