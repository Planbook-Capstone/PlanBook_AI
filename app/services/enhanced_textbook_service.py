"""
Enhanced Textbook Service - C·∫£i ti·∫øn x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM
Tr·∫£ v·ªÅ c·∫•u tr√∫c: S√°ch ‚Üí Ch∆∞∆°ng ‚Üí B√†i ‚Üí N·ªôi dung
"""

import logging
import asyncio
import re
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import fitz  # PyMuPDF
from PIL import Image
import io

from app.services.simple_ocr_service import get_simple_ocr_service

logger = logging.getLogger(__name__)


class EnhancedTextbookService:
    """Service c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω s√°ch gi√°o khoa v·ªõi OCR v√† LLM"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.ocr_service = get_simple_ocr_service()

    async def process_textbook_to_structure(
        self,
        pdf_content: bytes,
        filename: str,
        book_metadata: Optional[Dict[str, Any]] = None,
        lesson_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω PDF s√°ch gi√°o khoa v√† tr·∫£ v·ªÅ c·∫•u tr√∫c ho√†n ch·ªânh

        Args:
            pdf_content: N·ªôi dung PDF
            filename: T√™n file
            book_metadata: Metadata s√°ch (title, subject, grade, etc.)
            lesson_id: ID b√†i h·ªçc t√πy ch·ªçn ƒë·ªÉ li√™n k·∫øt v·ªõi lesson c·ª• th·ªÉ

        Returns:
            Dict v·ªõi c·∫•u tr√∫c: book -> chapters -> lessons -> content
        """
        try:
            logger.info(f"üöÄ Starting enhanced textbook processing: {filename}")

            # Step 1: Extract all pages with OCR
            logger.info("üìÑ Extracting pages with OCR...")
            pages_data = await self._extract_pages_with_ocr(pdf_content)
            logger.info(f"‚úÖ Extracted {len(pages_data)} pages")

            # Skip image analysis to improve speed
            logger.info("‚ö° Skipping image analysis for faster processing")

            # Step 2: Combine all text from pages
            logger.info("üìù Combining text from all pages...")
            full_text = ""
            all_page_numbers = []
            for page in pages_data:
                full_text += page.get("text", "") + "\n"
                all_page_numbers.append(page.get("page_number", 0))

            logger.info(f"üìÑ ALl Data {len(pages_data)} pages")
            logger.info(f"üìÑ Combined text from {len(pages_data)} pages")

            # Step 3: Refine content directly with OpenRouter LLM
            logger.info("ü§ñ Refining content with OpenRouter LLM...")
            refined_content = await self.refine_raw_content_with_llm(full_text)
            logger.info("‚úÖ Content refinement completed")
            logger.info("‚úÖ Content refinement {refined_content}")
            # Step 4: Return clean text content directly
            logger.info("‚úÖ Content processing completed")

            # Skip image extraction for faster processing
            images_data = []
            logger.info("‚ö° Skipping image extraction for faster processing")

            return {
                "success": True,
                "clean_book_structure": refined_content,  # Return clean text directly
                "images_data": images_data,  # Empty array
                "total_pages": len(pages_data),
                "message": f"Textbook processed successfully with LLM content refinement",
            }

        except Exception as e:
            logger.error(f"‚ùå Error processing textbook: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "Failed to process textbook",
            }

    async def _extract_pages_with_ocr(self, pdf_content: bytes) -> List[Dict[str, Any]]:
        """Extract t·∫•t c·∫£ pages v·ªõi OCR n·∫øu c·∫ßn"""

        def extract_page_data():
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            pages_data = []

            for page_num in range(doc.page_count):
                page = doc[page_num]

                # Extract text normally first
                text = page.get_text("text")  # type: ignore

                # Skip image extraction for faster processing
                images = []

                pages_data.append(
                    {
                        "page_number": page_num + 1,
                        "text": text,
                        "images": images,
                        "has_text": len(text.strip()) > 50,
                    }
                )

            doc.close()
            return pages_data

        # Extract pages in background thread
        pages_data = await asyncio.get_event_loop().run_in_executor(
            self.executor, extract_page_data
        )

        # Apply OCR to pages with insufficient text
        ocr_tasks = []
        for page in pages_data:
            if not page["has_text"]:
                ocr_tasks.append(self._apply_ocr_to_page(page, pdf_content))

        if ocr_tasks:
            logger.info(
                f"üîç Applying OCR to {len(ocr_tasks)} pages with insufficient text"
            )
            ocr_results = await asyncio.gather(*ocr_tasks, return_exceptions=True)

            # Update pages with OCR results
            ocr_index = 0
            for page in pages_data:
                if not page["has_text"] and ocr_index < len(ocr_results):
                    if not isinstance(ocr_results[ocr_index], Exception):
                        page["text"] = ocr_results[ocr_index]
                        page["ocr_applied"] = True
                    ocr_index += 1

        return pages_data

    async def _apply_ocr_to_page(
        self, page_data: Dict[str, Any], pdf_content: bytes
    ) -> str:
        """Apply OCR to a specific page"""
        try:
            # Use existing OCR service for this page
            page_num = page_data["page_number"]

            # Extract just this page as bytes
            def extract_single_page():
                doc = fitz.open(stream=pdf_content, filetype="pdf")
                page = doc[page_num - 1]

                # Convert page to image
                mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better OCR
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                doc.close()

                return img_data

            img_data = await asyncio.get_event_loop().run_in_executor(
                self.executor, extract_single_page
            )

            # Apply OCR using SimpleOCRService
            image = Image.open(io.BytesIO(img_data))

            # Use SimpleOCRService's _ocr_image method which handles EasyOCR initialization
            text = await self.ocr_service._ocr_image(image, page_data['page_number'])
            return text

        except Exception as e:
            logger.error(f"OCR failed for page {page_data['page_number']}: {e}")
            return ""





















    def clean_text_content(self, text: str) -> str:
        """L√†m s·∫°ch n·ªôi dung text - lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, format kh√¥ng c·∫ßn thi·∫øt"""
        if not text:
            return ""

        logger.info("üßπ Cleaning text content...")

        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát v√† format markdown
        cleaned_text = text

        # Lo·∫°i b·ªè d·∫•u * (markdown bold/italic)
        cleaned_text = re.sub(r'\*+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u # (markdown headers)
        cleaned_text = re.sub(r'#+\s*', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u _ (markdown underline)
        cleaned_text = re.sub(r'_+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u ` (markdown code)
        cleaned_text = re.sub(r'`+', '', cleaned_text)

        # Lo·∫°i b·ªè d·∫•u [] v√† () (markdown links)
        cleaned_text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', cleaned_text)

        # Lo·∫°i b·ªè k√Ω t·ª± \b (backspace)
        cleaned_text = cleaned_text.replace('\b', '')

        # Lo·∫°i b·ªè k√Ω t·ª± \r
        cleaned_text = cleaned_text.replace('\r', '')

        # Thay th·∫ø nhi·ªÅu d·∫•u xu·ªëng d√≤ng li√™n ti·∫øp b·∫±ng 1 d·∫•u xu·ªëng d√≤ng
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)

        # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng h√≥a h·ªçc tr∆∞·ªõc ti√™n ƒë·ªÉ tr√°nh xung ƒë·ªôt v·ªõi d·∫•u -
        cleaned_text = self._normalize_chemistry_format(cleaned_text)

        # B∆∞·ªõc 1: Chu·∫©n h√≥a format c∆° b·∫£n
        # Th√™m kho·∫£ng c√°ch sau d·∫•u : n·∫øu thi·∫øu
        cleaned_text = re.sub(r':([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])', r': \1', cleaned_text)

        # B∆∞·ªõc 2: X·ª≠ l√Ω c√°c ti√™u ƒë·ªÅ ph·∫ßn h·ªçc t·∫≠p (in nghi√™ng)
        learning_sections = ['Nh·∫≠n bi·∫øt', 'Tr√¨nh b√†y', 'V·∫≠n d·ª•ng', 'Ph√¢n t√≠ch', 'ƒê√°nh gi√°', 'T·ªïng h·ª£p']
        for title in learning_sections:
            # Lo·∫°i b·ªè d·∫•u - tr∆∞·ªõc ti√™u ƒë·ªÅ ph·∫ßn
            cleaned_text = re.sub(rf'- {title}:', f'{title}:', cleaned_text)
            # Format in nghi√™ng cho ti√™u ƒë·ªÅ ph·∫ßn h·ªçc t·∫≠p
            cleaned_text = re.sub(rf'([^<br/>]){title}:', rf'\1<br/><em>{title}:</em>', cleaned_text)
            cleaned_text = re.sub(rf'^{title}:', f'<em>{title}:</em>', cleaned_text)

        # B∆∞·ªõc 3: X·ª≠ l√Ω c√°c ti√™u ƒë·ªÅ nƒÉng l·ª±c (in nghi√™ng)
        capacity_sections = ['NƒÉng l·ª±c chung', 'NƒÉng l·ª±c ƒë·∫∑c th√π', 'NƒÉng l·ª±c giao ti·∫øp v√† h·ª£p t√°c', 'NƒÉng l·ª±c gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v√† s√°ng t·∫°o', 'NƒÉng l·ª±c v·∫≠n d·ª•ng ki·∫øn th·ª©c, kƒ© nƒÉng ƒë√£ h·ªçc']
        for title in capacity_sections:
            # Format in nghi√™ng cho ti√™u ƒë·ªÅ nƒÉng l·ª±c
            cleaned_text = re.sub(rf'([^<br/>]){title}:', rf'\1<br/><em>{title}:</em>', cleaned_text)
            cleaned_text = re.sub(rf'^{title}:', f'<em>{title}:</em>', cleaned_text)

        # B∆∞·ªõc 4: X·ª≠ l√Ω danh s√°ch - th√™m d·∫•u - cho m·ª•c con (kh√¥ng th·ª•t l√πi)
        # X·ª≠ l√Ω sau d·∫•u ch·∫•m
        cleaned_text = re.sub(r'\.(\s*)([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])', r'.<br/>- \2', cleaned_text)

        # X·ª≠ l√Ω sau d·∫•u hai ch·∫•m (kh√¥ng ph·∫£i ti√™u ƒë·ªÅ)
        cleaned_text = re.sub(r':(\s*)([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])(?![^<]*</em>)', r':<br/>- \2', cleaned_text)

        # X·ª≠ l√Ω sau <br/> (ch∆∞a c√≥ d·∫•u -)
        cleaned_text = re.sub(r'<br/>(\s*)([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])(?![^<]*</em>)', r'<br/>- \2', cleaned_text)

        # B∆∞·ªõc 5: X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
        # Thay th·∫ø ". - " b·∫±ng ".<br/>- "
        cleaned_text = re.sub(r'\.\s*-\s*', '.<br/>- ', cleaned_text)

        # X·ª≠ l√Ω k√Ω hi·ªáu + th√†nh -
        cleaned_text = re.sub(r'<br/>\+([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])', r'<br/>- \1', cleaned_text)

        # B∆∞·ªõc 6: Lo·∫°i b·ªè th·ª•t l√πi kh√¥ng nh·∫•t qu√°n - chu·∫©n h√≥a t·∫•t c·∫£ v·ªÅ d·∫°ng kh√¥ng th·ª•t l√πi
        cleaned_text = re.sub(r'<br/>&nbsp;&nbsp;&nbsp;&nbsp;-\s*', '<br/>- ', cleaned_text)
        cleaned_text = re.sub(r'&nbsp;&nbsp;&nbsp;&nbsp;-\s*', '- ', cleaned_text)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi m·ªói d√≤ng nh∆∞ng gi·ªØ l·∫°i c·∫•u tr√∫c xu·ªëng h√†ng
        lines = cleaned_text.split('\n')
        cleaned_lines = [line.strip() for line in lines]

        # Gh√©p l·∫°i v·ªõi vi·ªác gi·ªØ nguy√™n c√°c d√≤ng tr·ªëng (xu·ªëng h√†ng)
        final_text = '\n'.join(cleaned_lines)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a trong m·ªói d√≤ng nh∆∞ng gi·ªØ l·∫°i xu·ªëng h√†ng
        final_text = re.sub(r'[ \t]+', ' ', final_text).strip()

        # Thay th·∫ø k√Ω t·ª± xu·ªëng h√†ng b·∫±ng th·∫ª <br/> ƒë·ªÉ frontend hi·ªÉn th·ªã ƒë√∫ng
        final_text = final_text.replace('\n', '<br/>')

        # X·ª≠ l√Ω th√™m tr∆∞·ªùng h·ª£p c√≥ <br/> li√™n ti·∫øp
        # Thay th·∫ø 2 ho·∫∑c nhi·ªÅu <br/> li√™n ti·∫øp b·∫±ng ch·ªâ 1 <br/>
        final_text = re.sub(r'(<br/>){2,}', '<br/>', final_text)

        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: sau d·∫•u : c·∫ßn c√≥ <br/> ƒë·ªÉ t√°ch ph·∫ßn
        final_text = re.sub(r':(\s*)<br/>', ':<br/>', final_text)

        logger.info(f"üßπ Text cleaned: {len(text)} ‚Üí {len(final_text)} chars")
        return final_text

    def _normalize_chemistry_format(self, text: str) -> str:
        """
        Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng h√≥a h·ªçc t·ª´ HTML sang ƒë·ªãnh d·∫°ng chu·∫©n
        VD: <sup>6</sup>Li -> ‚Å∂Li, S<sub>8</sub> -> S‚Çà, Fe<sup>2+</sup> -> Fe¬≤‚Å∫
        """
        if not text:
            return text

        # Chuy·ªÉn ƒë·ªïi superscript v·ªõi s·ªë v√† k√Ω hi·ªáu (ch·ªâ s·ªë tr√™n)
        # Ch·ªâ x·ª≠ l√Ω d·∫•u + v√† - khi ch√∫ng n·∫±m trong th·∫ª <sup>
        sup_pattern = r'<sup>([^<]+)</sup>'

        def replace_sup(match):
            content = match.group(1)
            result = ''
            superscript_map = {
                '0': '‚Å∞', '1': '¬π', '2': '¬≤', '3': '¬≥', '4': '‚Å¥',
                '5': '‚Åµ', '6': '‚Å∂', '7': '‚Å∑', '8': '‚Å∏', '9': '‚Åπ',
                '+': '‚Å∫', '-': '‚Åª'
            }
            for char in content:
                result += superscript_map.get(char, char)
            return result

        text = re.sub(sup_pattern, replace_sup, text)

        # Chuy·ªÉn ƒë·ªïi subscript (ch·ªâ s·ªë d∆∞·ªõi)
        sub_pattern = r'<sub>(\d+)</sub>'
        subscript_map = {
            '0': '‚ÇÄ', '1': '‚ÇÅ', '2': '‚ÇÇ', '3': '‚ÇÉ', '4': '‚ÇÑ',
            '5': '‚ÇÖ', '6': '‚ÇÜ', '7': '‚Çá', '8': '‚Çà', '9': '‚Çâ'
        }

        def replace_sub(match):
            number = match.group(1)
            return ''.join(subscript_map.get(digit, digit) for digit in number)

        text = re.sub(sub_pattern, replace_sub, text)

        return text

    async def refine_raw_content_with_llm(self, raw_text: str) -> str:
        """G·ª≠i text th√¥ tr·ª±c ti·∫øp ƒë·∫øn OpenRouter LLM ƒë·ªÉ l·ªçc v√† ch·ªânh s·ª≠a n·ªôi dung"""
        try:
            from app.services.openrouter_service import get_openrouter_service

            openrouter_service = get_openrouter_service()
            logger.info("ü§ñ Sending raw content to OpenRouter for refinement...")

            prompt = f"""
B·∫°n l√† chuy√™n gia gi√°o d·ª•c, h√£y l·ªçc v√† c·∫•u tr√∫c l·∫°i n·ªôi dung s√°ch gi√°o khoa ƒë·ªÉ t·ªëi ∆∞u cho h·ªá th·ªëng chunking th√¥ng minh.

Y√äU C·∫¶U C·∫§U TR√öC:
1. ƒê·ªäNH NGHƒ®A: B·∫Øt ƒë·∫ßu b·∫±ng "ƒê·ªãnh nghƒ©a:" ho·∫∑c s·ª≠ d·ª•ng c·∫•u tr√∫c "X l√†..." r√µ r√†ng
2. B√ÄI T·∫¨P/V√ç D·ª§: ƒê√°nh s·ªë r√µ r√†ng "B√†i 1.", "V√≠ d·ª• 1:", "H√£y cho bi·∫øt..."
3. B·∫¢NG BI·ªÇU: B·∫Øt ƒë·∫ßu b·∫±ng "B·∫£ng X:" v√† gi·ªØ nguy√™n c·∫•u tr√∫c b·∫£ng
4. TI·ªÇU M·ª§C: S·ª≠ d·ª•ng "I.", "II.", "1.", "2." cho c√°c ph·∫ßn ch√≠nh

Y√äU C·∫¶U N·ªòI DUNG:
- Gi·ªØ l·∫°i to√†n b·ªô kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, c√¥ng th·ª©c, v√≠ d·ª• quan tr·ªçng
- Lo·∫°i b·ªè header, footer, s·ªë trang, watermark kh√¥ng li√™n quan
- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ khoa h·ªçc v√† c√¥ng th·ª©c (H2O, 1.672 x 10^-27, etc.)
- ƒê·∫£m b·∫£o m·ªói b√†i t·∫≠p/v√≠ d·ª• c√≥ ƒë·∫ßy ƒë·ªß ƒë·ªÅ b√†i v√† l·ªùi gi·∫£i
- B·∫£ng ph·∫£i ho√†n ch·ªânh v·ªõi ti√™u ƒë·ªÅ v√† n·ªôi dung

ƒê·ªäNH D·∫†NG XU·∫§T:
- Text thu·∫ßn t√∫y, kh√¥ng markdown
- M·ªói ph·∫ßn c√°ch nhau b·∫±ng d√≤ng tr·ªëng
- Gi·ªØ nguy√™n k√Ω hi·ªáu khoa h·ªçc (^, ¬≤, ¬≥, ‚Üí, ‚Üê, etc.)

N·ªòI DUNG C·∫¶N CH·ªàNH S·ª¨A:
{raw_text[:8000]}

Tr·∫£ v·ªÅ n·ªôi dung ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c l·∫°i theo y√™u c·∫ßu:"""

            result = await openrouter_service.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_tokens=2048
            )

            if result.get("success") and result.get("text"):
                refined_content = result.get("text", "").strip()
                # L√†m s·∫°ch n·ªôi dung sau khi nh·∫≠n t·ª´ OpenRouter
                clean_content = self.clean_text_content(refined_content)
                logger.info(f"ü§ñ{prompt} ")
                logger.info(f"ü§ñ{refined_content} ")
                logger.info(f"‚úÖ Content refined and cleaned: {len(raw_text)} ‚Üí {len(clean_content)} chars")
                return clean_content
            else:
                logger.warning("OpenRouter returned insufficient content, using original")
                return self.clean_text_content(raw_text)

        except Exception as e:
            logger.error(f"‚ùå Error refining content with OpenRouter: {e}")
            return self.clean_text_content(raw_text)




# Factory function ƒë·ªÉ t·∫°o EnhancedTextbookService instance
def get_enhanced_textbook_service() -> EnhancedTextbookService:
    """
    T·∫°o EnhancedTextbookService instance m·ªõi

    Returns:
        EnhancedTextbookService: Fresh instance
    """
    return EnhancedTextbookService()
