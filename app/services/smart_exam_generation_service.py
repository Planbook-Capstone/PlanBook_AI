"""
Service cho viá»‡c táº¡o Ä‘á» thi thÃ´ng minh theo chuáº©n THPT 2025
"""

import logging
import json
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime

from app.models.smart_exam_models import SmartExamRequest, ExamStatistics
from app.services.openrouter_service import get_openrouter_service
from app.core.config import settings

logger = logging.getLogger(__name__)


class SmartExamGenerationService:
    """Service táº¡o Ä‘á» thi thÃ´ng minh theo chuáº©n THPT 2025"""

    def __init__(self):
        self.llm_service = get_openrouter_service()
        # Äáº£m báº£o service Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘áº§y Ä‘á»§
        self.llm_service._ensure_service_initialized()
        logger.info("ðŸ”„ SmartExamGenerationService: First-time initialization triggered")

    async def generate_smart_exam(
        self, exam_request: SmartExamRequest, lesson_content: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Táº¡o Ä‘á» thi thÃ´ng minh theo chuáº©n THPT 2025

        Args:
            exam_request: Request chá»©a ma tráº­n Ä‘á» thi
            lesson_content: Ná»™i dung bÃ i há»c tá»« Qdrant

        Returns:
            Dict chá»©a Ä‘á» thi Ä‘Ã£ Ä‘Æ°á»£c táº¡o
        """
        try:
            start_time = datetime.now()
            print(f"Starting smart exam generation for subject: {exam_request.subject}")

            # Ensure LLM service is initialized
            self.llm_service._ensure_service_initialized()

            if not self.llm_service.is_available():
                return {
                    "success": False,
                    "error": "LLM service not available. Please check OpenRouter API configuration."
                }

            # Táº¡o cÃ¢u há»i cho tá»«ng pháº§n theo chuáº©n THPT 2025
            all_questions = []
            part_statistics = {"part_1": 0, "part_2": 0, "part_3": 0}

            for lesson_matrix in exam_request.matrix:
                lesson_questions = await self._generate_questions_for_lesson(
                    lesson_matrix, lesson_content, exam_request.subject
                )
                
                # PhÃ¢n loáº¡i cÃ¢u há»i theo pháº§n
                for question in lesson_questions:
                    part_num = question.get("part", 1)
                    part_statistics[f"part_{part_num}"] += 1
                
                all_questions.extend(lesson_questions)

            # Sáº¯p xáº¿p cÃ¢u há»i theo pháº§n vÃ  Ä‘Ã¡nh sá»‘ láº¡i
            sorted_questions = self._sort_and_renumber_questions(all_questions)

            # TÃ­nh toÃ¡n thá»‘ng kÃª
            end_time = datetime.now()
            generation_time = (end_time - start_time).total_seconds()
            
            statistics = self._calculate_statistics(
                sorted_questions, exam_request, generation_time
            )

            return {
                "success": True,
                "exam_id": f"smart_exam_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "questions": sorted_questions,
                "statistics": statistics,
                "total_generated": len(sorted_questions),
                "exam_request": exam_request.model_dump()
            }

        except Exception as e:
            logger.error(f"Error generating smart exam: {e}")
            return {
                "success": False,
                "error": f"Failed to generate smart exam: {str(e)}"
            }

    async def _generate_questions_for_lesson(
        self, lesson_matrix, lesson_content: Dict[str, Any], subject: str
    ) -> List[Dict[str, Any]]:
        """Táº¡o cÃ¢u há»i cho má»™t bÃ i há»c theo ma tráº­n"""
        try:
            lesson_id = lesson_matrix.lessonId
            print(f"Generating questions for lesson: {lesson_id}")

            # Debug logging cho cáº¥u trÃºc dá»¯ liá»‡u
            print(f"DEBUG: lesson_content keys: {list(lesson_content.keys()) if isinstance(lesson_content, dict) else 'Not a dict'}")

            # Láº¥y ná»™i dung bÃ i há»c
            lesson_data = lesson_content.get(lesson_id, {})
            print(f"DEBUG: lesson_data type: {type(lesson_data)}")
            print(f"DEBUG: lesson_data keys: {list(lesson_data.keys()) if isinstance(lesson_data, dict) else 'Not a dict'}")

            if not lesson_data:
                logger.warning(f"No content found for lesson: {lesson_id}")
                return []

            # Kiá»ƒm tra náº¿u lesson_data cÃ³ cáº¥u trÃºc {"success": True, "content": {...}}
            if isinstance(lesson_data, dict) and "content" in lesson_data:
                actual_content = lesson_data.get("content", {})
                print(f"DEBUG: Found nested content structure, extracting actual content")
                print(f"DEBUG: actual_content keys: {list(actual_content.keys()) if isinstance(actual_content, dict) else 'Not a dict'}")
            else:
                actual_content = lesson_data
                print(f"DEBUG: Using lesson_data directly as content")

            if not actual_content:
                logger.warning(f"No actual content found for lesson: {lesson_id}")
                return []

            all_lesson_questions = []

            # Táº¡o cÃ¢u há»i cho tá»«ng pháº§n
            for part in lesson_matrix.parts:
                part_questions = await self._generate_questions_for_part(
                    part, actual_content, subject, lesson_id
                )
                all_lesson_questions.extend(part_questions)

            return all_lesson_questions

        except Exception as e:
            logger.error(f"Error generating questions for lesson {lesson_matrix.lessonId}: {e}")
            return []

    async def _generate_questions_for_part(
        self, part, lesson_data: Dict[str, Any], subject: str, lesson_id: str
    ) -> List[Dict[str, Any]]:
        """Táº¡o cÃ¢u há»i cho má»™t pháº§n cá»¥ thá»ƒ"""
        try:
            part_questions = []
            part_num = part.part
            objectives = part.objectives

            # Táº¡o cÃ¢u há»i theo ma tráº­n user input cho táº¥t cáº£ cÃ¡c pháº§n
            for level, count in [("Biáº¿t", objectives.Biáº¿t), ("Hiá»ƒu", objectives.Hiá»ƒu), ("Váº­n_dá»¥ng", objectives.Váº­n_dá»¥ng)]:
                if count > 0:
                    level_questions = await self._generate_questions_for_level(
                        part_num, level, count, lesson_data, subject, lesson_id
                    )
                    part_questions.extend(level_questions)

            return part_questions

        except Exception as e:
            logger.error(f"Error generating questions for part {part.part}: {e}")
            return []

    async def _generate_questions_for_level(
        self, part_num: int, level: str, count: int, lesson_data: Dict[str, Any],
        subject: str, lesson_id: str
    ) -> List[Dict[str, Any]]:
        """Táº¡o cÃ¢u há»i cho má»™t má»©c Ä‘á»™ nháº­n thá»©c cá»¥ thá»ƒ"""
        try:
            print(f"DEBUG: _generate_questions_for_level - Part {part_num}, Level {level}, Count {count}")

            # XÃ¡c Ä‘á»‹nh loáº¡i cÃ¢u há»i theo pháº§n
            question_type = self._get_question_type_by_part(part_num)
            print(f"DEBUG: Question type for part {part_num}: {question_type}")

            # Táº¡o prompt cho LLM
            prompt = self._create_prompt_for_level(
                part_num, level, count, question_type, lesson_data, subject, lesson_id
            )
            print(f"DEBUG: Prompt created, length: {len(prompt)}")
            print(f"DEBUG: Prompt preview: {prompt[:200]}...")

            # Gá»i LLM Ä‘á»ƒ táº¡o cÃ¢u há»i
            print(f"DEBUG: Calling LLM service...")
            response = await self.llm_service.generate_content(
                prompt=prompt,
                temperature=0.3,
                max_tokens=4000
            )
            print(f"DEBUG: LLM response received, length: {len(str(response))}")
            print(f"DEBUG: LLM response preview: {str(response)[:200]}...")

            # Debug logging for LLM response
            print(f"DEBUG: LLM response success: {response.get('success', False)}")
            if response.get("success", False):
                response_text = response.get("text", "")
                print(f"DEBUG: LLM response length: {len(response_text)}")
                print(f"DEBUG: LLM response preview: {response_text[:300]}...")
            else:
                logger.error(f"DEBUG: LLM error: {response.get('error', 'Unknown error')}")

            if not response.get("success", False):
                logger.error(f"LLM failed for part {part_num}, level {level}: {response.get('error')}")
                return []

            # Parse response JSON
            questions = self._parse_llm_response(response.get("text", ""), part_num, level, lesson_id)

            # Debug logging for parsed questions
            print(f"DEBUG: Parsed {len(questions)} questions from LLM response")
            
            # Giá»›i háº¡n sá»‘ cÃ¢u há»i theo yÃªu cáº§u
            return questions[:count]

        except Exception as e:
            logger.error(f"Error generating questions for level {level}: {e}")
            return []

    def _get_question_type_by_part(self, part_num: int) -> str:
        """XÃ¡c Ä‘á»‹nh loáº¡i cÃ¢u há»i theo pháº§n"""
        if part_num == 1:
            return "TN"  # Tráº¯c nghiá»‡m nhiá»u phÆ°Æ¡ng Ã¡n
        elif part_num == 2:
            return "DS"  # ÄÃºng/Sai
        elif part_num == 3:
            return "TL"  # Tá»± luáº­n
        else:
            return "TN"  # Default

    def _create_prompt_for_level(
        self, part_num: int, level: str, count: int, question_type: str,
        lesson_data: Dict[str, Any], subject: str, lesson_id: str
    ) -> str:
        """Create prompt for LLM according to THPT 2025 standards"""

        # Debug logging cho cáº¥u trÃºc dá»¯ liá»‡u
        print(f"DEBUG: _create_prompt_for_level - lesson_data keys: {list(lesson_data.keys()) if isinstance(lesson_data, dict) else 'Not a dict'}")

        # Láº¥y ná»™i dung bÃ i há»c - lesson_data Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ á»Ÿ _generate_questions_for_lesson
        main_content = lesson_data.get("main_content", "")
        lesson_info = lesson_data.get("lesson_info", {})

        # Debug logging
        print(f"DEBUG: main_content type: {type(main_content)}")
        print(f"DEBUG: main_content length: {len(str(main_content))}")
        print(f"DEBUG: lesson_info: {lesson_info}")

        # Ensure main_content is string and limit length
        if isinstance(main_content, str):
            content_preview = main_content[:2000] if len(main_content) > 2000 else main_content
        elif isinstance(main_content, list):
            # If it's a list, join the items
            content_preview = " ".join(str(item) for item in main_content)[:2000]
        else:
            content_preview = str(main_content)[:2000] if main_content else ""

        if not content_preview.strip():
            logger.warning(f"Empty content for lesson {lesson_id}, using fallback")
            content_preview = f"Ná»™i dung bÃ i há»c {lesson_id} - {lesson_info.get('lesson_title', 'ChÆ°a cÃ³ tiÃªu Ä‘á»')}"

        # Part descriptions
        part_descriptions = {
            1: "PART I: Multiple choice questions (A, B, C, D)",
            2: "PART II: True/False evaluation questions (each question has a main scenario and 4 independent statements a, b, c, d to evaluate)",
            3: "PART III: Essay questions requiring calculations, chemical equations, and logical reasoning (6 questions total)"
        }

        prompt = f"""
You are an expert in creating {subject} exams according to THPT 2025 standards.

{part_descriptions.get(part_num, "")}

LESSON INFORMATION:
- Lesson: {lesson_id}
- Chapter: {lesson_info.get('chapter_title', '')}
- Content: {content_preview}...

REQUIREMENTS:
- Create {count} questions at "{level}" cognitive level
- Part {part_num} - {self._get_part_description(part_num)}
- Questions must be based on lesson content
- Ensure scientific accuracy

{self._get_specific_instructions_by_part(part_num, level)}

JSON RESPONSE FORMAT:
[
    {{
        "question": "Question content",
        "answer": {self._get_answer_format_by_part(part_num)},
        "explanation": "Answer explanation",
        "cognitive_level": "{level}",
        "part": {part_num}
    }}
]

Return only JSON, no additional text.
"""
        return prompt

    def _get_part_description(self, part_num: int) -> str:
        """Get detailed description for each part"""
        descriptions = {
            1: "Multiple choice questions",
            2: "True/False questions",
            3: "Short answer questions"
        }
        return descriptions.get(part_num, "")

    def _get_specific_instructions_by_part(self, part_num: int, level: str) -> str:
        """HÆ°á»›ng dáº«n cá»¥ thá»ƒ cho tá»«ng pháº§n"""
        if part_num == 1:
            return """
HÆ¯á»šNG DáºªN PHáº¦N I:
- Má»—i cÃ¢u cÃ³ 4 phÆ°Æ¡ng Ã¡n A, B, C, D
- Chá»‰ cÃ³ 1 Ä‘Ã¡p Ã¡n Ä‘Ãºng
- CÃ¢u há»i rÃµ rÃ ng, khÃ´ng gÃ¢y nháº§m láº«n
"""
        elif part_num == 2:
            return """
HÆ¯á»šNG DáºªN PHáº¦N II - QUAN TRá»ŒNG:
- Táº¡o cÃ¢u há»i chÃ­nh vá» má»™t tÃ¬nh huá»‘ng thÃ­ nghiá»‡m hoáº·c pháº£n á»©ng hÃ³a há»c cá»¥ thá»ƒ
- Sau cÃ¢u há»i chÃ­nh, táº¡o 4 phÃ¡t biá»ƒu Ä‘á»™c láº­p a), b), c), d)
- Má»—i phÃ¡t biá»ƒu lÃ  má»™t kháº³ng Ä‘á»‹nh cá»¥ thá»ƒ vá» tÃ¬nh huá»‘ng Ä‘Ã³
- Trong trÆ°á»ng "answer", Ä‘áº·t cáº£ ná»™i dung phÃ¡t biá»ƒu VÃ€ Ä‘Ã¡nh giÃ¡ Ä‘Ãºng/sai:
  {"a": {"content": "PhÃ¡t biá»ƒu a cá»¥ thá»ƒ", "evaluation": "ÄÃºng"}, "b": {"content": "PhÃ¡t biá»ƒu b cá»¥ thá»ƒ", "evaluation": "Sai"}, ...}
- CÃ¡c phÃ¡t biá»ƒu pháº£i liÃªn quan Ä‘áº¿n cÃ¹ng má»™t chá»§ Ä‘á»/tÃ¬nh huá»‘ng
- VÃ­ dá»¥ format:
  "question": "XÃ©t thÃ­ nghiá»‡m hÃ²a tan kim loáº¡i X trong dung dá»‹ch HCl. Cho biáº¿t cÃ¡c phÃ¡t biá»ƒu sau Ä‘Ãºng hay sai:",
  "answer": {
    "a": {"content": "Kim loáº¡i X tÃ¡c dá»¥ng vá»›i HCl táº¡o ra khÃ­ H2", "evaluation": "ÄÃºng"},
    "b": {"content": "Pháº£n á»©ng nÃ y lÃ  pháº£n á»©ng oxi hÃ³a khá»­", "evaluation": "ÄÃºng"},
    "c": {"content": "Dung dá»‹ch sau pháº£n á»©ng cÃ³ pH > 7", "evaluation": "Sai"},
    "d": {"content": "Kim loáº¡i X bá»‹ oxi hÃ³a trong pháº£n á»©ng nÃ y", "evaluation": "ÄÃºng"}
  }
"""
        elif part_num == 3:
            return """
HÆ¯á»šNG DáºªN PHáº¦N III - Tá»° LUáº¬N HÃ“A Há»ŒC:
- Táº¡o 6 cÃ¢u há»i tá»± luáº­n cÃ³ tÃ­nh toÃ¡n, viáº¿t phÆ°Æ¡ng trÃ¬nh hÃ³a há»c
- CÃ¡c dáº¡ng cÃ¢u phá»• biáº¿n: tÃ­nh hiá»‡u suáº¥t/khá»‘i lÆ°á»£ng, viáº¿t chuá»—i pháº£n á»©ng, tÃ­nh Î”H, chuáº©n Ä‘á»™, xÃ¡c Ä‘á»‹nh cÃ´ng thá»©c
- CÃ¢u há»i pháº£i cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ vÃ  yÃªu cáº§u láº­p luáº­n logic
- ÄÃ¡p Ã¡n chá»‰ lÃ  sá»‘ (khÃ´ng cÃ³ chá»¯ A, B, C, D)
- Má»™t sá»‘ cÃ¢u cÃ³ thá»ƒ yÃªu cáº§u sáº¯p xáº¿p hoáº·c suy luáº­n tá»« lÃ½ thuyáº¿t
"""
        return ""

    def _get_answer_format_by_part(self, part_num: int) -> str:
        """Format Ä‘Ã¡p Ã¡n theo tá»«ng pháº§n"""
        if part_num == 1:
            return '{"A": "PhÆ°Æ¡ng Ã¡n A", "B": "PhÆ°Æ¡ng Ã¡n B", "C": "PhÆ°Æ¡ng Ã¡n C", "D": "PhÆ°Æ¡ng Ã¡n D", "dung": "A"}'
        elif part_num == 2:
            return '{"a": {"content": "PhÃ¡t biá»ƒu a cá»¥ thá»ƒ", "evaluation": "ÄÃºng"}, "b": {"content": "PhÃ¡t biá»ƒu b cá»¥ thá»ƒ", "evaluation": "Sai"}, "c": {"content": "PhÃ¡t biá»ƒu c cá»¥ thá»ƒ", "evaluation": "ÄÃºng"}, "d": {"content": "PhÃ¡t biá»ƒu d cá»¥ thá»ƒ", "evaluation": "Sai"}}'
        elif part_num == 3:
            return '{"dap_an": "Sá»‘ hoáº·c giÃ¡ trá»‹ cá»¥ thá»ƒ (VD: 2.5, 0.1M, 25%)"}'
        return '{"dung": "A"}'

    def _parse_llm_response(self, response_text: str, part_num: int, level: str, lesson_id: str) -> List[Dict[str, Any]]:
        """Parse response tá»« LLM"""
        try:
            # TÃ¬m JSON trong response
            start_idx = response_text.find('[')
            end_idx = response_text.rfind(']') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.error("No JSON array found in LLM response")
                return []

            json_str = response_text[start_idx:end_idx]
            questions = json.loads(json_str)

            # Validate vÃ  bá»• sung thÃ´ng tin
            validated_questions = []
            for q in questions:
                if isinstance(q, dict) and "question" in q:
                    q["part"] = part_num
                    q["cognitive_level"] = level
                    q["lesson_id"] = lesson_id
                    q["question_type"] = self._get_question_type_by_part(part_num)
                    validated_questions.append(q)

            print(f"DEBUG: Validated {len(validated_questions)} questions out of {len(questions)} raw questions")
            return validated_questions

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM response: {e}")
            return []
        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}")
            return []

    def _sort_and_renumber_questions(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Sáº¯p xáº¿p cÃ¢u há»i theo pháº§n vÃ  Ä‘Ã¡nh sá»‘ láº¡i"""
        try:
            # Sáº¯p xáº¿p theo pháº§n
            sorted_questions = sorted(questions, key=lambda x: x.get("part", 1))
            
            # ÄÃ¡nh sá»‘ láº¡i theo tá»«ng pháº§n
            part_counters = {1: 1, 2: 1, 3: 1}
            
            for question in sorted_questions:
                part = question.get("part", 1)
                question["stt"] = part_counters[part]
                question["stt_global"] = len([q for q in sorted_questions[:sorted_questions.index(question)+1]])
                part_counters[part] += 1

            return sorted_questions

        except Exception as e:
            logger.error(f"Error sorting and renumbering questions: {e}")
            return questions

    def _calculate_statistics(
        self, questions: List[Dict[str, Any]], exam_request: SmartExamRequest, generation_time: float
    ) -> ExamStatistics:
        """TÃ­nh toÃ¡n thá»‘ng kÃª Ä‘á» thi"""
        try:
            # Äáº¿m cÃ¢u há»i theo pháº§n
            part_counts = {1: 0, 2: 0, 3: 0}
            difficulty_counts = {"Biáº¿t": 0, "Hiá»ƒu": 0, "Váº­n_dá»¥ng": 0}
            
            for question in questions:
                part = question.get("part", 1)
                part_counts[part] += 1

                level = question.get("cognitive_level", "Biáº¿t")
                if level in difficulty_counts:
                    difficulty_counts[level] += 1

            return ExamStatistics(
                total_questions=len(questions),
                part_1_questions=part_counts[1],
                part_2_questions=part_counts[2],
                part_3_questions=part_counts[3],
                lessons_used=len(exam_request.matrix),
                difficulty_distribution=difficulty_counts,
                generation_time=generation_time,
                created_at=datetime.now().isoformat()
            )

        except Exception as e:
            logger.error(f"Error calculating statistics: {e}")
            return ExamStatistics(
                total_questions=len(questions),
                part_1_questions=0,
                part_2_questions=0,
                part_3_questions=0,
                lessons_used=0,
                difficulty_distribution={"Biáº¿t": 0, "Hiá»ƒu": 0, "Váº­n_dá»¥ng": 0},
                generation_time=generation_time,
                created_at=datetime.now().isoformat()
            )


# Factory function Ä‘á»ƒ táº¡o SmartExamGenerationService instance
def get_smart_exam_generation_service() -> SmartExamGenerationService:
    """
    Táº¡o SmartExamGenerationService instance má»›i

    Returns:
        SmartExamGenerationService: Fresh instance
    """
    return SmartExamGenerationService()

# Backward compatibility - deprecated, sá»­ dá»¥ng get_smart_exam_generation_service() thay tháº¿
# Lazy loading Ä‘á»ƒ trÃ¡nh khá»Ÿi táº¡o ngay khi import
def _get_smart_exam_generation_service_lazy():
    """Lazy loading cho backward compatibility"""
    return get_smart_exam_generation_service()


