"""
LLM Endpoints - Endpoint ƒë∆°n gi·∫£n ƒë·ªÉ x·ª≠ l√Ω text v·ªõi LLM
"""

import logging
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, Any, Optional

from app.services.llm_service import get_llm_service
from app.services.openrouter_service import get_openrouter_service

logger = logging.getLogger(__name__)

router = APIRouter()


class LLMRequest(BaseModel):
    """Request model cho LLM endpoint"""
    text: str
    temperature: Optional[float] = 0.1
    max_tokens: Optional[int] = 4096
    custom_prompt: Optional[str] = None


class LLMResponse(BaseModel):
    """Response model cho LLM endpoint"""
    success: bool
    response: str
    error: Optional[str] = None
    usage: Optional[Dict[str, Any]] = None


@router.post("/generate", response_model=LLMResponse)
async def generate_llm_response(request: LLMRequest):
    """
    Endpoint LLM ƒë∆°n gi·∫£n nh·∫≠n v√†o string v√† tr·∫£ v·ªÅ response
    
    Args:
        request: LLMRequest ch·ª©a text c·∫ßn x·ª≠ l√Ω
        
    Returns:
        LLMResponse ch·ª©a k·∫øt qu·∫£ t·ª´ LLM
    """
    try:
        llm_service = get_llm_service()

        # Ki·ªÉm tra LLM service c√≥ s·∫µn kh√¥ng
        if not llm_service.is_available():
            raise HTTPException(
                status_code=503, 
                detail="LLM service kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh API key."
            )
        
        # T·∫°o prompt - s·ª≠ d·ª•ng custom_prompt n·∫øu c√≥, kh√¥ng th√¨ d√πng text tr·ª±c ti·∫øp
        if request.custom_prompt:
            prompt = f"{request.custom_prompt}\n\nText c·∫ßn x·ª≠ l√Ω:\n{request.text}"
        else:
            prompt = request.text
        
        # G·ªçi LLM service
        logger.info(f"Processing LLM request with {len(request.text)} characters")
        result = await llm_service._generate_content(
            prompt=prompt,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        if result["success"]:
            return LLMResponse(
                success=True,
                response=result["text"],
                error=None,
                usage=result.get("usage")
            )
        else:
            return LLMResponse(
                success=False,
                response="",
                error=result["error"]
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in LLM generate endpoint: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"L·ªói x·ª≠ l√Ω LLM: {str(e)}"
        )


@router.post("/test-openrouter", response_model=LLMResponse)
async def test_openrouter_with_logging(request: LLMRequest):
    """
    Endpoint test ƒë·ªÉ ki·ªÉm tra OpenRouter service v·ªõi logging chi ti·∫øt

    Args:
        request: LLMRequest ch·ª©a text c·∫ßn x·ª≠ l√Ω

    Returns:
        LLMResponse ch·ª©a k·∫øt qu·∫£ t·ª´ OpenRouter API v·ªõi logging chi ti·∫øt
    """
    try:
        openrouter_service = get_openrouter_service()

        # Ki·ªÉm tra OpenRouter service c√≥ s·∫µn kh√¥ng
        if not openrouter_service.is_available():
            raise HTTPException(
                status_code=503,
                detail="OpenRouter service kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh API key."
            )

        # T·∫°o prompt - s·ª≠ d·ª•ng custom_prompt n·∫øu c√≥, kh√¥ng th√¨ d√πng text tr·ª±c ti·∫øp
        if request.custom_prompt:
            prompt = f"{request.custom_prompt}\n\nText c·∫ßn x·ª≠ l√Ω:\n{request.text}"
        else:
            prompt = request.text

        # G·ªçi OpenRouter service (s·∫Ω in ra chi ti·∫øt request/response)
        logger.info(f"üß™ Testing OpenRouter with {len(request.text)} characters")
        result = await openrouter_service.generate_content(
            prompt=prompt,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )

        if result["success"]:
            return LLMResponse(
                success=True,
                response=result["text"],
                error=None,
                usage=result.get("usage")
            )
        else:
            return LLMResponse(
                success=False,
                response="",
                error=result["error"]
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in OpenRouter test endpoint: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"L·ªói x·ª≠ l√Ω OpenRouter: {str(e)}"
        )


@router.get("/test-connection")
async def test_openrouter_connection():
    """
    Endpoint test k·∫øt n·ªëi OpenRouter API

    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ test connection
    """
    try:
        openrouter_service = get_openrouter_service()
        result = await openrouter_service.test_connection()

        if result["success"]:
            return {
                "success": True,
                "message": "OpenRouter API connection successful",
                "details": result
            }
        else:
            return {
                "success": False,
                "message": "OpenRouter API connection failed",
                "error": result.get("error")
            }

    except Exception as e:
        logger.error(f"Error testing OpenRouter connection: {e}")
        return {
            "success": False,
            "message": "OpenRouter connection test failed",
            "error": str(e)
        }

